Welcome back, and in this lesson I want to introduce another core AWS service, the simple
storage service known as S3.
If you use AWS in production, you need to understand S3.
This lesson will give you the very basics, because I'll be deep diving in a specific
S3 section later in the course, and the product will feature constantly as we go.
Pretty much every other AWS service has some kind of interaction with S3.
So let's jump in and get started.
S3 is a global storage platform.
It's global because it runs from all of the AWS regions and can be accessed from anywhere
with an internet connection.
It's a public service.
It's regional based because your data is stored in a specific AWS region at rest.
So when it's not being used, it's stored in a specific region.
And it never leaves that region unless you explicitly configure it to.
S3 is regionally resilient, meaning the data is replicated across availability zones in
that region.
S3 can tolerate the failure of an AZ, and it also has some ability to replicate data
between regions, but more on that in the S3 section of the course.
Now S3 might initially appear confusing.
If you utilise it from the UI, you appear not to have to select a region.
Instead, you select the region when you create things inside S3, which I'll talk about soon.
S3 is a public service, so it can be accessed from anywhere as long as you have an internet connection.
The service itself runs from the AWS public zone.
It can cope with unlimited data amounts, and it's designed for multi-user usage of that
data.
So millions of users could be accessing cute cat pictures added by the Animals for Life
rescue officers.
S3 is perfect for hosting large amounts of data.
So think movies, audio distribution, large-scale photo storage like stock images, large textual
data or big data sets.
It could be just as easily used for millions or billions of IoT devices, or to store images
for a blog.
It scales from nothing to near unlimited levels.
Now S3 is economical.
It's a great value service for storing and allowing access to data, and it can be accessed
using a variety of methods.
There's the GUI, you can use the command line, the AWS APIs or even standard methods such
as HTTP.
I want you to think of S3 as the default storage service in AWS.
It should be your default starting point unless your requirement isn't delivered by S3, and
I'll talk more about the limitations and use cases later in this lesson.
S3 has two main things that it delivers.
Objects and buckets.
Objects are the data that S3 stores, your cat picture, the latest episode of Game of
Thrones, which you have stored legally of course, or it could be large-scale data sets
showing the migration of the koala population in Australia after a major bushfire.
Buckets are containers for objects.
It's buckets and objects that I want to cover in this lesson as an introduction to the service.
So first, let's talk about objects, and this is an object in S3.
You can think about objects like files.
Conceptually most of the time they're interchangeable.
An object in S3 is made up of two main components and some associated metadata.
First, there is the object key, and for now you can think of the object key as similar
to a file name.
The key identifies the object in a bucket, so if you know the object key and the bucket,
then you can uniquely access the object, assuming that you have permissions.
Remember by default, even for public services, there is no access in AWS initially except
for the account root user.
Now the other main component of an object is its value, and the value is the data or
the contents of the object.
In this case, a sequence of binary data which represents a koala in his house.
In this course, I want to avoid suggesting that you remember pointless values.
Sometimes though, there are things that you do need to commit to memory, and this is one
of those times.
The value of an object, in essence how large the object is, can range from 0 bytes up to
5 terabytes in size.
So you can have an empty object, or you can have one that is a huge 5 TB.
This is one of the reasons why S3 is so scalable and so useful in a wide range of situations,
because of this range of sizes for objects.
Now the other components of an object aren't that important to know at this stage, but
just so you hear the terms that I'll use later, objects also have a version ID, metadata,
some access control, as well as sub-resources.
Now don't worry about what they do for now, I'll be covering them all later.
For this lesson, just try to commit to memory what an object is, what components it has,
and the size range for an object.
So now that we've talked about objects, let's move on and look at buckets.
Buckets are created in a specific AWS region, and let's use Sydney, or AP SE2 as an example.
This has two main impacts.
Firstly, your data that's inside a bucket has a primary home region, and it never leaves
that region unless you as an architect, or one of your system admins, configures that
data to leave that region.
That means that S3 has stable and controlled data sovereignty.
By creating a bucket in a region, you can control what laws and rules apply to that
data.
What it also means is that the blast radius of a failure is that region.
Now this might be a new term.
What I mean by blast radius is that if a major failure occurs, say a natural disaster or
a large-scale data corruption, the effect of that will be contained within the region.
Now a bucket is identified by its name, the bucket name, in this case, Koala data.
A bucket name needs to be globally unique, so that's across all regions and all accounts
of AWS.
If I pick a bucket name, in this case Koala data, nobody else can use it in any AWS account.
Now I'm making a point of stressing this as it often comes up in the exam.
Most AWS things are often unique in a region or unique in your account.
For example, I can have an IAM user called Fred, and you can also have an IAM user called
Fred.
Buckets, though, are different.
With buckets, the name has to be totally unique, and that's across all regions and all AWS
accounts.
I've seen it come up in the exam a few times, so this is definitely a point to remember.
Now buckets can hold an unlimited number of objects, and because objects can range from
zero to five TB in size, that essentially means that a bucket can hold from zero to
unlimited bytes of data.
It's an infinitely scalable storage system.
Now one of the most important things that I want to say in this lesson is that as an
object storage system, an S3 bucket has no complex structure.
It's flat.
It has a flat structure.
All objects are stored within the bucket at the same level.
So this isn't like a file system where you can truly have files within folders within
folders.
Everything is stored in the bucket at the root level.
But if you do a listing on an S3 bucket, you will see what you think are folders.
Even the UI presents this as folders, but it's important for you to know at this stage
that that's not how it actually is.
In a bucket where you see three image files, koala1, 2 and 3.jpg, the first thing is that
inside S3 there's no concept of file type based on the name.
These are just three objects where the object key is koala1.jpg, koala2.jpg and koala3.jpg.
Now folders in S3 are represented when we have object names that are structured like
these.
So the objects have a key of forward slash old, forward slash koala1, 2 and 3.jpg.
When we create object names like this, then S3 presents them in the UI as a folder called
old.
So because we've got object names that begin with slash old, then S3 presents this as a
folder called old and then inside that folder we've got koala1, 2 and 3.jpg.
Now 9 out of 10 times this detail doesn't matter, but I want to make sure that you understand
how it actually works.
Folders are often referred to as prefixes in S3 because they're part of the object
names.
They prefix the object names.
As you move through the various stages of your AWS learnings, this is going to make
more and more sense and I'm going to demonstrate this in the next lesson, which is a demo lesson.
Now to summarise, buckets are just containers, they're stored in a region and for S3 they're
generally where a lot of permissions and options are set.
So remember that, buckets are generally the default place where you should go to to configure
the way that S3 works.
Now I want to cover a few summary items and then step through some patterns and anti-patterns
for S3 before we move to the demo.
But first in exam power up, these are things that you should try to remember and they will
really help in the exam.
Most bucket names are globally unique, remember that one because it will really help in the
exam.
I've seen a lot of times where AWS have included trick questions which test your knowledge
of this one.
If you get any errors that you can't create a bucket, a lot of the time it's because somebody
else already has that bucket name.
Now bucket names do have some restrictions, they need to be between 3 and 63 characters,
all lowercase and no underscores.
They have to start with a lowercase letter or a number and they can't be formatted like
IP addresses, so you can't have 1.1.1.1 as your bucket name.
Now there are some limits in terms of buckets, now limits are often things that you don't
need to remember for the exam, but this is one of the things that you do.
There is a limit of a hundred buckets that you can have in an AWS account, so this is
not per region, it's for the entire account.
There's a soft limit of 100 and a hard limit, so you can increase all the way up to this
hard limit using support requests and this hard limit is a thousand.
Now this matters for architectural reasons, it's not just an arbitrary number.
If you're designing a system which uses S3 and users of that system store data inside
S3, you can't implement a solution that has one bucket per user if you have anywhere near
this number of users.
So if you have anywhere from a hundred to a thousand users or more of a system, then
you're not going to be able to have one bucket per user because you'll hit this hard limit.
You tend to find this in the exam quite often, it'll ask you how to structure a system which
has potentially thousands of users.
What you can do is take a single bucket and divide it up using prefixes, so those folders
that aren't really folders, and then in that way you can have multiple users using one
bucket.
Remember the 100 and 1000, it's a 100 soft limit and a 1000 hard limit.
You aren't limited in terms of objects in a bucket, you can have zero to an infinite
number of objects in a bucket, and each object can range in size from zero bytes to 5 TB
in size.
And then finally in terms of the object structure, an object consists of a key which is its name
and then the value which is the data.
And there are other elements to an object which I'll discuss later in the course, but
for now just remember the two main components, the key and the value.
Now all of these points are worth noting down, maybe make them into a set of flashcards and
you can use them later on during your studies.
S3 is pretty straightforward in that there tend to be a number of things that it's really
good at and a fairly small set of things that it's not suitable for.
So let's take a look.
S3 is an object storage system, it's not a file storage system and it's not a block
storage system which are the other main types.
What this means is that if you have a requirement where you're accessing the whole of these
entities, so the whole of an object, so an image, an audio file, and you're doing all
of that at once, then it's a candidate for object storage.
If you have a Windows server which needs to access a network file system, then it's not
that needs to be file-based storage.
S3 has no file system, it's flat, so you can't browse to an S3 bucket like you would a file
share in Windows.
Likewise, it's not block storage, which means you can't mount it as a mount point or a volume
on the Linux or Windows.
When you're dealing with virtual machines or instances, you mount block storage to them.
Block storage is basically virtual hard disks.
In EC2, you have EBS, which is block storage.
Block storage is generally limited to one thing accessing it at a time, one instance
in the case of EBS.
S3 doesn't have that single user limitation and it's not block storage, but that means
you can't mount it as a drive.
S3 is great for large-scale data storage or distribution.
Many examples I'll show you throughout the course will fit into that category and it's
also good for offloading things.
If you have a blog with lots of posts and lots of images or audio or movies, instead
of storing that data on an expensive compute instance, you can move it to an S3 bucket
and configure your blog software to point your users at S3 directly.
You can often shrink your instance by offloading data onto S3, and don't worry, I'll be demoing
this later in the course.
Finally, S3 should be your default thought for any input to AWS services or output from
AWS services.
Most services which consume data and or output data can have S3 as an option to take data
from or put data to when it's finished.
So if you're faced with any exam questions and there's a number of options on where
to store data, S3 should be your default.
There are plenty of AWS services which can output large quantities of data or ingest
large quantities of data, and most of the time it's S3 which is an ideal storage platform
for that service.
OK, time for a quick demo, and in this demo we're just going to run through the process
of creating a simple S3 bucket, uploading some objects to that bucket, and demonstrating
exactly how the folder functionality works inside S3, and I'm also going to demonstrate
a number of elements of how access and permissions work with S3.
So go ahead and complete this video, and when you're ready, join me in the next, which
is going to be a demo of S3.
