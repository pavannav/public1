Welcome back. In this lesson, I want to either introduce or refresh your memory on the high-level architecture of CloudFront.
So what it does, what components it has, and some of the important terminology.
So this lesson is just going to be an introduction or a refresher.
So let's jump in and get started.
CloudFront is a content delivery network.
Its job is to improve the delivery of content from its original location to the viewers of that data.
And it does so by caching and by using an efficient global network.
So let's look at an example.
Let's say that I'm running an application from Australia.
And the application becomes so successful that it has global users.
Bob on the west coast of the US and Julie in the UK.
Now if they access the application, then the data has to flow from Australia to them.
And in reality, the route that the data takes will be a lot less direct than I'm showing on screen now.
Whatever route the data takes, it's traveling long distances.
And this introduces two problems.
Higher latencies and slower transfer speeds.
Both of these impact user experience.
It means that data is being transferred globally.
And it means that it's being transferred each and every time that data is requested.
And CloudFront helps us with this.
I want to introduce some concepts first before we look visually at the architecture of CloudFront.
Now you might be familiar with these terms if you've studied for the Associate Level Solutions Architect certification.
If so, consider this a refresher for some of the more advanced topics covered in later lessons of this section.
So first we've got an origin.
And an origin is the original location of your content.
An origin can either be an S3 origin or a custom origin.
It's either S3 or anything else which runs a web server and has a publicly routable IP version 4 address.
So an S3 bucket or anything else.
The origin though is where your content lives and it's where it's served from.
And we'll talk more about origins as we move on in the section.
But for one CloudFront configuration you would either have one or more origins.
Next is a distribution.
And the distribution is the unit of configuration within CloudFront.
To use CloudFront you create a distribution.
And this distribution gets deployed out to the CloudFront network.
Almost everything is configured within a distribution.
Either directly or indirectly.
And so when I just mentioned that a CloudFront configuration can have multiple origins.
Well they're all configured inside a distribution.
But indirectly as I'll talk about later in this lesson.
Next we have edge locations.
And these are the names of the pieces of global infrastructure where your content is cached.
AWS have regions which are located globally.
Generally at least one in all major markets for AWS.
But these are almost always located in or around capital cities or other major areas of usage for AWS.
Edge locations are bigger in number and more widely distributed.
So there are more edge locations.
They're smaller than regions and these are distributed globally much closer to your customers.
At the time of creating this lesson there are over 200 edge locations.
And there's a big chance that there's one in the city closest to you.
So just to put this into context edge locations are smaller than AWS regions.
They're generally one or more racks in a third party data center.
And are usually 90% storage with the odd bit of compute services tacked on for certain AWS services.
Which we'll be talking about throughout this course.
But generally edge locations are used for the storage or the caching of data for CloudFront.
You can't use them for example for deploying EC2 instances.
So you're not able to select an edge location as a target to deploy an EC2 instance.
Now the last term which I want to introduce is a regional edge cache.
And these are much bigger than edge locations and there are fewer of them.
They're designed to hold more data to cache things which are accessed less frequently.
But where there's still a performance benefit for caching your data closer to your customers than your origins.
So regional edge caches do provide a real benefit when it comes to larger more global deployments of CloudFront.
Now the diagram coming up next will make it clear how regional edge caches and edge locations are related.
So let's look at that next.
CloudFront isn't that complex as an architecture.
At a high level Bob uploads some content to an S3 bucket which is going to be used as the origin for our CloudFront distribution.
In addition Bob creates a CloudFront distribution which as I've mentioned is the configuration for CloudFront.
On the distribution he configures the S3 bucket to be the origin.
So the original location of the content.
And then at the other side of this architecture are our edge locations.
So the locations which cache the content which will be distributed globally.
So these edge locations as with the content are distributed globally as close to our customers as possible.
Now one of the things which is created along with the distribution is a domain name for that distribution.
And this looks something like this.
It always ends in CloudFront.net and it will be unique to every distribution which you create.
You can also take your own domain name and configure the distribution to use that alternate domain name.
In this case animalsforlife.org.
Once you've configured a distribution exactly how you like it you can deploy that distribution to the CloudFront network.
And what this actually does is push the distribution configuration to all of the chosen edge locations.
Meaning that these edge locations can now be used by your customers because they have the configuration stored within the distribution.
Architecturally in between the edge locations and the origin, remember this is where your content is stored.
So in between those two things are the regional edge caches.
Now they're bigger than the edge locations and generally support a number of local edge locations in the same geographic area.
So now let's assume that we have two customers who want to access our content hosted in our S3 bucket.
We have Julie and Moss and let's assume that they're in different locations but they're within the same continent.
Maybe Julie is in France on holiday and Moss lives in London.
So when Julie or Moss attempt to access animalsforlife.org both of them will be directed towards their closest edge location.
And both Moss and Julie in this example are looking to access the same object, whiskers.jpeg.
Now let's assume though that Julie attempts to access this object first.
So Julie makes the request for whiskers.jpeg and her local edge location is checked for this object.
Now if the object was locally cached in this edge location then the object would be returned immediately and the process would stop.
Julie would get the request returned quickly, her experience would be really positive and have high performance.
This is called a cache hit and this is a good thing.
Delivering content from a local edge location in all cases is what you want because it will always deliver better performance.
So faster speeds and lower latency.
If the object is not stored locally in an edge location then this is called a cache miss and this is a bad thing.
If this happens then Julie's edge location might check its closest regional cache and this is bigger so there's a bigger chance that the object will be stored regionally within this cache.
So the regional cache acts as a bigger cache for multiple edge locations and so if any local edge locations have accessed this object before then it's likely to be stored in the regional edge cache.
Now if the object is not in the regional edge cache then the process that happens next is called an origin fetch.
The content is fetched from the origin and this means that whiskers.jpeg is now stored on the regional cache.
The regional cache pushes the object back to the edge location which originally requested it which also means that it's now cached at the local edge location.
Once it's locally cached in the edge location then it's returned to Julie and all of this happens behind the scenes.
Julie has no awareness of the multiple step process which is occurring within the cloud front network.
If Julie or anyone else near that local edge location requests the object again then it can be delivered directly from that edge location.
This is a cache hit and this will offer improved access times so lower latency and better performance and this will occur from the second access onward whenever it's locally cached at the edge location.
But what about Moss? Well because Moss is accessing from a different edge location he would make his own request and this would contact his local edge location.
But because his local edge location doesn't have a cached copy of that object this will be a cache miss and so it also checks its regional edge cache.
It's checking for the whiskers.jpeg object.
This time though the regional edge cache does have the object based on the previous time when Julie accessed that object and so this time it's immediately returned to the edge location from the regional edge cache.
The local edge location that Moss is using caches that object and then immediately it's delivered to Moss.
Any customers accessing the object from a similar location to Moss will immediately get the object returned from the edge location again a cache hit and this is instead of using the regional cache or the origin.
So by deploying cloud front you can reduce the load on origins and get improved performance for your customers globally.
Now there are two important architectural things that you need to know about cloud front at this stage.
First it integrates with ACM or the AWS certificate manager so you can use SSL certificates with cloud front.
And also cloud front is for download style operations only. Any uploads go direct to the origin for processing.
Cloud front performs no write caching and that's important to understand because there are questions which test your knowledge of whether it does read only or read and write caching.
Now I want to elaborate on one thing at this point because it will make the lessons which follow a lot easier to understand.
I talked about distributions earlier in this lesson and I mentioned that they're the base configuration entity within cloud front.
But they aren't actually where a lot of the important configuration is stored, at least not directly.
That's actually contained within behaviors which themselves are contained within distributions.
A behavior is a configuration within a distribution. Think of it like a sub-configuration.
It works on the principle of a pattern match so let's look at this visually.
So we start with two users on the right connected to two edge locations and these edge locations receive their high level configuration via the distribution within cloud front.
At the other side on the left we've got the origins. It's easy to assume that origins are directly linked to distributions but that's not actually how it's architected.
Instead, origins are linked to behaviors which themselves are linked to distributions.
So behaviors architecturally sit in the middle between origins and distributions.
A cloud front distribution always has at least one behavior but it can have many more.
So all cloud front distributions start with the one behavior, the default behavior.
And this has a pattern of star which matches everything. It's a wild card.
But you can define other behaviors which are more specific and these take priority.
Let's assume that this architecture on screen is for the Categram application.
The default behavior is used for anything not matched by another behavior.
So low security things. But let's say that we have private images which are located in the top bucket and this is more heavily restricted.
Well we could define a second behavior matching a more specific pattern.
Let's say img forward slash star so anything accessed via the distribution which has a path of img forward slash and then anything else would use this other behavior.
Now the path pattern as the name suggests matches a certain path.
And this can allow us to have different configurations within a distribution.
So we can have different configuration options for different components of our cloud front distribution.
Things like TTL, policies, origins and even the public or private nature of cloud front may have been described as being set on the distribution.
But that's not actually entirely accurate. They're actually set on behaviors which are themselves part of a distribution.
So that's a quick refresher and over the remaining lessons in this section of the course I'm going to be focusing in on a few of the key bits of functionality provided by cloud front.
For now just go ahead and complete this lesson and when you're ready I'll look forward to you joining me in the next.
