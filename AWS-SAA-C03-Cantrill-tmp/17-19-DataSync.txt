Welcome back, and in this lesson I want to talk about an AWS service which you will use in the real world as a solutions architect.
And it's also one which starts to feature more and more in the exam.
Now that product is AWS DataSync. We've got a lot to cover, so let's jump in and get started.
Now AWS DataSync tends to feature in the exam currently in a very light way.
You might be lucky and not even have a question on it, but I do know that it features in at least two unique questions that I'm aware of.
And so you do need to be aware of what it is, what it does, and the type of situations where you might use it.
So DataSync is a data transfer service which allows you to move data into or out of AWS.
Historically, many of the transfer tasks involving AWS have either been manual uploads or downloads,
or have used a physical device like the Snowball or Snowball Edge series of transfer devices,
while DataSync is a service which manages this process end-to-end.
DataSync tends to be used for workloads like data migrations into AWS,
or when you need to transfer data into AWS for processing and then back out again,
or when you need to archive data into AWS to take advantage of cost-effective storage.
It can even be used as part of disaster recovery or business continuity planning.
Now as a product, it's designed to work at huge scales.
Each agent, and I'll introduce the concept of an agent later in this lesson,
but each agent can handle 10 gigabits per second of data transfer,
and each job, and I'll also introduce the concepts of jobs within this lesson,
can handle 50 million files.
Now this is obviously huge scale, very few transfer jobs will require that level of capacity or performance,
but in addition to that scale, it also handles the transfer of metadata,
such as permissions and timestamps, which are both essential for complex data structure migrations.
And finally, and this is a huge benefit for some scenarios, it includes built-in data validation.
Imagine if you're transferring huge numbers of medical records or scans into AWS,
you need to make sure that the data, as it arrives in AWS, matches the original data,
and DataSync includes this functionality as default.
Now in terms of the key features of the product, it is really scalable,
each agent can handle 10 gigabits per second of data transfer, which equates to around 100 terabytes per day,
and you can add additional agents, assuming you have the bandwidth to support it.
You can use bandwidth limiters to avoid the saturation of internet links,
so reducing the customer impact of transferring the data.
The product supports incremental and scheduled transfers, and it supports compression and encryption.
If you're transferring huge amounts of data and have concerns over reliability issues,
then it also supports automatic recovery from transit errors.
It also handles integration with AWS services, such as S3, EFS transfer,
so moving data from EFS to EFS inside AWS, even cross-region.
And best of all, it's a pay-as-you-use service, so there is a per gigabyte cost for any data that's moved by the product.
So let's quickly look at the architecture visually to help you understand exactly how it gets used,
and this is going to be useful for the exam.
In this example architecture, we have a corporate on-premises environment on the left,
and an AWS region on the right.
The business premises has an existing SAN or NAS storage device,
which has data on it that we want to move into AWS.
And so we install the DataSync agent on the business's on-premise VMware platform,
and this agent is capable of communicating to the NAS or SAN with either the NFS or SMB protocols.
So most SANs or NASs or any other storage devices will support either one or both of these protocols,
so the DataSync agent is capable of integrating with nearly all local on-premises storage.
Once DataSync is configured, the agent communicates with the DataSync endpoint running within AWS,
and from there, it can store the data in a number of different types of locations.
Examples of this include various S3 storage classes,
or VPC resources such as the Elastic File System,
or FSx for Windows Server.
Now you can configure a schedule for the transfer,
so either targeting or avoiding certain time periods,
and if you do have any link speed performance issues,
you can set a bandwidth limit to throttle the rate at which DataSync syncs the data
between your on-premises environment and AWS.
Now for the exam, it's just the architecture that you need to understand.
It won't feature at the level where you'll need to be aware of the implementation details.
So at a very high level, be aware that you need to have the DataSync agent installed locally
within your on-premises environment.
Be aware that it communicates over NFS or SMB with on-premises storage,
and then it transfers that data through to AWS.
It can recover from failures, it can use schedules,
it can throttle the bandwidth between on-premises and AWS,
and then from there, it can currently store into S3
the Elastic File System or FSx for Windows Servers.
So if you see any questions in the exam
which talk about the reliable transfer of large quantities of data,
if it needs to integrate with EFS, if it needs to integrate with FSx,
if it needs to integrate with S3 and support bi-directional transfer,
incremental transfer, schedule transfer,
then it's likely to be AWS DataSync that's the right answer.
Now let's finish up by reviewing the main architectural components of the DataSync product.
First we have the task, and a task within DataSync is essentially a job.
A job defines what is being synced,
how quickly, any schedules that need to be used,
any bandwidth throttling that needs to take place,
but it also defines the two locations that are involved in that job.
So where is the data being copied from, and where is it being copied to?
Next we have the agent, and I've already talked about this.
This is the software that's used to read or write to on-premises data stores.
So it uses NFS or SMB, and it's used to pull data off that store and move it into AWS or vice versa.
Lastly, we've got a location, and every task, so every job, has two locations,
the from location and the to location.
And examples of valid locations are network file systems or NFS,
server message block or SMB, and both of these are very common corporate data transfer protocols,
you tend to use NFS with Linux or Unix systems, and SMB is very popular in Windows environments.
Other valid locations include AWS storage services such as EFS, FSX, and Amazon S3.
Now that's all of the information that you'll need for the exam,
I just wanted to introduce the service because as I mentioned at the start,
I am aware that there are at least two questions involving this product which feature on the new architecture,
so if you do see Data Sync mentioned, you can at least identify whether it's an appropriate use of that technology or not.
You'll find that those questions aren't asking you to interpret different features of Data Sync.
You'll be asked to select between Data Sync and another product or another method of getting data into AWS.
And so in this lesson, I wanted to focus on exactly when you would use Data Sync.
So if you need to use an electronic method, obviously Snowball or Snowball Edge aren't appropriate.
If you need something that can transfer data in and out of AWS,
if that product needs to support schedules, bandwidth throttling, automatic retries, compression,
and cope with huge scale transfers involving lots of different AWS and traditional file transfer protocols,
then it's likely to be Data Sync that's the product that you need to pick.
With that being said, that's everything that you need to know for any Data Sync questions on the exam,
so go ahead, complete this video, and when you're ready, I look forward to you joining me in the next.
