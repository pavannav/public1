Welcome back and in this lesson I want to talk in detail about Amazon Kinesis data firehose.
Now this is one product out of the Kinesis product set which combined are designed to cope with large amounts of streaming, data ingestion, consumption and management within AWS.
Now it's important for the exam that you really understand the different situations when you would use each of the Kinesis family of products.
So let's jump in and explore data firehose in detail.
You learned in the last lesson that Kinesis Data Streams is a product which provides a way for producers to send huge quantities of data into AWS, storing that data for a window of time and then allowing multiple consumers to consume that data at different rates.
Now producers need to be designed to put data into Kinesis and consumers need to be designed to consume data from Kinesis.
What Kinesis by default doesn't offer is a way to persist that data. Once records in Kinesis age past the end of the rolling window then they're gone forever.
Now Kinesis Data Firehose is a fully managed service to deliver data to supported services including S3 which lets data be persisted beyond the rolling window of Kinesis Data Streams.
Data Firehose is also used to load data into data lake products, data stores and analytic services within AWS.
So Data Firehose scales automatically. It's fully serverless and it's resilient.
Firehose accepts data and it offers near real time delivery of that data to destinations.
Now this is key for the exam. It is not a real time product, it is a near real time product. So generally the delay is anywhere around the 60 second mark.
So it's not like Kinesis which offers consumers fully real time access to data which is ingested.
Firehose is near real time so remember that one for the exam.
Firehose also supports the transformation of data on the fly using Lambda.
Anything that you can define in a Lambda function can be done to data being handled by Firehose but be aware that it can add latency depending on the complexity of the processing.
Now Firehose is a pay as you go service. You'll build based on data volume passing through the service.
So it's a really cost effective service which handles the delivery of data through to supported destinations.
So let's look at the architecture of Firehose visually.
We start with Kinesis Data Firehose in the middle.
The end result of Firehose is to deliver incoming data through to a number of supported destinations.
Now these are important for you to remember for the exam.
You need to be able to pick if Firehose is a valid solution and for that you need to know the valid destinations for the service.
So it can deliver data to HTTP endpoints which means it can deliver to third party providers.
It directly supports delivery to Splunk.
It can deliver data into Redshift.
It can also deliver data into Elasticsearch and then finally it can deliver data into S3.
Firehose can directly accept data from producers or that data can be obtained from a Kinesis data stream.
So if you already have a set of producers adding data into a Kinesis data stream then we might want to integrate that with Firehose.
Remember these producers are adding data into the Kinesis stream.
That data is available in real time by any consumers of that stream.
But Kinesis offers no way to persist that data anywhere or no way to deliver it natively to any other services.
But what we can do is to integrate the Kinesis data stream with the Kinesis Firehose delivery stream.
That data is delivered into Firehose in real time.
Now producers can also send data directly into Firehose if you have no need for the features that Kinesis data streams provide or you just want to use Firehose directly.
In any case Firehose actually receives the data in real time.
But this is where that changes. So even though Firehose receives data in real time, Firehose itself is not a real time service.
Kinesis data streams are real time but Firehose is what's known as a near real time service.
What this means in practice is that any data being handled by the service is buffered for delivery.
Firehose waits for one MB of data or 60 seconds.
These can be adjusted but these are the general minimums of the product.
So for low volume producers, Firehose will generally wait for the full 60 seconds and then deliver that data through to the destinations.
For high volume producers, it will deliver every MB of data that's injected into the product.
So even though Firehose gets the data in real time, it doesn't deliver it to the destination in real time and that's essential to remember for the exam.
So if there are any answers which involve Firehose, it cannot be a real time solution. It can only be near real time.
So from an AWS perspective, something in the range of 200 milliseconds would be a real time product but something in the range of 60 seconds would be classified as near real time.
And you need to get a feel for the differences between those two and which products fit into which of those categories.
Now Firehose can actually transform the data passing through it using Lambda.
So source records added to Firehose are sent to a Lambda function and functions can be created from blueprints to perform common tasks and then transformed records are sent back for delivery.
But this can add to the latency of data flowing through the product.
And if you do decide to do a transform, then you can optionally store the unmodified data in a backup bucket which you define.
Once the buffer or time buffer passes, then data is passed into the final destinations, so transformed REC or HTTP endpoints.
The only exception to this architecture for delivery is when you're using Redshift.
What happens with Redshift is it uses an intermediate S3 bucket and then runs a Redshift copy to bring the data from S3 into the product.
So even though conceptually it's direct, when used you're actually copying data to an intermediate location, an S3 bucket, and then you're running the copy command to pull that data into Redshift.
And that's handled all end to end by the data Firehose product.
Now there are a few common situations where Firehose will be used.
You might use it to provide persistence to data coming into a Kinesis stream, so providing a permanent storage of data that comes into a stream,
so it's not lost when it exits the rolling window that Kinesis data streams provide.
Or you might use it if you want to store data in a different format, because Firehose can transform it using Lambda.
Or you might want to deliver data that comes either directly into Firehose or via DataStream into one of the supported products.
But just keep in mind though, it is not real-time. I need to stress that for the exam. It's only near real-time.
You trade the fact that you don't have to build this yourself, so you don't need to put in the effort to build this solution, but what you lose is the real-time nature of Kinesis.
If you need a solution which handles data in real-time, then you need to stick to Kinesis and use something like a Lambda function to handle what to do with that data,
say delivering it in real-time to Elasticsearch.
Now with that being said, that is everything I wanted to cover in this lesson.
For the exam, you just need a good architectural overview of how the Firehose product works and some of the scenarios which it might be used for.
So really try to focus on these core concepts, exactly what Firehose does, try to commit to memory that it's only a near real-time product,
and make sure that you remember all of the supported destinations.
Now thanks for watching, go ahead and complete this lesson and then when you're ready, I'll look forward to you joining me in the next.
