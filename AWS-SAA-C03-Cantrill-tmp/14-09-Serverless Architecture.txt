Welcome back, and in this lesson I want to cover the serverless architecture.
Serverless is a type of architecture which is relatively commonplace within AWS,
mainly because AWS includes many products and services which support its use.
The key thing to understand about the serverless architecture, aside from the fact that there are really servers running behind the scenes,
is that it's not one single thing.
Serverless is an architecture, but it's more a software architecture than a hardware architecture.
The aim with the serverless architecture and where its name comes from is that as a developer or an architect or an administrator,
you're aiming to manage few, if any, servers.
Servers are things which carry overhead, so cost, administration and risk,
and the serverless architecture aims to remove as much of that as possible.
In many ways, serverless takes the best bits from a few different architectures,
mostly microservices and event-driven architectures.
Within serverless, you break an application down into as many tiny pieces as possible,
even beyond microservices, collections of small and specialized functions.
These functions start up, do one thing really, really well, and then they stop.
And in aid of the US logically, because of this, Lambda is used.
But there are other platforms, such as Microsoft Azure, which has their own equivalent, namely Azure Functions.
From an architecture perspective, the actual technology which is used is less relevant.
Now these functions which make up your application, they run in stateless and ephemeral environments.
And why this matters is because if the application is architected to assume a clean and empty environment,
then these functions can run anywhere.
Every time they run, they obtain the data that they need, they do something,
and then optionally, they store the result persistently somehow, or they deliver that output to something else.
The reason why Lambda is cheap is because it's scalable.
Each environment is easy to provision, and each environment is the same.
So the serverless architecture uses this to its advantage.
Each function that runs does so in an ephemeral and stateless environment.
And another key concept within serverless is that generally everything is event driven.
This means that nothing is running until it's required.
Any function code that your application uses is only running on hardware when it's processing a system or customer interaction, an event.
Serverless environments should use fast products such as Lambda for any general processing needs.
Lambda, as a service, is built based on execution duration,
and functions only run when some form of execution is happening.
Because serverless is event driven, it means that while not being used,
a serverless architecture should be very close to zero cost until something in that environment generates an event.
So serverless environments generally have no persistent usage of compute within that system.
Now where you need other systems beyond normal compute,
a serverless environment should use, where possible, managed services.
It shouldn't reinvent the wheel.
Examples are using S3 for any persistent object storage,
or DynamoDB, which we haven't covered yet, for any persistent data storage,
and third party identity providers such as Google, Twitter, Facebook,
or even corporate identities such as Active Directory instead of building your own.
Other services that AWS provides, such as Elastic Transcode,
can be used to convert media files or manipulate these files in other ways.
With the serverless architecture, your aim should be to consume, as a service, whatever you can,
code as little as possible, and use function as a service for any general purpose compute needs,
and then use all of those building blocks together to create your application.
Now let's look at this visually, because I think an architecture diagram might make it easier
to understand exactly what a serverless architecture looks like.
So let's step through a simple serverless architecture, and we're going to do so visually.
And I want your default position to be that unless we state otherwise,
you're not using any self-managed compute.
So no servers and no EC2 instances unless we discuss otherwise.
So that should be your starting position.
And at each step throughout this architecture, I'll highlight exactly why the parts are serverless and why it matters.
Now we're going to use a slightly more inclusive example.
This time we're going to use PetTube.
There's an uproar about CatTube only being for cats, and so it's rebranded to be a little bit more inclusive.
So to start with, we've got Julie using her laptop, and she wants to upload some woofy holiday videos.
And so to do that, she browses to an S3 bucket that's running as a static website for the PetTube application.
She downloads some HTML, and that HTML has some JavaScript included within it.
Now one crucial part of the serverless architecture is that modern web browsers are capable of running client-side JavaScript inside the browser.
And this is what actually provides the front end for the PetTube application,
JavaScript that's running in the browser of the user that's downloaded from a static website S3 bucket.
So at this point, the application has no self-managed compute that's being used.
We've simply downloaded HTML from an S3 bucket with some included JavaScript that's now running in Julie's web browser.
Now PetTube uses third-party identity providers for its authentication.
Like all good serverless applications, it doesn't use its own store of identity, its own store of users.
Its lower admin overhead, and also remember there's a limit on the number of IAM users that can exist inside one AWS account.
That's 5,000 IAM users per account.
And so if we used IAM users for authentication, then PetTube would be limited to 5,000 users,
and each user of the application would need one additional account.
So one additional username and one additional password.
So instead of doing that, we use a third-party identity provider and one that our users are already likely to have an account inside.
So that reduces the number of accounts that our users are required to maintain.
So the JavaScript that's running in Julie's browser communicates with the third-party identity provider,
and we're going to assume that we're using Google.
And you'll have seen the screen that's generated if you've ever logged into Gmail or anything that uses Gmail logins,
but this could just as easily be Twitter, Facebook, or any other third-party identity provider.
The key thing to understand is that Julie logs in to this identity provider.
It's this identity provider that validates that the user claiming to be Julie is in fact Julie,
so it checks her username and password.
And if it's happy with the process or if it's happy with the username and password combination that Julie's provided,
then it returns to Julie an identity token.
And this token proves that she's authenticated with the Google identity provider.
Now, AWS can't directly use third-party identities,
and so the JavaScript that's running in Julie's browser communicates with an AWS service called Cognito,
and Cognito swaps this Google identity token for temporary AWS credentials,
and these can be used to access AWS resources.
So the JavaScript in Julie's browser now has available some temporary AWS credentials that it can use to interact with AWS,
and so it uses these temporary credentials to upload a video of Woofie to an S3 bucket.
This is the original bucket of our application, the bucket where the master videos go that our customers upload.
Notice that so far in this process, no self-managed compute, no servers have been used to provision this service.
We've performed all of these activities without using any compute servers or compute instances
that we need to manage or design as solutions architects.
It's all delivered by using managed services, so S3, Cognito, and the Google identity provider.
Now, when the Woofie video arrives inside the original's bucket, that bucket is configured to generate an event.
That event contains the details of the object which was uploaded,
and it's set to send that event to and invoke a Lambda function to process that video.
That Lambda function takes in the event, and it creates jobs within the Elastic Transcoder service,
which is a managed service offered by AWS which can take in media and manipulate that media,
and one of the things that it can do is to transcode the media,
so generate media of different sizes from one master video file.
So multiple jobs get created, one for each size of video that's required.
The Elastic Transcoder gets the location of the original video as part of the initiation of the job,
and it loads in that video at the start of each job processing cycle.
So each job outputs an object to a Transcode bucket,
so one object for each different size of the original video.
In addition, details on each of the new videos are added to a database, in this case DynamoDB.
Now again, at this stage, notice that we still have no self-managed servers.
The only resources that are consumed are storage space in S3, DynamoDB,
and any processing time used for the Lambda function, and any Elastic Transcoder jobs.
With this architecture so far, we've allowed a customer to upload a master video,
we've transcoded it into different video sizes,
and at no point have we consumed any self-managed compute.
No EC2 instances or no other long-running compute services.
It's all managed services or compute that's used in Julie's browser.
Now the last part of the architecture is where Julie,
by clicking another part of the client site that's running inside her browser,
can interact with another Lambda function, and we'll call this MyMedia.
And this Lambda function will load data from the database,
identify which objects in the transcode bucket are Julie's,
and return URLs for Julie to access.
And this is how Julie can load up a web page,
which show all of the videos that she's uploaded to the PetTube application.
Now this is a simplified diagram.
In reality, it's a little bit more complex.
For example, API Gateway would generally be used
between any client-side processing and the Lambda functions,
but conceptually, this is actually how it works.
We've got no self-managed servers, we've got no self-managed database servers,
we've got little, if any, costs that are incurred for base usage.
It's a fully consumption-based model.
It consumes compute only when it's being used,
so when events are generated, either from a system side or a client side,
and it uses third-party services as much as possible.
Now there are many third-party services to choose from,
and you can never expect to know them all end-to-end.
The key thing to understand about serverless is the way to do things,
and I've covered that in this lesson.
Later in the section, you'll experience how to implement a serverless application
within the demo lesson called PetCuddletron,
and this will show you how to implement a serverless application
just like the one that's on screen.
It's slightly less complex,
but it's one that uses many of the same architectural fundamentals,
and it should start to really cement the theory that you're learning right now.
Now before we move on to this demo,
there are a few more services that I need to cover
which the PetCuddletron demo lesson will utilise.
So for now, that's it for this lesson.
Thanks for watching.
Go ahead and complete this video,
and then when you're ready, I'll look forward to you joining me in the next.
