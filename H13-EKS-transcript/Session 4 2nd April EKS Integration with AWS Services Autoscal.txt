ript


12:39
So hello everyone good afternoon. Let's first quickly revise and then Wimlso
12:45
will join for the session. Just a second I'll share my screen.
12:51
Okay I hope my screen is visible to you. So in the previous session we went more
12:58
deep dive into uh EKS CPL tool and the cubernetus okay so you know every
13:05
cluster has some configuration file and in case of kubernetus we have a cube config file okay and this are also
13:13
discussed about that cubectl is a command that is for kubernetus not for eksctl tool so on the top of eksctl we
13:21
used kubernetus okay So whenever we create a pod one of the responsibility of pod is to launch
13:28
an app. Okay. So you we did uh we launched one app uh in the previous
13:34
session of this and we exposed uh the service as well for that. So if so uh
13:40
sir went to the same cluster which s created in the last week and sir deleted the that cluster. Okay. So you know the
13:49
how to to get a cluster you can use the command as ekl get cluster and to delete
13:55
the cluster you need to give the command ekl delete cluster name that is the name
14:01
of your cluster okay so you have the name as existing wardrobe this something
14:06
like this and you have to give the name of that cluster which you want to delete okay so this was the command for
14:12
deleting the cluster after that uh you know very well s already disced first
14:18
everything how to create a cluster. So to create a cluster you need to use o uh
14:24
ekctl create cluster and then the name of the cluster and you if you remember
14:30
sir also told about uh how to use enable ssm. So you need to give a hyphen enable
14:37
ssm while creating the cluster. Okay. So this was created and after that
14:44
application needs storage. So application needs a storage that is either permanent or you can say the
14:50
persistent storage. So it is the duty of the pod to request which is referred to
14:55
as a claim for persistent volume. Okay. And this concept is known as persistent
15:00
volume claim that is PVC. So uh there are two ways to create that. One is
15:08
manually and the other is dynamically. Okay. And here you if you uh remember
15:14
that we used OC get SC command. So what is SC here? So SC is the storage class
15:21
which helps to provision storage dynamically. Okay. And SC is a kind of resource type. So you can see that for
15:28
dynam dynamic provisioning we need storage class and it is a kind of
15:35
resource type. Okay. So we can either get it from worker nodes that is local. We can either get it from NFS. So for
15:42
that we can use EFS service of AWS. And the other thing is we can use EBS that
15:48
is block storage. Okay. So pod is the one who is responsible to run the app.
15:54
Okay. And you can see this diagram. Uh so we have a storage class and we need a
16:00
story container storage interface. So from where we can get the container storage uh interface either from the
16:07
public cloud, private cloud or you can use on premises. Okay. And in case of
16:13
public cloud you can use AWS and you can use a service as EBS, EFS and many more.
16:20
Uh apart from that uh if you want to use private so you can use open stack and
16:25
sender uh s to discussed about that and for on premises you can use block sand
16:32
and for uh for example you can use dell ec okay product like this so I hope till
16:39
now it is clear after that uh in kubernetes there is one standard interface for the storage so if
16:46
container storage wants to connect with EDS then we need to give a driver. Okay. So it is very important that uh if we
16:53
want EBS to connected uh EBS connected with the container so we need to give
16:59
the driver. Okay. And or the you can say the provisioner. Okay. So in AWS that
17:05
driver is known as CSI provisioner. Okay. So in our case we downloaded that
17:11
uh in the rest part I'll tell you what how we downloaded. So uh Kubernetes only understand PVC PVC. So you know what is
17:23
okay. So if we want cubern it is resource type take from third party. So
17:29
for that we use CSI. Okay. So we need to use the driver or the provisioner. Okay.
17:34
And in our case it is CSI driver. Okay. So storage class is the one that use
17:40
driver from external provisioners. If we run our cluster in KS with EKS service,
17:47
it will launch KS and whenever EKS want storage, it will connect to the storage
17:52
class internally. Okay, that means it is integrated internally with the service
17:57
of AWS like EBS, EFS. Okay, so I hope this pointer is clear to you. So after
18:04
that if we want to connect to a cluster then the then um the credentials can be
18:10
found in the file called cube config file. Okay. And uh we have PV uh which
18:16
is stored which is for which is used for storage and SC is the storage class.
18:21
Okay. So as soon as we launch a pod in EKS and that pod needs a storage. So
18:28
internally EKS connect with the EBS service and launch a new volume. So you
18:34
can see internally your whenever we create you can see in the volume you you have a volume precreated.
18:41
Okay. So and everything is possible because of the driver. So we gave the driver that is why everything was
18:47
possible. So uh I covered the theoretical part first and then we will
18:52
move to the practical. Okay. Every resource type has a notations. So here
18:57
in the down screenshot I will show you where what what is the purpose of annotations and where we can find
19:03
annotations. So which give different responsibilities. So in annotations
19:08
there are keywords like parameters and uh you can see I'll show you a notation
19:14
is used for all resource types. So for every resource type you can see a notations for that and uh in storage
19:22
class we need a driver uh that is CSI driver. So it is a program which runs
19:27
inside the pod in Kubernetes. Okay. So after that all internal drivers of KS
19:34
is managed in a area called the namespace. Okay. So all the internal drivers are managed in the area named
19:41
namespace. Okay. One of the namespace is cube system. So we will see the use of
19:48
cube system. So it is a kind of namespace. Okay. So whenever EKS launch
19:53
it creates role and attached to the EKS. Okay. So config map is a resource which
20:01
like which is like a document and maintains the config file. So we will also see where we use the config map in
20:08
the upcoming commands which I'll show you via the screenshots. Okay. So there were two ways to install the driver.
20:15
Either you can use add-ons or you can use a self-managed add-on which is a
20:21
kind of script and that is written in K test scripts you can say. Okay. So but
20:27
the standard way is uh for the drivers is by using CSI. Okay. So sir showed you
20:34
the why CSI is required. So I put this screenshot here for your reference.
20:40
After that what we did we checked how many clusters we had. So we checked
20:47
eksctl get cluster and then we got the clusters running at that time. After
20:53
that uh we run the command cubectl get pods to know how many pods we have. So
20:59
you can use cubectl get pods to check how many pods. Okay. After that uh we uh
21:05
checked PVC. So since we haven't created anything so we don't find any PVC by
21:12
using this command that is cubectl get PVC - cube config and then the
21:18
cubecluster config. So these commands were already discussed so I won't be uh
21:24
taking these commands. So uh this all the commands were discussed in the
21:29
previous sessions. So this was the repeat thing and after that u we uh for
21:35
checking the storage class we use the command cubectl get sc and then you have
21:41
to give the cube config and then the file name. Okay so you know by default
21:46
we have a GP2 okay so we have GP2 and we can also describe what is this. So we use the
21:53
cubectl describe storage class that is SC for GP2. Okay. And you can see here a
22:00
notations are there. So notations will help cubernators internally for getting
22:06
the metadata things and uh discussed about that it is for using metadata and
22:12
uh you can see that here is a metadata and you can get everything uh related to
22:18
that uh storage class GB2 here in the annotations. Okay. So, uh we described
22:24
the storage class GP2. After that, we use cubectl get pods command and uh we
22:30
didn't find anything in that name space. So, uh after that we use cube system
22:37
nameace to get the pods. Okay. So, we use cubectl get pods n and then the name
22:44
space that is cube system and rest is the same. So you can see that uh this
22:49
was running AWS node and then uh cube proxy everything was running. So this we
22:54
tested the command by using this okay to get ports. After that we use cubectl get
23:02
sc and then this command and we got this as a GP2 as a default same command
23:07
repeated. Okay. After that uh we check the config map. Okay. So to get uh to
23:14
get the config map you have this uh cubectl get config map and then you have
23:20
to go to the name space that is cube system and then the cube config file. Okay. So you will get the cube config
23:28
map here. After that um we have the name as AWS O. So we uh use cubectl get
23:37
config map AWS which is in the name space cube system. So you can see that
23:43
we have name as AWS or and the data and then you can see the 37 minute uh m.
23:50
Okay. After that u we if you want to describe the config map that is AWS o if
23:57
you want to describe so we use this command cubectl describe config map. So you can see that to describe you can you
24:04
don't have to remember everything you can also take help. Okay. So uh now the
24:10
uh main part is so after that uh what s did sir uh created one role by attaching
24:16
a permission. Okay. So we go to the IM service and then we clicked on to the
24:21
roles and we use this already which was create uh the node group which was already created in our cluster. So we
24:29
choose this and we attached a policy by clicking on uh you can say that at
24:35
attach policy. So we attached the policy of uh Amazon EC2 full access. Okay. So we
24:44
clicked on to Amazon EC2 full access and after that we added the permissions.
24:49
Okay. So after adding the permissions you can see uh here it shows you the everything
24:57
what is there in the Amazon EC2 full access in the form of key value pairs. Okay. So after that what we did we used
25:07
EKS get add-on for that cluster one. So add-on was also discussed in the
25:12
previous sessions. So I hope you know what is add-on. Okay. So to download the
25:18
driver. Okay. So sir gave you the link also and we used AWS EBS CSI driver in
25:27
our case. So we uh clicked on to this um okay charts AWS EBS CSI driver and after
25:34
that we use this driver installation. Okay. So our driver got installed.
25:40
After installing the driver we use this command cubectl apply and we use this uh
25:46
GitHub link. Okay. So to create that uh so since we downloaded the CSI driver so
25:52
we were able to use this okay GitHub link and we use this command as cubectl
25:58
apply and uh the GitHub link okay after that uh we uh did cubectl get pods. So
26:07
you can see after uh using this command we were able to see the ABS CSI
26:12
controller and the node for and also we can use the SC uh to check how many
26:18
storage class we have. So right now we we were having GP2 okay only which was
26:25
the default one and but the provisioner you can see that AWS EDS provisioner was
26:31
there okay so after that we describe this SC uh that is GP2 uh okay uh which
26:38
was already described in the previous screenshot you can see and um to get the
26:44
pods you know the command cubectl get pods okay so we use this command so we
26:50
have the EBS CSI controller and you can see the state was ready and the status was running. Okay. So after that uh we
26:59
used uh sir used one uh code and I have also mentioned the link of that code
27:05
which was already there in GitHub account. So you can use that code. Okay.
27:11
So we use cubectl apply and then we use this GitHub link for the creation of EBS
27:18
SC. So since we don't have the storage class so you can see uh now we were able
27:24
to get the storage class here with the name as EBSS SCC okay and the
27:29
provisioner is edscsi.awws.com aws.com. Okay. So I hope it is clear about
27:36
storage class. Then we move to uh this uh then we see the uh by using the
27:42
describe command about this EBS SC storage class. So we were able to see
27:47
the notations and the provisioner about that. So I think it is clear to you. So
27:54
we um after that we compared the both the two like the default one that is GP2
28:01
and the one which we created that is EBSSC. So I gave you both uh uh commands for
28:08
that. Uh then we uh we looked to the uh we went to the file and we s discussed
28:16
few pointers here that is we if you want to give the type you can also change here. So since we have GP2 then we give
28:23
GP2 and we have the provisioner as EBS CSI. AAWS. So we uh have the provisioner
28:29
as this. Okay. So after that if you want to edit this um file EBS SC so you can
28:38
use this command. Okay. Cubectl edit SC and the storage class name. Okay. After
28:44
that uh we created one file named my SC.L IML and we gave the API version
28:51
kind that is storage class and the metadata name as EBSC and we gave the provisioner as well as
28:59
uh we use the everything which was uh written in the file you can see. So after doing uh after creating this file
29:06
we used the command cubectl apply and the file named as my st.imml
29:12
which created the storage class. Okay. So you can see this uh we created this.
29:18
So uh now you can see that we have a GP3. Okay. Here and same command uh for
29:27
uh creating the PV. Okay. So we use the this GitHub link for creating the PVC.
29:33
So every link uh I provided you in uh after this end of this document. Okay.
29:39
So you can use that. So to get PVC uh you have to use cubectl get PVC command
29:46
and uh uh also you can use uh to create a secret we use this uh link that is u s
29:54
links link okay so for testing you can use cubectl get secrets so you just need
29:59
to follow these steps and use that code okay after that uh you know whenever we
30:06
create the PVC uh so we uh earlier when you check in the AWS portal. So you had
30:12
eight sir had eight volumes but after running this you know command PVC
30:17
cubectl apply. So we got nine volumes and the name of the volume which created
30:23
was this. You can see this was the volume created with the type GP3 and the
30:28
size as 4GB. Okay. So you can see and you can also see that
30:34
volume state was in use. Okay. And after that you use cubectl get PVC command to
30:41
check how many PVC you had. How many PV if you want to check you can use cubectl
30:46
get PV command and the rest you can see for the pods you can use cubectl get
30:51
pods. Okay. So I hope it is clear. So you can also check your cluster was
30:56
running. Okay. So I put the screenshot for that. And we have the volume attached here. Okay. So you can also
31:03
check that this 4 GB volume was attached to the cluster. Okay. So then uh we have uh like you
31:11
know pod become the consumer of PVC and one EBS can connect to only one single
31:17
pod. Okay. And multi-AZ is not possible with EBS. So sir uh asked you to go with
31:26
uh for multia uh you know requirement you can use EFS for that uh EFS uh
31:32
service of AWS you can use for that requirement. Okay. So you know annotations which I discussed here. So
31:39
that is used for a machine and it is a kind of uh you can say it gives you the metadata. So KS can internally get lot
31:47
of information with the help of annotations. Okay. So here I provided you the link. So you can go to this link
31:55
and use okay uh for creating the secret we use this comm uh this file. For PVC
32:02
we use this file and for uh SVC we use this file. So I think sir already shared
32:09
the link but at the same time I also shared you in this document. I hope it is clear. If you have any doubts you can
32:16
ask in the chat box otherwise uh so we'll continue to the rest part. Thank
32:21
you.
32:26
uh Kab uh it's already been shared in the discord also it is uploaded in the hash3 so you can uh get it from there
32:37
Jendra you can download it from your discord okay so I'll reshare this uh in
32:43
the discord channel otherwise we have already shared last week itself
32:49
Okay.
32:57
Uh Ganvi. Okay. Uh if you have missed uh filling the form, then I'll ask my team
33:04
if they could share the form again. So you can join the discord channel for that. Okay.
33:10
Uh yes, what I said, I we will share you the form again and you need to fill that form then only you will be added to the
33:17
discord channel. now.
33:27
Okay. Manu uh I'll ask one of our team member uh to look why you are unable to
33:33
download. Okay.
33:42
No ques you don't have I think access. Oh yes. So for that you will be also u
33:50
there in the discord I think.
33:55
Okay Quesv uh we'll look for earth one and earth two learners how can we share the pdf of this either we can share in
34:02
the group or we will uh also add you guys in the discord channel.
34:13
Uh yes pankos you can ask uh doubts of the previous session as well if you have Okay.
34:46
uh Wenesh uh it is not possible right now but yes uh I can say that uh we will be share if we can we will share you the
34:53
uh way how you can download the process how you can download so you can follow
34:59
that okay we'll share it in your in your respective group
35:21
Paris which batch you are a part of if you can tell me I mean if you are uh
35:27
with a role with EKS then uh it's there in your portal So you can watch your uh record we can
35:35
you can watch your recorded sessions there. Okay.
35:44
Yes.
36:13
I think pankers that won't affect your uh this thing uh your web server but uh
36:19
you know whenever you will uh start that your IP will got changed so you need to
36:26
change all the details in that file as Well, and then you can able to run your web server and your WordPress whatever
36:33
you asking for.
36:38
So if you stop it won't do anything.
36:49
So okay for that I need to look your file and tell you what you have to change in that then you can proceed.
36:57
Okay. So I think you have to change your IP for that and uh rest is same. So try
37:03
changing the IP and then test if it works then good. Otherwise if you still
37:08
unable to do you can ping us or you can ask in the next session.
37:18
Yeah. Yes. Wangadesh. So I think you uh are able to download.
37:27
Yes. Okay. Great.
37:47
A link for download can't be shared. you know you have to go to the discord channel and then you have to uh download
37:54
that PDF from discord itself chendra so we don't have any link for that you have
38:00
to go to the discord channel and then you have to download that PDF okay and if you are unable to download uh you can
38:07
ask our team
38:12
uh as I said we don't share you the discord channel link we shared it earlier and that link got expired So for
38:19
that we create uh we shared a form. For that you need to fill that form and then you would be added to the discord
38:25
channel. So wait for the form. Our team will share and uh if you are not a part
38:32
of discord then you have to fill that form and you would be able to join that channel after that.
38:49
Yes, Manu. I'll ask my team to reshare the form. But uh please note that uh uh
38:56
you have to join. If we share it again, then everyone needs to join because everything is uh you know you can ask
39:02
your doubts there only.
39:15
Uh yes, punk. again.
39:36
Uh Ganvi in your respective WhatsApp group it will share.
40:12
Okay. So the channel name uh someone is asking for that. So we have the name as
40:18
AWS EKS advance. So you can check if you're a part of uh EKS advanced
40:24
training then you should be the part of uh AWS EKS advanced channel in Discord.
40:31
Okay. Yes, Manu if you're not a part of that
40:37
then we will be sharing you the form again and you have to fill that form then only you will be added to that
40:42
channel.
40:59
uh myth uh I mean if you no it's not covered I think uh that is why we are
41:04
sharing you the links for that so if you want to learn advanced EKS then you can
41:10
join otherwise it's your choice I mean it's new thing that is why we shared so
41:19
sir we'll be joining in two to three minutes meanwhile you can ask okay so sir joined over to you sir thank you.
41:44
Hey guys, uh welcome back and let's continue with the next part. Uh so last
41:50
few classes guys, we have discussed about EKS, right? And uh we have done
41:56
lots of concepts uh to be discussed on the EKS and we have done this right. So
42:01
let's go to the next level right in the EKS as a service right and now you I
42:07
believe you understood the EKS itself individual big service right uh it
42:13
contain lot of thing so uh again the uh difference EKS is service who going to
42:21
set up the kubernetes for you after kubernetes has been set up now the
42:29
entire skills of kubernetes that you know or you might know will come up right so whatever the cubernetes can do
42:34
for you you can achieve by uh this setup right so what EKS is doing you guys know
42:40
EKS is is providing you all right the internal infrastructure of AWS
42:48
okay or I can say EK service providing you whatever cuber as a cluster need to
42:57
um for their internal requirement Right for example as EKS service is going to
43:04
help you to integrate with EC2 that we have seen VPC we have seen EBS we have
43:09
seen IM also sometime we have seen right the policy all the things rule
43:14
okay and many more like ELBM we have seen right or EFS also many more other
43:19
services also the point is we now you don't have to do it by yourself they have a internal integrations or internal
43:28
coupled that is called kind of type tightly coupled uh way to connect those
43:34
dots right so we have discussed all those point right uh in detail but to
43:39
implement the EKS either you can use AWS CLI tool graphical way or you can use e
43:47
EKTL command right so EKC is a command uh we can use also that's what we have
43:53
used till date okay so guys today's plan is there's multiple topics IC we are
44:00
going to learn today again very very interesting and very important also uh one topic that we are going to start
44:08
today is more about autoscaling okay and now there's two moto of
44:14
autoscaling in with the respect to EKS services
44:19
okay so both the things we will discuss today so what I mean by this if you want
44:26
to autocale Okay, if you want to autoscale, what
44:31
autoscale? We also discussed. Okay, but if you want to autoscale your cluster that is your worker node.
44:38
Okay, so node. So there's a different concept all together. That is here the
44:44
role of EKS service come in play. Okay, or do you want to scale your ports run
44:51
inside the cluster? There's a concept of Kubernetes come in
44:56
play. this component of Kubernetes called HPA that we'll very quickly see today.
45:03
So there's two different prospective autoscaling. Uh one is the worker node I want to
45:10
autoscale or do you want to autoscale the port? So we'll see both the thing uh
45:15
today. So this is one of the topic we will going to discuss today. uh which is
45:20
topic number maybe this can be first topic we'll see this can be second topic we'll see and uh one thing is uh EKSCl
45:29
is a great tool okay and they give you lots of capabilities
45:35
okay or I can say facilities or features to customize your cubernetes cluster the
45:41
way you want to customize right and for this they have a yl files so I have show you some of the demonstration of yl file
45:47
in my insl classes But we go more deep into the YML file. There's lots of things you can do with the KCTL YML file
45:56
to customize your cluster. Okay. So if the D suggest we will also uh see this
46:02
very detailed advanced uh YML code for EKCTL commands. Right. So there are
46:08
three topics guys we have planned for today. Right? Everything I've explained you step by step to give you the clarity
46:15
and the use cases and how to set up and demos all the things we'll see. Right. Last class was very important if you
46:21
miss it. Uh in my last class we talk about EBS um integration with uh with uh
46:29
Kubernetes or EK service but it is not about the integration of uh EBS. It is
46:35
the concept I explained to you there's a core concepts. So what I mean by this if you want to
46:41
integrate with other service also almost the same process we going to use. All right. So topic was EBS but it give
46:50
you the clarity that if you want to integrate with let's say um EMR services
46:56
Hadoop services or you want to integrate with lambda service if I integrate with E EFS services right so what is the
47:03
normal typical approach we should use okay so my last class was talking about
47:08
that perspective right the demo that I showed you is about EBS your blog stoages uh but the the concept and the
47:18
approach that I explained you in my last class was about how to integrate EKS
47:24
services with any other services uh of your AWS cloud.
47:30
Okay. So let me explain you. Okay. U before I
47:36
start this topic because to show this topic I need a cluster. My cluster is still running. Okay. By the meantime my
47:47
this cluster is up. I would like to save our time right now and let me start
47:52
discussing about this topic called autoscaling of your cluster worker node.
47:58
Okay. So, so what this topic is all about. So you
48:05
guys know about the cluster right? So we have a master node because a fully managed services. The master node is
48:12
more like a serverless environment. in your uh in your
48:18
uh EKS services. Okay, it means AWS is saying the entire
48:25
management of the master node we will handle. I won't show you it is running somewhere
48:31
behind the scene. Okay, if more and more requests come from the customers or the
48:37
users come to master node. So if let's say we running with one or two or three
48:44
master nodes and after we found that all the master node is highly busy, highly
48:49
occupied, lot of load is there. Okay. So you don't worry about it. We will
48:55
increase the master node automatically.
49:00
If you think more load going to come up, we increase the master node automatically.
49:06
Okay. So adding anything or maybe removing anything is known as scaling.
49:14
We scale more and more computer come to four coming to five. Okay. But if you are adding the
49:20
computers okay they are mostly known as horizontal scaling not the vertical horizontal
49:27
scaling but sometime if you use the word scaling mostly it means horizontal scaling only.
49:33
Okay. If you want to increase the computer they are known as scale out.
49:39
So do you want to scale out means adding more computers or more nodes or if
49:44
you're decreasing the computer maybe you don't have a more now load. So there's no meaning to run five computers to pay
49:51
for five computer for something like this right. So if your lesser load is coming, lesser requirement is coming,
49:57
lesser request is coming, you can decrease it. Decreasing the resources or the node of computer are known as scale
50:04
in. It's not scale up and down. Scale up and down is a different concept. This is
50:09
scale out and scale in in the horizontal scaling world. Okay. So what AWS is saying? Okay. If
50:17
the load increase or decrease, scale out, scale in, I will do for you behind the scene. How? Automatically.
50:25
Okay. So that is called auto scale. Based on the load, I keep on checking
50:32
RAM. I keep on checking um CPU. I keep on checking network bandwidth. Okay.
50:38
whichever goes high and you need more RAM or more CPU note network bandwidth
50:43
or maybe IO speed of hard disk I will autoscale for you. So my point is you
50:49
don't have to worry about the master node. I will handle everything for you. Who is saying AWS guys is saying okay so
50:57
autoscaling uh we don't have to worry about the for
51:02
the master node and you guys know master node is also known as control plane.
51:08
So we don't have to worry about the autoscaling for the control plane. AWS will handle as per the requirement and
51:14
as per the standard also. Okay. But if you talk of worker node, okay, so worker
51:21
node is a kind of managed service but it's not fully managed.
51:27
Why is not fully managed? Because you have to say something what you need in the worker node as a user of the EKS
51:33
service. For example, you have to tell I need two worker node. I need uh two worker node in this control plane. Okay,
51:42
in this control plane or in this node group I have one worker node to M5
51:49
L or something like this, right? I want in this worker node this particular driver to be installed like EBS driver
51:56
or pro to be stored. Okay. So installation part is done by AWS EK
52:01
service. How to connect the worker node to the master node is called register.
52:07
the working no worker node to the master node is done by AWS automatically. So it's like a managed service but it's not
52:13
fully managed. Something you have to say something you have to tell uh your requirement and based upon the
52:19
requirement they will tell to you. Okay. How you tell this your requirement here that is the role of EKC it will come in
52:26
play. Okay. So either you can use EKCTL command line tool CLI tool to tell your
52:32
requirement or you can use vinyl code that we'll see in today in very detail.
52:38
Okay. So let's say let's say initially we ask them to launch two worker node or
52:44
to compute node or to slave node whatever you say here. Okay. So what is
52:50
node you guys know this node behind this is a real EC2 instances.
52:57
Okay. And let's say we are using T2 micro or T T3 micro where I think we
53:03
have 2 GB RAM. All right. this called memory. We have one CPU core.
53:12
Okay, we have here it means in this worker node this is the maximum RAM we
53:19
have or maximum CPU we have. Okay, on the top of this worker node we have the OS on the OS we have a container engine
53:27
and you guys know in EKS uh EKS service the container engine we using is
53:32
containerd not the docker or not the podman not the
53:38
cryo uh containerd right even though docker
53:43
is the container manager tool but let's say we are using containerd but doesn't matter whichever they're using okay and
53:49
on the top of this we are going to launch a port right pool is also give you a field that is an individual OS
53:57
my point here is okay this port is running with your application inside the
54:02
container because port will run a container container will run application for you let's say WordPress or database
54:09
or for PHP or Python or Java right a point here is that you know already is
54:16
these everybody using the same physical ical resources from EC2.
54:24
Okay. But the maximum limit of this port
54:30
okay they can use in total right is 2 GB RAM and one CPU. They
54:36
can't use more than this in this at one point in time. Okay. The maximum RAM and CPU they can use is two GB RAM and one
54:45
CPU. Okay. So let's say uh these three port
54:52
together are consuming this entire RAM and CPU. There's no RAM and CPUB. So we
54:57
have one more worker node remaining port. If you want to launch, we will launch here. And again you don't have to
55:03
worry about where to launch. We have a scheduleuler in the cubernetes master.
55:08
So control plane is the one who will do the things right. In the control plane they will control the things right. One
55:13
of the things they control from the master is where to schedule. So
55:18
they know worker node one is full. So they can schedule the port on the worker node two that you guys know. Okay. But
55:27
again let's say one worker node we have a 2 GB RAM and one CPU is there. As soon
55:33
as they launch two port and this entire port together maybe this might using 1/2
55:38
GB RAM. This is using like let's say half GB RAM. So, so entire uh RAM has
55:45
been completely utilized. Okay. So, by the by the way here we can
55:51
use one more term. Okay. If you are keep on checking that how much RAM or memory
55:57
has been used or free, how much CPU percentage has been used or free.
56:03
Okay. How much network bandwidth or hard this speed IO? How much time you read or write? So, IO performance operations.
56:11
Okay. These are known as metrics. Metrics.
56:17
Okay. So um so uh my point is after you
56:22
run port one port two and worker node one port one port two and
56:28
by looking at the metrics and we see have your entire RAM or memory has been
56:34
fully utilized. There's no space or no memory available.
56:40
Okay. Now if you want to launch one more port so what happen if your client or
56:46
the user with ek uh not ek cubec command they say I have one more image
56:52
or let's say a web server image I want to launch one more boot out of it
56:58
okay so you know what it requests go to master node control plane and you guys
57:03
know in the master node request always go to API service again one of the part of your master node in kubernetes so cub
57:10
cube API server. Okay, cube APR will give this information to the cube
57:16
scheduleuler and cube scheduleuler say um I can schedule this port in one of
57:22
the worker node but what I can see by looking at the metrics both the worker
57:27
node that I have right now is is the RAM is fully utilized we don't have the
57:33
physical resources me is so RAM is not available or CPU is not available
57:39
something like this in this is by launching this port will fail either
57:46
port will be there always in the pending state. There's a state of the port which
57:51
means pending means uh my scheduleuler is not yet scheduled it in one of the worker node. Why? Because there's no
57:57
work or node resources available. Okay. So it is always in the pending
58:03
state. Okay. Why? Because there's no resource available. how much how long it will
58:10
pending even though if you see the logs of it that's called events of the port they will tell you very clearly that we
58:16
are trying to schedule it but because of the resources limited we are out of the resources out of the memory okay we
58:23
don't find any any uh worker node so we
58:29
keep this guy in the pending state or maybe after some time we will give the error but they won't schedule how long
58:35
we can't schedule we don't know the worker work node to they're running the port and don't know how long these port
58:42
will run whatever is been scheduled already. Okay. So in this case what can you do as
58:50
a kubernetes uh administer you can't do much only
58:57
thing that you can do okay again I would like to add one more point here is what can you do as a user
59:05
of the kubernetes nothing user means you want to launch your application right you can't do anything here only one who
59:12
can help you here is the cubernetes admin guys or presence guys or present
59:18
team. What they do? They keep on monitoring all the worker node
59:25
and after they see after they see this worker node is been fully utilized then
59:32
they can go manually and launch one more physical worker node and let's say they
59:38
are running in AWS world. So they can launch one EC2 instance after they launch this worker node and
59:45
then register to the control plane or rest to the cluster.
59:51
Okay, why? So that we have one more worker node. We have more RAM and CPU going uh will be attached to uh this
59:59
cluster. Okay, this concept guys again known as scaling.
1:00:04
they scale it and they because they're increasing one more node that's called scale out concept. So it's a horizontal
1:00:10
scaling but in the horizontal scaling they're doing scale out but how they're scaling
1:00:15
manually. Okay. And guys this demo I always show you in my last classes okay because
1:00:21
right now with this cluster we managing with EKL command and EKL command give you a capability to do the scaling
1:00:29
manually. So if you remember guys this is one of the command I showed you in my
1:00:35
uh some of my initial classes. So, EKCTL command if you see there's a option here
1:00:43
in EKC command scale your resources right so if you want to scale let's go
1:00:49
horiz scaling okay so here we can tell our node group name because because your
1:00:56
instances is work inside the node group only so which node group you want to scale
1:01:03
okay and obviously node group is a part of your of your cluster you can tell the
1:01:09
cluster name but here this is what we can do
1:01:14
okay maybe initially my total number of node in the cluster that's called desired number is two but we can
1:01:22
increase it we can increase to four or five or maybe three
1:01:28
okay so this is the command I show you in my last I show you one more time very quickly today nothing special but it's
1:01:33
also this is also scaling It's called horizontal scaling or scale out. Okay, we can do scale out scale in
1:01:41
also. Sometime if you have five for example, we can decrease to two or one if it though there's no load.
1:01:46
But this scaling is called manual scaling. We have to by ourself keep on checking the load
1:01:54
on the worker node. In the cube we have a commands available to check the load
1:02:00
or even though we have a lot of matrix tool available in the market right who can keep on capturing the
1:02:06
information about all the worker nodes and keep on showing you some kind of beautiful graphical
1:02:13
portal. For example, if you guys know Prometheus so we can install the Prometheus agent in all the worker node and they keep on
1:02:20
capturing the information about the metrics of RAM and CPU of the worker node. Okay. and they show you this those
1:02:26
things in some kind of graphical portal like graphana is one graphical tool where they can show you uh the entire uh
1:02:33
information the very beautiful graph right so so those who know promethus
1:02:39
graphana they can integrate these things with them and can show the things in very live way
1:02:46
okay or uh because this is a EC2 instances so we have one more way in the EC2
1:02:52
instance we have cloud services cloud service is a way so they can show you the metrics there on the uh real time.
1:03:01
So if you go to AWS cloud. Okay. So let me stop sharing for a
1:03:07
minute. So if you go to AWS cloud we you guys know is we have a cloud watch. This also
1:03:13
one of the way we can see the the live uh status metrics of the
1:03:20
instances. For example in my case this instance is running. This is what we launch right now. And in the monitoring
1:03:28
tab you can see what is the current status of your CPU. So at this point in
1:03:33
time if you see this CPU is around 6% used uh something like this or we can go
1:03:40
for cloud watch. So we have a uh
1:03:45
detail uh we can create a graph and filters and many more things we can do
1:03:51
in the cloud watch. See this is the matrix here. you got the matrix and you can find a lot of things about EC2.
1:03:58
Okay, again this is not the cloud of watch topic. Okay, this topic we
1:04:03
normally discuss very detail uh in our CIS op trainings of AWS or the advanced
1:04:10
architecting training of AWS. So those who belongs to CISOPS training of AWS we
1:04:15
will have this topic in very very detail where we discuss all the topic logs
1:04:21
metrics X-ray event inside application monitoring in very detail but very quick information uh even though you guys know
1:04:28
this but let me I try to give you very quick information this is what they monitor right my point here is
1:04:34
a lot of two development market okay now why I'm discussing this point here is because EKL command will give you a way
1:04:44
okay that will scale your um
1:04:49
cluster okay scale in and scale out uh manually
1:04:56
but I don't want to manual do it right because manual means we have to keep on monitoring by using of this tool
1:05:03
cloudatch or promethus graphana or maybe you can use plank also okay to as a
1:05:09
monitoring tool and based on this we manually come and launch this right. It's a it's a long process.
1:05:17
Okay. So what I want it I want this to be automated.
1:05:22
Okay. This is called autoscaling. So I want scaling. Okay. But rather than it come to human
1:05:29
being as a manual intervention, right? I want this to be done automatically. So my point here is okay I want m guys
1:05:38
again because this is EK service so you don't have to worry about master node but is EK service if you want your
1:05:45
worker node to be scaled automatically then this
1:05:51
concept is called autoscale and that's one thing I'm just only talking about node I'm talking about p
1:05:57
autoscaling is a different concept all together concept is same but it's a different requirement altogether we'll talk about it in a
1:06:04
Okay. So how we can do auto scaling
1:06:09
of the worker node. So if you have two worker node, how can we increase to three and four and five as the load
1:06:16
increase and we can decrease the load decrease automatically.
1:06:22
Okay, how can we do this? So guys this concept is not actually
1:06:30
uh managed by kubernetes the kubernet running inside the work or not they can't see mostly what is going
1:06:36
outside okay even though we have some way to manage from kubernetes also but it is
1:06:41
not the role of the kubernetes as a cubernetes software as a cluster
1:06:47
because worker node is outside thing of the kubernetes and kubernetes running inside the worker node. So what
1:06:53
happening inside the cubernetes, Kubernetes can manage for you. Whatever happening outside the cubernetes, we
1:06:59
have to use some outside tool that is not part of Kubernetes as such.
1:07:05
Okay. And that tool obviously is called EKS tool. The EK is the one who launched the Kubernetes for us. So my my my main
1:07:11
point here is how we can autoscale the worker node.
1:07:16
Okay. So how can we do this? So here what can we do? We have very fantastic service available in AWS if you know
1:07:24
about AWS. Okay. So there's one very powerful service very very useful
1:07:29
service in AWS called AS Autocaling group service
1:07:35
ASG. Okay. So those who have done some basic training of AWS CSS there we talk
1:07:41
about the service in very detail. Autoscaling group services. So if you go to your EC2 page, EC2 page is one where
1:07:51
we'll launch our instances. In our case, in this is a node. So here we can see
1:07:56
there's two worker node. We have two worker node. We have a but same page
1:08:01
have some subservice. So we go down there's subservice called autoscaling group.
1:08:08
Okay. Autoscaling group. So um so autoscaling group guys is a
1:08:15
service that will help us to autoscaling our instances.
1:08:21
Okay. So what I mean by this? So again I'm not going to discuss this service in detail but we're going to use it. But a
1:08:28
very quick information if I want to give about this service is uh what happened? Okay. Autoscaling group has a
1:08:34
capability. Okay. Uh what I mean by this for example if you want to launch EC2
1:08:41
instances there's multiple way to launch these two instances. One way we can go and manually launch it.
1:08:48
Okay. How the way we have done till date you click here and we launch it. Okay.
1:08:56
One more way to launch the EC2 instances. Okay, not by ourself. We can launch with
1:09:03
the help of this service called autoscaling group service. Okay, they will launch the instance as
1:09:09
it is the way you launch. But the only difference is okay if you launch the
1:09:14
issue instances your web server or any other Linux system or Windows system with the help
1:09:20
of this service what the server will do for you the services keep on checking
1:09:27
keep on I can say the better word would monitoring your EC2 instance whatever they launch
1:09:34
okay and in the autoscaling group service what can we tell we can write some rules
1:09:41
And one of the rule is I I'll tell my autoscaling group service my rule is I
1:09:47
want when you launch this instances you launch two instance together or three
1:09:53
instance together or four instance together that's called desired capacity
1:09:59
so I want my minimum cy or I can say not minimum I can say my desired capacity
1:10:04
of my node let's say is two always
1:10:09
Okay. So this is one of thing what autoscaling group service can do. They launch two issue instance together.
1:10:16
Okay. But because we ask them to launch two and that is my desire it means they also keep a monitoring due to any of the
1:10:22
reason if out of two let's say a and b2 instance they launch if any of the
1:10:29
instance goes down terminated. So they make sure they keeping this as soon as
1:10:34
that goes down they launch one more for you. How automatically it means they make sure your current
1:10:41
state current state it always true every time
1:10:47
the two instances keep on running for you. This is one of thing autoscaling group
1:10:52
can do for but if you launch your instance by ourself if this if you delete this they will
1:10:59
not does not launch again for you. Okay, this this is one thing. The second
1:11:06
interesting facility of autoscaling group they give you is is this service have a facility.
1:11:12
Okay, they keep on monitoring your node. Let's say you launch two nodes or two
1:11:18
instance A and B. They keep on monitoring the metrics.
1:11:25
Again they don't don't know monitoring the metrics by themsel. They're taking the help of cloud service behind the
1:11:30
scene. So cloud services keep on monitoring the metrics of A and B worker nodes or the
1:11:36
let's say instances here. Okay. And this information
1:11:42
AWS uh sorry A as the services keep on populating keep on retrieving
1:11:48
and what now what can we do to ASG service? We can create one rule.
1:11:54
The rule is something like this. Okay. Now we can tell as service that I know
1:11:59
you are keep on moning the RAM or keeping the CPU. Okay. So what I want
1:12:05
from you just keep oning the RAM or let's say you keeping the CPU if okay
1:12:11
the CPU utilization of all these worker node together go above 80%.
1:12:20
Okay. And 80% let's say continue for 5 minute. It will continue for 5 minute 80% CPU is
1:12:26
going on. It means it means something is going on uh and they're continuously
1:12:32
utilizing the CPU. It means if you launch more pores either they won't launch or they maybe they slow down. So
1:12:40
if this is the case this is called kind of condition. If this is the condition we have
1:12:48
okay then what you do you do scale scale out means launch let's say you
1:12:57
have capture two change this capture to three automatically mean scale out means
1:13:03
launch one more or two more depend upon the rule what you write
1:13:08
instances for us how we do now we don't have to come manually as being
1:13:14
automatically 2 is running launch one more one more
1:13:21
node for us again three now three launch ABC
1:13:26
keeping again out of the three if CPU go 80% average in all three launch one more
1:13:32
launch one more keep on launching okay so it means as a load increase my
1:13:38
user my client all things that does not face any issue as a load increase they will keep on launching
1:13:44
the uh the nodes. Yeah, we can set the maximum limit because if you they keep
1:13:50
on launching because of some unwanted behavior so it does not impact a bill.
1:13:56
Okay, we can set the maximum limit. Let's say 10. So keep on launching till 10. After 10 stop it and the load
1:14:03
decrease so keep on removing this call scale in keep on removing again I want minimum always to be two for example. So
1:14:11
keep on decreasing till two. So it's your number whatever you want to fix as per your requirement of your your worker
1:14:18
nodes. But we can tell okay these rules in the autoscaling group what is the
1:14:24
maximum what is the minimum what is my current requirement is let's say three for example you start with three and and
1:14:32
a lower degree can go to four then five and six uh this is what so autoscaling
1:14:38
is a very very powerful service so those who are doing AWS CSA training under me
1:14:45
this is a topic we have discussed there either in the classroom or we have provided a recorded video of my classroom but this service we have
1:14:51
discussed in very very detail from very basics okay how to set up by yourself so
1:14:57
again here the plan is not to talk about autoscaling setup okay uh because I don't want to use
1:15:03
autoscaling by myself instead e service has this facility
1:15:10
okay so what we can do in the EK service while launching the worker node
1:15:17
okay worker node. Let me create this diagram very quickly one more time. While launching the uh cluster master
1:15:25
node we can't see but worker node you guys can see it right. So while launching the cluster
1:15:32
okay we can launch this worker node with the help of S Ag service autosklling
1:15:37
group service. I'll say autoscaling group service you will launch this worker node.
1:15:44
Okay. Rather than this worker node has been launched directly without autoscreen group. Now I want you launch
1:15:51
this worker node. Okay. Why? Because I will tell you some
1:15:57
rules. Okay. And based on the rule is if my RAM goes out this or my CPU go about this.
1:16:03
So this is your duty automatically scale and add one more worker node or
1:16:09
compute node. Okay. auto do this right. So autoscan
1:16:15
group will launch this and as soon they launch
1:16:21
okay automatically because this now this autoscaling group service is actually managed by EK
1:16:26
service. So EK service will tell as soon as you launch the
1:16:33
worker node the now your duty is complete. Now my duty is this worker node I will set up as a compute node and
1:16:40
attach to the master computers or attach to my my cluster or register this worker
1:16:47
node to my cluster. So my cluster will increase in the capacity and the computing power or the resources power.
1:16:55
Okay. So now my point here is see lots of things is connected together right. EC2 is different service. Cloud watch is
1:17:03
different. Autosklling is different. EKS is different. But everybody has doing
1:17:08
integrated very very closely very effectively uh with the help of EKS service one central unit and they work
1:17:16
together very very uh properly right to to manage everything right. So I'll show
1:17:22
you I'll show this demo to also to you how to autoscale
1:17:28
automatically. Okay. So only thing you have to do is if
1:17:33
you want your EK service worker node is to be managed by autoscaling group. If
1:17:40
you want this then while launching the cluster
1:17:45
let me launch the cluster. So ektl command
1:17:51
create cluster command you guys know if you see the help. So what can you do
1:17:58
while launching the cluster? Okay, with the way we have set up lot of
1:18:04
things here, we can specify this option.
1:18:10
Okay, this option and what this will do they will help us to do cluster auto. No, it's not port
1:18:17
autoscaling. P is different autoscaling. We'll discuss this is the cluster autoscaling. It means they're talking
1:18:22
about the worker node autoscalers. they will do for you. Okay, you have to
1:18:28
only enable this feature over here. So while launching the
1:18:34
cluster, you can enable this part of feature now. Okay, your worker node will
1:18:39
launch. Okay, but those worker node is launch
1:18:45
with the help of obviously worker node EC2 instances in launch with the help of autoscaling group services.
1:18:53
Okay, the guys here is so simple right and those who know autoscaling services even though it's a simple service but a
1:18:59
lot of things we have to do right by ourself but here everything is done automatically by EK or the EK services
1:19:06
this is one thing right let me give the name of the cluster as a cluster uh auto
1:19:11
scaling group auto sorry autoscaling group this
1:19:18
I'm just giving the name to this cluster okay or Maybe you can start with number
1:19:24
of node initially that you guys know we can start with number of node we can decide okay uh so that's
1:19:34
what we can do but because if you using autoscaling group that's what they're talking about
1:19:40
okay then you can discard these options also if you want okay that in autoscaling group I want
1:19:48
number of node desired is two or max is
1:19:53
minimum is two max is 10. Okay. So if autoscaling kicking kick in
1:20:00
maximum they won't go beyond beyond uh 10. So let me also write this option. So
1:20:07
I want my uh in autoscaling group the max let's say is 10
1:20:16
min let's say is two my desire means initially when launch
1:20:23
okay my desire is three okay that's what we can go let me go for
1:20:30
smaller node types let's say node type is T2 micro
1:20:36
I I want to go for and if you want to give a node group name also let's say
1:20:43
node group A okay this command you know guys we have
1:20:49
discussed already and if you want to access the SS access also enable this and also enable SSM also
1:21:00
okay uh to connect directly with from graphical portal and uh fet we'll see in
1:21:07
the future. And uh uh zones version let me go for the latest version I think is
1:21:15
1.25 25 of the of the uh EKS version it is and name of
1:21:24
the cluster we have already given right. So this is this is a command line right
1:21:30
we can do with YML file also but this is what we have seen but main thing is I want to show you this facility of auto
1:21:38
scaling group. So let me copy this code or command I can say sorry
1:21:45
created this ekctl
1:21:51
get cluster the
1:21:58
uh cluster 21 I think started yeah so
1:22:03
let me launch this enter.
1:22:12
So I think this what actually the behind the scen the command already ran something they have done. So let me change I will clear after some time. Let
1:22:19
me say my cluster as that's what the name I'm giving here. Okay. So here uh this node
1:22:29
group created okay for your cluster.
1:22:34
Okay. Main thing is they are going to launch three worker node that's called desire
1:22:42
but this three worker node is been managed by auto scaling group that's
1:22:47
what I enable here so when you see guys after some time three node come up here but these three
1:22:54
what are coming in some time will be managed by this service so autoscaling
1:23:00
will automatically created here where you will see the desired capacities which is 3 minutes 2 max is 10.
1:23:09
Okay. But we'll do many more things here in some minute but let's us launch uh
1:23:16
this part. Okay. So
1:23:24
so guys let me delete some because I have some limit ect or let me change the region. Right.
1:23:32
I think we have a limit in the regions. So, let me go and launch US East one.
1:23:42
Region US East one.
1:23:51
So, I'm launching this cluster in this region. Uh you might see somewhere. Yeah, in the Virginia region, right?
1:23:58
Okay. So by the meantime guys this will up it take again 5 10 minutes it will up
1:24:07
uh let me move to next topic. I think you understood the idea right. My my intention is now we have three worker
1:24:14
now running and what I will do I will do some load testing here. We will run some
1:24:20
application uh that will give a load to our cluster and you see I will give so
1:24:26
much load that three node is not enough to handle those load. So autoscaling
1:24:32
group will automatically scale my cluster.
1:24:38
This go worker node. Okay. So um so this what guys uh you
1:24:46
will see in some time. Okay. But let's have this cluster coming up launch then
1:24:53
I will explain this uh show you this demonstration to you. Okay. So this is
1:24:59
one thing but let me move to next topic. So I let me hold this topic for a minute
1:25:04
by the meantime cluster up. Okay let me move to next topic again it's almost the
1:25:10
same topic but is the autoscaling on the pool. Okay, that's what we want to do here.
1:25:18
Right. So what I mean by the application autoscreening on the port, right?
1:25:24
So this concept is not have to manage by EK service. Neither has to manage by
1:25:30
by uh your uh AWS as such is internal
1:25:36
feature of the Kubernetes. Okay. And that is reason this topic is
1:25:42
mostly uh mostly uh learned under the cubernetes
1:25:48
training. But I want to show you something here because there's something you might do uh from AWS also. So again
1:25:56
the topic is auto scaling. Okay. But it is with respect to port.
1:26:04
What is port? to know but very quickly you have the image container image that has your application from the image you
1:26:12
launch a container container have one like a OS who give entire OS to you but
1:26:17
this container is managed in the cubernetes world under the port
1:26:22
okay so this how it work uh guys actually I'll be launching two
1:26:30
cluster here there's one thing I missed right because both the cluster have their own keys
1:26:36
okay and key by default override in this file in this file
1:26:43
so um what I'm doing right now as soon this cluster hub this file will be override
1:26:50
because I already have a two cluster running to show you some
1:26:56
demonstration but let me by the meantime guys they launched I will show this demonstration very quickly so my point
1:27:02
here is When you launch a port, let me launch a port very quickly. Cubectl U
1:27:09
create deployment. Let's say my ID. I'm launching uh
1:27:17
from the image. I'm using this image that I used in my last class also. It's been given by me. You can find in
1:27:23
hub.docker.com. Okay. You can use any images or create
1:27:29
your own but you can use this one if you want. So I'm launching this image.
1:27:37
Okay. So now right now one single port launch right one single port launch.
1:27:45
Sorry one single port launch which we're creating right now because downloading
1:27:50
this image. Okay. But if it is just a one single um uh one single
1:28:00
what I say okay and this port running with the web application right PHP application in my
1:28:06
case okay but if you think let's say this is one OS one application and let's
1:28:12
say this application can handle 100 requests in concurrent
1:28:17
okay but you want thousand requests to be handled in concurrent so you can scale
1:28:23
Okay, how to scale? You guys know this command from the Kubernetes is is this coming from Kubernetes. The port is a
1:28:30
internal resource of Kubernetes. Okay, so if you have more load come up in the
1:28:35
port, okay, so we can scale the port. So cube
1:28:41
is a Kubernetes command. Okay, it's not EKL cube serial is for Kubernetes and we can say we scale and we scale the
1:28:49
deployment because we may launch with the help of this deployment name my D
1:28:54
and I want to create a replicas of it. Let's say I want to
1:29:01
uh create total five replicas. So what we have done we scale it
1:29:09
okay we scale it and now we have five ports are running
1:29:15
and they're running behind the load balancer either you can use the internal load balancer or the external I show you
1:29:21
the external demo in this case because they're running EKS so external load balancer would be your ELB services
1:29:29
okay that's what we can use even though we have we will talk more about The external load balances like
1:29:36
ingress controller how can you set up and many more things we'll see in upcoming discussions but those are demo
1:29:42
creating using the external load balancer ELB uh we have already uh we have already seen right
1:29:48
so till here is good nothing uh any issue here but again this scaling we are
1:29:54
doing manually right there's a manual scaling it means somebody
1:30:00
okay somebody have to keep on monitoring the resources
1:30:09
consumed by this port how much RAM and CPU they're using
1:30:14
okay so somebody's keep on monitoring so if you see the describe command or the node describe command okay let's say
1:30:22
this port they'll tell you especially node discard command okay they tell you
1:30:27
that this particular port is using how much resources it's not here but there a lot of command available
1:30:33
like top command and many more command to tell the current resources used by uh
1:30:39
this particular ports. Okay, my point here is okay if let's say
1:30:47
this port is uh maximum CPU this port can handle let's
1:30:54
say let's say 1 GB this 1 GB 5 12 5GB is
1:30:59
there and out of 5GB the entire 5GB almost consume RAM and CPU network bandwidth right now some client come to
1:31:06
this port they won't able to handle it it will slow down or they may be not able to handle this right. So my point
1:31:13
is what we can do again we can come manually type this command and and uh
1:31:20
scale to let's say six six port launch.
1:31:25
Now more clients we can handle, more requests we can handle. Again guys the
1:31:31
request goes you know more traffic going to come up and keep on mering
1:31:38
continuously all the port okay and then after the need arise they
1:31:44
manually launch and scale it is docker container I can say containers so they
1:31:50
launch very quickly there's a good thing about the content technology but the bad thing is okay we as a human being has to
1:31:57
monitor continuously uh and then launch it. There may be some
1:32:02
obvious delay going to come up because of like a manual way. But if you want that this can be done
1:32:09
automatically as a load increase in these ports.
1:32:15
Okay. Uh or these application automatic port will increase
1:32:20
replica is created and the load decrease automat delete. For example, you want to
1:32:27
decrease it then we can change the four. example now it's called scale in so out of seven
1:32:34
three will be deleted here only three four remaining but I don't want this thing manually
1:32:39
right I want this to be scale out scaling to be done automatically so here the role of autoscale of port come in
1:32:46
play okay and because port is inbuilt futility of the AWS so inbuilt facility
1:32:53
of kubernetes so kubernetes has a one uh one uh resource for this this called
1:33:01
HPA. So HPA is one of the resource in the cubernetes. Okay, if you search for
1:33:07
check cube you'll get HPA. HP is the one who will give this facility to you
1:33:13
facility to you. Okay, again it is not part of EKS. So I'm not discussing more
1:33:19
in detail. I'll show you very quick demo also obviously but I don't want to talk more it into this. But point is HPA is a
1:33:28
uh is a horizontal scaling for your code.
1:33:34
Okay. And what they do almost same because I explained to you right they keep on monitoring their CPU and many
1:33:41
other things also and as soon as the load increase they will scale out load
1:33:47
decrease the scale in how automatically who will handle this in Kubernetes HPA
1:33:53
will handle okay HPA will handle this for you okay
1:34:00
for this what you need okay minimum requirement you need. There's important
1:34:05
point to understand here. The minimum requirement uh for you to do so is to
1:34:12
keep on checking the metrics. Okay. Anything if you want to scale
1:34:19
automatically. Okay. So for this you need information
1:34:25
and because here we're scaling it. So we need the matrix information.
1:34:31
Matrix information. Okay. So my point is what I'm trying to tell you is if you want autoscaling here
1:34:39
to be done maybe HPA by the tools maybe does not matter if you don't know the
1:34:46
current realtime metrics real time metrics metrics where you guys
1:34:51
know RAM and CPU plus a CPU for example. So if you don't have any tool
1:34:57
that keep on giving you what is the current utilization of these guys, how much CPU they're utilizing at this point
1:35:03
in time, you can't scale. That's what I'm trying to tell you. Okay? So you
1:35:10
need certain program certain tool that keep on retrieving the metrics information
1:35:16
and those metric information we retrieve and keep on providing to the HPA
1:35:23
and HP we write a rule the way I explained to you in my ASU also uh uh by
1:35:28
the rule we write for example we write the rule is let's say 80% CPU CPU so
1:35:34
after all the four nodes port I can say not the node after all four ports If my average CPU go above 80% then you
1:35:44
kick in and launch one more port create one more replica scale automatically.
1:35:51
So this kind of rule we can create here but this rule makes sense if you have
1:35:57
the information uh current information right okay and who give this information
1:36:02
your realtime matrix tool and for this in the cubernetes you have to install some metric server
1:36:10
who keep on giving this information to you okay and right now if you see do we
1:36:15
have any metric server okay so guys metric server is again one kind of program running as a food where you can
1:36:22
find it. So in cubectl if you check for the code
1:36:28
not here in your in your current uh uh current
1:36:34
uh name space it is available in a special name space called cube system
1:36:39
that I saw in my last class also okay and if you if you see here there's
1:36:46
no code running for the metric server
1:36:52
Okay. So if you don't have the metric server, it means there's no program which keep on monitoring the metrics of
1:37:01
your poops running in your cubernetes this kubernetes.
1:37:07
Okay. So there is nobody uh who is ming this. So what you can do you have to install
1:37:15
the metric server. Okay. So how can you install? So we can
1:37:21
find a software on the internet. So if you search for cubernetes
1:37:27
metric server installed
1:37:32
okay maybe in as also you can find some link or there's maybe a link for the
1:37:40
metric server. So metric server is a one who will
1:37:45
monitor the metrics for you. Okay. And you might find some
1:37:52
quick link to install. I think this can be the link, right?
1:37:58
So metric server can be installed directly with this code. That's what I'm going to do here with this code.
1:38:06
Okay. And uh it will install the metric server for your system. Right? So I
1:38:12
don't know this is a compatible or not, but let me very quickly install it otherwise we'll see some more thing. Right? Again there's a many more thing
1:38:19
we can do but let me install this metric server.
1:38:32
Enter. So this version is compatible with this current version of cubernetes. So M
1:38:38
server will install successfully. Technically they launch a deployment for you and deploy one who launch the port
1:38:47
for you. Okay. And now we see some difference is going to come up. Can see
1:38:52
some metric server is is is running. It's going to run in some minute.
1:38:58
Okay. And who run this is run by the deployment. So if you check the
1:39:03
deployment you can see this deployment they created
1:39:09
the meta server has been created and they launch one port and that's what we
1:39:16
can see the port here and port is running I think it's now compatible I believe so that's what there are
1:39:24
okay so in this kubernetes my deployment had launch a port and there's launch a
1:39:31
port is launch the matrix. Now what this server will do in this Kubernetes
1:39:39
whichever ports and things you launch they are now start monitoring
1:39:46
your your uh current metrics of all the poor
1:39:53
they monitoring CPU and many more things okay and now because they're monitoring
1:40:01
okay they're monitoring so what can we do we and launch the horizontal scaling
1:40:07
with the HP. HP is horiz autoscaler. We can launch.
1:40:12
Okay. How we can create? So cubectl there's a command called autoscale command, right?
1:40:20
Autoscale. If you use scale is manual scaling. Autoscale is a autoscaling.
1:40:28
Okay. And if you help you will see these options here.
1:40:33
Min, max. Okay. CPU percentage one of the metrics
1:40:39
or you can find some options here for example uh is autoscaling okay so we can find this can be one of
1:40:48
the example or let's say this can be one of the example let me copy from here
1:40:55
so what we are doing here so if you see cubectl get
1:41:02
deployment this deployment is the one who launched
1:41:08
um this port. Okay, let me do one small thing. Let me
1:41:14
bring down my port to one the by default the way we started.
1:41:20
So we have only one single port running. Now what I'm going to do I'm using the
1:41:26
uh cubectl auto scale and I say I want to create
1:41:33
auto scale. Let me check the help
1:41:39
auto scale of this one.
1:41:46
Okay. So this deployment I want to set the autoscale.
1:41:53
Okay, autoscale and auto we always attach to the deployment mostly.
1:41:58
Okay, my deployment is my ID and this is the deployment we have
1:42:04
deployment my where the max I'm setting let's set 10 min I am setting let's say
1:42:13
two and be only based on the CPU let's me let's set 60 CPU every CPU
1:42:21
okay that's what we have uh set here
1:42:27
Okay. Now if you hit enter
1:42:33
okay this is some error come up because by chance this my next cluster also been launched. So my this file override.
1:42:41
Okay. So because two cluster we actually have to do the thing what we have done in my last class. We create a separate
1:42:46
conf file but now this file override. So let me do one thing. Let me let don't
1:42:53
use this cluster. Right. So let me do same setup in the this cluster is a new
1:42:58
cluster actually. Okay with this name. Okay. So let me do the remaining setup
1:43:04
and same setup again very quickly here. And what I'm doing here same thing let
1:43:10
me do it again very very quickly. So we need minimum
1:43:16
my matrix program
1:43:26
has been launched. Then I'm going to uh
1:43:33
launch my code with this deployment.
1:43:42
Okay, that's all. So technically if you see right now
1:43:49
there's no HPA set sorry HPA set for the
1:43:54
port HP is only for the port is a hor horizontal port autoscaler
1:44:01
okay and now finally if you want to set I'm going to setting the auto scale
1:44:11
auto scale
1:44:17
autoscaling. So now this autoscaling been set over here for now I can see HP
1:44:22
has been created. Now what you will see current
1:44:32
we have one but according autoscaling in this port
1:44:38
my ID in this port if you have a load okay go above 60% of the average CPU
1:44:46
right I only one so again belong to this port only if you go above 60%
1:44:52
automatically one more port launch Then one more port launch then four launch then we see the average four 60%
1:45:00
one more port they launch and maximum they go till 10 and there's no load they will keep on decreasing and minimum they
1:45:06
go to two that's what you can see here okay but if you notice here automated
1:45:12
launch one more right why because we asked them I need minimum
1:45:17
two always they found one okay so the automatic scale to two
1:45:25
because if you ask them the minimum requirement is two always. So they always keep oning two. Even if you
1:45:30
delete it manually also one okay again they launch one more for you. So minimum
1:45:36
guarantee will be two always maximum they going to 10 but between two and 10 they will scale out scale in based on
1:45:43
your load. Okay based on this load. Okay and guys there one more point here
1:45:49
is this unknown uh line come up. This unknown means 60% this is what the rule
1:45:55
we set after 60% CPU average goes then we increased from two to three till four
1:46:03
okay and you can see here this is the desired capacity right now my desire is two
1:46:08
by load increase my desire will change to three so they launch three this number will always fix but this number
1:46:14
is automatically scaling it change it okay as a load increase decrease okay
1:46:21
and this 60% is the rule we created. This unknown means the current average CPU utilization.
1:46:28
Okay. So these two port the average CPU right now let's say 10% or 0% or 5% will
1:46:33
be updated here but it takes some time to update because this information they
1:46:39
collecting from this uh this particular metric server. It takes some time
1:46:44
initially. Okay. If the unknown is not filled in some minute either we have metric sur
1:46:52
not installed or maybe this version does not support something may be challenging
1:46:58
okay but we'll see in some minute is filled then I'll show you the autoscaling uh things right
1:47:06
okay but I think you got the idea right so my point here is auto scaling it's
1:47:12
called HPA for the port is not belong to EKSs it is there inside the cube wherever You have the Kubernetes, you
1:47:18
have this facility. Okay, you have this facility. Otherwise,
1:47:24
uh HPA right now we have created with the command line. But if you create with the YL code
1:47:31
YL code, um this autoscaling
1:47:36
uh then there many more rules we can create and set up. Okay. So if we search for autoscaling
1:47:44
the cubernetes, this is a website you can get autoscaling at PA and there's many more
1:47:52
things you can um set in the autoscaling uh world. So this is a kind of yl
1:47:59
file maybe you might find somewhere many more things you can do we can find
1:48:05
somewhere. Yeah this is right. So this autoscaling port autoscaling we can
1:48:10
create many replica metrics we can set CPU utilization average 50%. And many
1:48:16
more metrics supported here and more more things you can do here. Again this
1:48:22
is not the uh this is the uh some maybe
1:48:28
network things also u but again we are not in the in the cubernetes class
1:48:33
essence so we don't discuss much but I think you got the idea right about this particular uh concept right so we'll
1:48:42
wait for some minute okay then this number feed
1:48:48
after this number feed then we'll increase a load load in our application.
1:48:55
Okay. And after load increase if a CP percentage increase then you will see
1:49:00
they do autoscaling. Okay. So this is one of the topic what
1:49:06
we plan for today setup I've shown to you but as a demo part demo this is the
1:49:12
demo what I show to you but for testing part I'll show you also in a minute.
1:49:18
Okay. So again let's back to my this topic. And guys I'm switching here and there to save our time because sometimes
1:49:24
some information takes some minute to populate. To save our time again I'm moving back to this topic.
1:49:31
So this topic I started with I explained this topic already to you. Okay. So this
1:49:36
topic is about again the autoscaling but this is the autoscaling of the worker node.
1:49:43
And right now if you see okay let me have this terminal here. Let
1:49:48
me open the new terminal. It's the same thing but look have this screen with us. All right. U so we have launched the
1:49:55
cubernetes cluster with EKS clail command. Okay. Get cluster. So this
1:50:02
cluster had been launched. This is what the cluster we launched. Okay. And if you see the node group of
1:50:09
this cluster
1:50:14
cluster A. Okay. So
1:50:26
cluster cluster a
1:50:33
um there's no node group why so it should be let me say cube ctl
1:50:44
get nodes they'll tell you how many nodes you
1:50:49
And you can see three node right. So three node uh we have
1:50:55
okay in this particular cluster node group maybe not we have created but we'll see in the other commands also in
1:51:02
a minute but yeah we have three no nodes we have created right three node we have
1:51:07
created and very quickly if you see the description of these nodes let one of
1:51:14
the node okay so this node obviously running with EC2
1:51:21
2. Okay. All right. With AC2.
1:51:27
So this node is under this node group name. It should be there. I don't know why it's not coming there. We'll see in
1:51:33
a minute. Uh in this cluster actually the cluster is my cluster. Actually I was
1:51:39
using the same my cluster is not coming here. I know why. Names should come right. My cluster
1:51:47
as let me check.
1:51:54
it's not there some you know I think there's some command we stop in between something happen right
1:52:00
uh uh something happened uh actually you know the challenge is not this issue
1:52:06
challenge is the region is different right so we launch in the different region right uh that was challenge right
1:52:14
now they're checking part Mumbai but we launch
1:52:19
uh into a different region altogether right we launch in the Virginia region right
1:52:26
there that was a challenge so this is four cluster in the Mumbai
1:52:33
and we launched this cluster in the
1:52:38
Virginia region right yeah it is here and in this cluster
1:52:45
if you see the resources these These are the community resources.
1:52:50
If you see the compute and we have one node group see here this is what we have
1:52:56
created right with three capacity. Okay. So what can I do from the command
1:53:02
line also same thing we can do in the command line I'll tell my node
1:53:08
group but it is the region called US east
1:53:14
one that's called Virginia region right.
1:53:21
So yeah it is there. So we have this node group having three
1:53:28
desired capacity min is 10 but current capacity three because
1:53:35
my desire is three. So that's what they show you
1:53:42
three cluster and one thing guys cube key command does not understand what a region right is a
1:53:47
cubend command. Okay region is only means mean for the AWS services.
1:53:55
Okay cube will need a cube config file and cube config file tell you the detail where the cluster is. So that's what
1:54:02
they showing you all the cluster information all the node we have in the cluster. Yeah. Okay. But one more thing if you
1:54:09
notice uh here uh is uh is this one right in
1:54:17
the node group they launch this three computers. Let me show you here in EC2
1:54:22
page also they launch this three
1:54:27
instances or the worker node
1:54:33
in the Virginia. This worker node this worker node does not launch
1:54:38
automatically. Uh so it does not manually kind of manual this worker node actually is been launched with the help
1:54:46
of auto scaling group. Okay. How I know that's what they show
1:54:52
you here. So if you know guys autoscaling concept there's a concept called launch template.
1:54:58
Okay. Launch template is a concept come from autoscaling group. Launch means
1:55:04
they are the one who launch the instances for you of EC2 and here we
1:55:09
tell in the launch template I need EC2 instance with T2 micro and this AMI and
1:55:15
something like this. Okay. So if you see here in the EC2 page
1:55:22
these three instances been launched by by
1:55:27
launch configuration is a new name but is again also launch template.
1:55:33
Okay. So maybe we don't have here the page for launch template but maybe
1:55:40
autoscaling go we can find
1:55:48
here this three instances been launched by this autoscaling group. Autoscaling
1:55:54
group is just a concept inside this we get a rule but finally always launch
1:55:59
with the help of launch template. If you see this is the same launch template what they were showing in the in the
1:56:08
this page EK64C EKS uh what the name difference
1:56:15
it should be same right actually it was Mumbai so
1:56:22
this one okay so this is the launch template my cluster ASZ
1:56:28
uh this what the name they're showing you here. So what a name this is not is ID
1:56:34
whatever name they will be they will but you can find here three instance the launch
1:56:40
okay and this what you can see here two and 10 we have set right so my current node is running three but
1:56:47
minimum they go to two and maximum they can go to 10 depending upon the load
1:56:53
okay so my point here is autoscaling group automatic created that's my main point this not the training for
1:56:58
autoscaling group but the launch of for you with this ektail command.
1:57:06
So this node will increase or decrease automatically for you based upon the
1:57:13
load. Okay. And one more thing guys here is if you know a little bit of autoscreening
1:57:19
group okay in the Virginia region uh you can see some information here activity.
1:57:27
Okay, this is the activity they are doing for us to launch and
1:57:32
delete. There's no activity right now. The only activities is to launch three nodes
1:57:38
in zero and because our desire is three. So they launch three worker node. See
1:57:43
here three machines or three worker node they launch. Okay, this one activity you can see and
1:57:51
uh but the autoscaling tab.
1:57:57
Okay, so there's no actually this is not the setup they have
1:58:04
done instance management. So there's three three instances they launch.
1:58:11
Okay, they launch because we asked them the minimum three. Okay. But now how we
1:58:17
can see this demo demos is done right. This this setup has been done.
1:58:23
Pre-worker node has been launched with the help of autoscaling group. So EKS
1:58:29
has been successfully integrated with autoscaling group. That's fantastic. But
1:58:36
how can we see this demonstration working right? So for this okay what we
1:58:43
have to do? you have to do some kind of um setup right what I mean by this so if
1:58:49
you go for AWS EKS auto escaling group
1:58:57
okay so this is a tab you will get from the AWS because EKS is a service
1:59:04
okay here this is autoscaling and when we talk of autoscaling with respect to EKS it is they're talking about
1:59:13
or talking about um um uh the autoscaling of the worker
1:59:20
node not the port not the master node okay and to do the autoscaling there's
1:59:27
two different product guys in that is supported in data support in AWS
1:59:33
okay for autoscaling one is cluster autoscaler that is what we using right now there's a clut cluster autoscaler
1:59:41
means they're using autoscaling and one is carpenter. Okay. So the two different products we
1:59:47
have we have some discussion on the carpent is very interesting service is a third party tool actually but is now
1:59:53
highly supported in AWS for autoscaling in the kubernetes world. Disco worker
2:00:00
autoscaling we'll talk in the future maybe but right now we are talking the the setup that we are talking right now
2:00:06
is cluster autoscaler. This is what we are talking right now.
2:00:12
Okay. And cross autoscale using auto ag that's what we using you know as
2:00:19
autoscaling groups. Okay. Carpenter is not using as they using a different method al together.
2:00:26
Okay. We'll talk about the carpent in the future but right now we're talking about the autoscaler means the AWS
2:00:33
uh you know as services. Okay. But there's two methods but right
2:00:39
now we're talking about this one standard one but carpenter is sometimes also very useful.
2:00:44
Okay. Now for this okay if you want to use autoscaling
2:00:53
okay autoscaling for this you need to do some settings
2:00:59
okay so first you have to create the clust IM we don't need I believe
2:01:07
okay so yeah so to to um
2:01:15
uh implement autoscaler, right? We need to do some some settings, right? Because obviously we have to monitor the node
2:01:22
work also, right? We have to monitor the nodes.
2:01:27
Okay? So what we have to do first we have to do this step. My point
2:01:33
is if you want this autoscaling will work technically we launch the uh minute
2:01:42
we launch the cluster with three worker node okay and we ask e service that this
2:01:51
worker nodes will work as autoscalers it has been managed by ASG
2:01:58
okay it's called autoscalers or auto scaling group Okay, they will manage but
2:02:04
how they manage they need some information right the way I explained to you they should need about the
2:02:11
information about okay about uh how they are going to take
2:02:17
the metrics because finally these are the EC2 instances right so how uh they monitor
2:02:25
get the RAM get the CPUs the score metrics information okay and many more aspects. So we have
2:02:33
to set up something and for this we have to use this file.
2:02:38
what this file will do. Okay, I'll explain to you in a minute. But let me
2:02:43
copy this file or run this file here
2:02:49
this file
2:02:55
autoscaling file and what we have to do so if you open this file I'm just
2:03:00
opening this file with notepad cluster auto scale auto discover file
2:03:11
Okay, in this file, uh what we are going to do? Uh first thing guys, we are going
2:03:17
to create some internal account. So if you guys know about Kubernetes,
2:03:22
this is concept called service account is not a user account. User account we create for the user. Service account is
2:03:29
used by some services internally by Kubernetes. So we're going to create some service account with some name in
2:03:37
this name space. Again, it's not very important right now. they'll be getting some rule. Rule means
2:03:43
we're giving some power. Okay, means uh if you want some user
2:03:49
will do something on your Kubernetes, we have to give some rule, some power, some permissions.
2:03:55
So what are permission they need to use? They giving here. So again, it's not
2:04:02
important for us. But yeah, one of the permission is I want to know the status of the port.
2:04:09
Okay, how much port is using the RAM and CPU? So these kind of power we are
2:04:14
procreating for them. Again, we don't have to worry about this.
2:04:20
Okay, so lot of permissions they created for you. Okay, then we're getting a rule that
2:04:26
apply to this permissions. Okay, so it's all about permissions, right? So we
2:04:32
don't have to worry about um to know this. But yeah, if you know Kubernetes or if you know the concept of role and
2:04:38
arpeggi. But what we are giving here the account
2:04:44
that we created, we have given some permissions. That account have power.
2:04:50
Name of the account is cube cluster autoscaler. This account have power to retrieve some information from the
2:04:56
cubernetes. How much RAM and CPU they are utilizing and many more other informations also.
2:05:03
Okay. Why we need this information? If you don't have the information, how can you take a decision? You doing a scale
2:05:09
or not? Okay, that's what they're doing here. And
2:05:14
finally guys, this is what the main part. Okay, because if you want to do
2:05:21
autoscaling you need to run some program or app
2:05:28
and you guys know in the cubernetes all app we run as a port and port is launched by the deployment. So we
2:05:35
launching a deployment and one port we are launching okay in this name space by this name it
2:05:42
name can be anything okay and we're launching with the help
2:05:47
of of one image this image so this image given
2:05:54
by by the Kubernetes this image
2:06:01
is going to launch a container container going to launch in a food.
2:06:06
Okay. And this image has a capability capability to retrieve the
2:06:14
entire metric information about your
2:06:19
your uh cluster because what we need we need to know the
2:06:25
entire information about node or how much RAM and CPU they're using the
2:06:31
demanding. Okay. So this image have a capability to
2:06:37
know this by technically Intel guys they're using Prometheus that you don't
2:06:43
have to manage by yourself if you know Promethus but does not require to manage in they're managing everything by the
2:06:49
Prometheus on the sport number but everything they have indicated so you don't have to worry about it but if you
2:06:55
know Prometheus you can go to the sport number also and to see what is that retrieving and how they managing it
2:07:01
right this called metric information. I'm just giving this information to you very quickly here. But technically we
2:07:07
just run this file. Okay. So a lot of things why I'm giving this file uh here you to do only one
2:07:15
small change here I believe. Okay. And the change they told you this file you
2:07:21
have to replace your cluster name. Okay. So somewhere here
2:07:28
this name you update because
2:07:33
we are running with lots of cluster which cluster you want to monitor
2:07:43
you have to update the information here. Okay. So this what we have open this
2:07:49
file. So in my case my name of my cluster we know already but very quickly
2:07:57
the name of our cluster we can find EKCTL get cluster in a region they're
2:08:04
running in a US east one
2:08:11
sorry
2:08:16
so this is the cluster name we have to update here. So this is generic file but here they're asking to give the cluster
2:08:23
name that we updated here. So I want my this
2:08:30
command in my AWS service provider
2:08:37
going to uh going to monitor
2:08:44
this cluster information. This is what we would like to do here.
2:08:51
And who will do this image? Okay. Uh for us, this is the one thing
2:08:58
uh we have to do based upon the what they're talking about.
2:09:04
Okay. So that's all anything as long as replacing CPU memory value by the
2:09:10
environment. No, we don't need to do that's okay. I think this is the main thing we have
2:09:16
to update in this file. Let me save this file. Anything else? Let me check very
2:09:21
quickly if I need to do. So
2:09:30
I believe no one.
2:09:38
So no I believe I don't think so. So this is a file only one thing we have to
2:09:43
do here. And let me run this file. So this is the cubernetes thing. So we
2:09:49
running inside the cubernetes. So I'm applying this file called cluster autoscale discover.
2:09:58
Okay. So that's what we are doing here. We apply this file.
2:10:05
Okay. So this been done. And finally if you see
2:10:11
uh they launch a port get port under the name space called uh cube system is
2:10:17
enter name space and you can see this program has been started.
2:10:23
Okay. Q cluster autoscaler program has been started and this is a program who
2:10:28
is monitoring your entire cluster information your physical node
2:10:35
information also and this is the one who keep on sending the information to a
2:10:41
services okay and a service I can say will taking the information from here okay and if
2:10:48
they found uh more traffic they've increased the nodes.
2:10:54
Okay, that's what they're doing here. Okay, some more thing we have to add
2:11:00
here as a role information. Okay, now here they created one role in
2:11:07
your account. Okay, you basically you know u uh you
2:11:12
have to give a role right to to to uh uh perform the thing right because
2:11:21
uh I told you this thing in my EBS uh topic also
2:11:27
okay so this still pending okay and one of the reason guys they are
2:11:32
pending if you see the answer if they're pending so Then one of the
2:11:39
reason maybe they don't have the role. Okay, maybe one of the reason maybe. So
2:11:46
if you describe this thing so uh pageuling too many port three and C
2:11:54
memory um guys here they failed because of other reason
2:12:00
okay because this particular port need some
2:12:07
memory. If I show you from my file again,
2:12:13
this particular port need some memory.
2:12:19
So in the deployment it is clear this board need
2:12:25
this much memory I don't have actually
2:12:30
okay I don't have so those who know our kubernet this file so when we launch a
2:12:36
container we can request for this memory
2:12:42
okay and this request is one has been given to the cellular
2:12:49
and suddenly find in all the node wherever you have this much memory available there they launch the port but
2:12:55
by chance guys I show you this demonstration where I launch the node worker node with T2 micro and T2 micro
2:13:03
don't have this memory available right so even though very quickly if you just go and check in your
2:13:11
EC2 page of Virginia right now I haven't given
2:13:17
all the instances or the worker node
2:13:23
with T2 micro. If you connect this guys because I believe I enabled
2:13:29
the SSM connect the account name would be here
2:13:35
if it's enabled. Uh you can see there none of the node has uh 600 MB around RAM free in the
2:13:46
worker node. Okay. So free an M
2:13:53
free is very less here 74 MB right so that is a challenge actually.
2:14:01
So what I'm doing I can increase the RAM right now but there's a one
2:14:07
way to flush your memory. So if you know guys about this Linux command procis
2:14:12
VM drop and I'm running this
2:14:17
here it won't run I believe. Let me log into my root account. So what this command is Linux command. So a lot of
2:14:24
guys memory has been reserved in your caches in your Linux system.
2:14:35
Sorry. So I'm I'm lening it
2:14:44
otherwise to change my cluster and increase more
2:14:49
different nodes. Now it look like a lot of RAM is free. Something is removed from here. But
2:14:56
still we don't have much RAM here. So to showing this demo what I'm going to do right now, let me decrease this RAM to
2:15:03
200 MB. Right? I don't know it will work or not because maybe this program might need more.
2:15:11
Okay. So let me don't remove this. Let me give this limit is 600 my request we are going 200 mean wherever we have a
2:15:18
200 mm free they give this to me and limit different thing this this port
2:15:24
if they need more than 5 600 mm we don't allow right is a kind of limit we set
2:15:29
but here request is important the error that was coming because of this because we requested 600 I don't have a that my
2:15:36
memory available that's what the failed so let me delete close this file let me
2:15:42
apply this file again. Apply.
2:15:48
When apply something will be changed updated. Okay. So the reconfigured
2:15:55
and uh what you will see
2:16:02
a port they're launching now they're fantastic. Right. So again
2:16:07
this is a pure cubis kind of troubleshootings. Uh but yeah, what are we looking for? It has been launched here. This has been
2:16:14
launched. Okay. So there's no head uh hurdle here
2:16:21
in launching this uh one. Let me also describe.
2:16:28
So I'm describing this particular
2:16:37
board and this board is a main port who successfully launch and run in one of
2:16:44
this computer. I believe they might run in this computers only or maybe some other also wherever they they find some
2:16:49
resources. Okay. So finally it's been launched.
2:16:56
Okay. Now one more thing we have to do because this particular
2:17:02
uh this particular port which one
2:17:08
is going to collect some information and you guys know in WS if you're going collect information some about some uh
2:17:16
resources some services they need role
2:17:22
okay so according to document they created this role for
2:17:29
Okay, role for us. So let me check this role
2:17:35
whatever they created. So I go to my again my
2:17:40
AWS service am
2:17:47
this role they created. We have to attach this role that is almost same concept what I show you in my EBS
2:17:54
topic. So this role is not there.
2:18:08
So we have to test the role. I think this role they have created somewhere above in the top.
2:18:16
I think this is the rule.
2:18:31
So I have to create the rule or it might not work otherwise.
2:18:37
So what I'm going to do guys I'm going to create this role very quickly. So uh even though you can give create a
2:18:44
role that give all the powers also that can also be uh also be uh done. So what
2:18:52
I'm going to do to make it a little bit quicker
2:18:57
uh well let me create this right this is document you can also use as it is. So
2:19:03
I'm going to create a file with this name. You can give any name does not matter.
2:19:14
So if you're using this option this policy already exist and you can't skip you can skip through step two. So
2:19:21
technically according to this document okay because I enable with EKL axis so
2:19:29
this is already exist right you don't have to do this so I'm skipping this step that's what they're talking about
2:19:35
okay some policy they have created maybe somewhere to find some name so let me skip this step they say it is already
2:19:42
exist okay so only thing is we have to
2:19:51
create the IM service.
2:20:02
So let me skip I think maybe this is there right? So let me move to this part right this is already launched so we
2:20:08
don't have to worry about right but uh we have to give the role what role they
2:20:14
have created uh if we need to give
2:20:20
so let's see in this minute in a minute I have to find out the name let me go go this part right okay so what we're doing
2:20:28
here okay so in a port what I launch
2:20:34
okay I'm enabling this facility. So if you guys know safe to evict facility in
2:20:39
autoscaling world if you know that's what we are doing here if you don't know that's not again very important right
2:20:46
now I'm I'm just updating this file
2:20:53
updating this file format let me take in the one single line
2:20:58
so the service we launch is
2:21:04
We are updating. Let me use my git bash. Sometime
2:21:10
this give command line give some issues.
2:21:16
Okay. So you can uh we can also edit this file and there's option guys here save to a we are enabling making it
2:21:23
false right this facility. Y is not again very required to know right now.
2:21:28
And uh and one more last thing is this deployment file.
2:21:36
This deployment file what I created with autoscaling program
2:21:42
we editing this file and this file the same file guys what we
2:21:48
deployment we created with replica one with this name in this file.
2:21:54
Okay, in this file what we are doing? Okay, we have to write these options.
2:22:05
Okay, somewhere here in the command line,
2:22:13
somewhere here in this command line. So guys this is a program actually who
2:22:20
is helping us to to increase and decrease scale in scale out your
2:22:26
cluster. Okay. So we are enabling this facilities. What the facility do for
2:22:32
you? You can read from this document. Okay. But technically what they're doing
2:22:39
they're increasing and decreasing the node in your node group. similar
2:22:45
node they are uh they are increasing. So
2:22:51
when you scale the scale in the same node group
2:22:56
okay and after they increase they're rebalancing the port
2:23:03
okay if as soon a new node increase right so older node if a four node
2:23:09
running so they rebalance it in the newer one so everything has a balanced
2:23:15
okay and and guys there's a lot of nodes also There's a system ports.
2:23:23
Okay, they're skipping those things in rebalancing or whatever they're doing, right? So they you can read this this again this autoscaling feature, right?
2:23:30
What they're doing is by skip enabling disabling. Okay, so that's what they're doing here.
2:23:39
So this option that enough available compute average all the zones, right? So they are
2:23:45
so they're decreasing sorry they are uh let me again reiterate so this node in
2:23:51
this node group they're launching a new node okay because node group wise we created
2:23:58
guys based on the ags right so we say three ags so two node we have so third
2:24:06
node they launch in the third ages so they're balancing in this way okay so we have a disaster plan properly that's
2:24:12
what this option is doing. Okay, that'll be right over here. Let me save. Close the file. And
2:24:19
you guys know as you close the file, automate the port redeployed over here.
2:24:26
Okay, and finally what you have to do. Okay, we have to check the current
2:24:32
version of the autoscaler program. The program what we are using here autoscaler program is a image. This
2:24:39
image is keep on updating new new versions.
2:24:44
Okay. So just go and find the version. Why? Because we are going to set this
2:24:50
image with a new version.
2:24:55
Okay. So point here is because we are using EKS. EKS also version
2:25:01
and this program also version. So if you're using EKS 1.5 version 25 then you
2:25:06
have to use this program with their respective versions right now in my EKS we using 1.25
2:25:13
version okay so you have to use a image of autoscaling what I'm using right now
2:25:19
with this respective version right so that's what they show you here is so
2:25:25
you have to use the version 1.25 25 whatever versions they have so
2:25:32
to find out yeah here it is okay so I can use this 1.25.1 and 2i.1
2:25:39
this particular version right uh so it is a way is a compatible right otherwise it won't compatible so I'm using this
2:25:46
command and this command
2:25:52
what I'm doing in same deployment I'm changing my image
2:25:58
okay of this particular program who is doing autoscaling for me and uh I'm
2:26:03
using 1.21 2.1. Why? Because this version is available in my GitHub
2:26:10
and this version is compatible 1.25 EKS version. It means you're using EKS
2:26:15
version 1.24 then the different image you use that is compatible to 1.24 and this can be this
2:26:22
image. Okay. So I'm setting this image here. Maybe I
2:26:31
was using by default maybe different image version. I'm updating my image name in my deployment what I launched
2:26:36
already. So if you see here again a port
2:26:48
this port is running a image and that image is using this version
2:26:58
1.1 25.1 because that is the version is compatible to my cluster because my EKS
2:27:05
cluster is 1.25 25 and this is the one who is collecting the this port is the one who is
2:27:11
collecting the entire information about my cluster. Okay. And you can see the log what is
2:27:18
going on. You're going to see.
2:27:26
Okay. So they they tell you they tell you that does they correcting
2:27:33
the information about the log or not. So they will tell you that autoscaling
2:27:39
program is mering your cluster load or not. So guys this is the entire cluster
2:27:45
load checking right tool. HPA that I talking about is only P basis right but
2:27:51
this is the program which program this program that we download from the
2:27:58
GitHub actually internally image the software is
2:28:04
is checking the entire cluster log because without the cluster log how can we increase it or decrease it. Okay. And
2:28:12
if you see the output something like this scaling out, scaling then down and this
2:28:18
kind of log if you see. Okay. That's what I think we can see
2:28:25
here. Okay. So, it's a live status scale out scale down.
2:28:32
Okay. So, it means uh setup is nice. No challenge. And what are they sort of
2:28:38
talking about? Let me control C. They're talking about we have three node
2:28:47
running. If you see the cubectl get nodes, three nodes running they can able
2:28:53
to detect. See here guys see they launch right. I think they found one one one
2:28:58
issue. They launch one more. Okay. So just now guys see here they
2:29:04
launch right? Because they found we require more. I tell you why they require more but it seems very
2:29:11
interesting as soon I launched they started working for us right so if you go to EC2 page very quickly where is my
2:29:16
EC2 it's fantastic the demo is done I using T2 micro T2 micro very small
2:29:23
guys so T2 micro is very small so guys automatically one more they launch right here
2:29:29
this node automatically launch one node right now they launch if you can see the
2:29:35
time if you have here yeah Yeah. So they launch
2:29:41
uh this one last one here. Right. Last one. This one launch automatically. Uh
2:29:48
right now who launch your autoscaling group.
2:29:54
Now if you say my des four right they change still minion max is 2 and 10 but
2:30:01
my des been changed to four. Automatically launch. Even though you see the activity also here you can see
2:30:08
there's one activity happen right now from 3 to four they change
2:30:13
automatically. Okay. And what they've done they change
2:30:18
and after they scale even if you keep on checking log there
2:30:24
you can see there in the log they scale it and because the scale now we have four nodes.
2:30:30
Okay. And why they change even though it doesn't increase the load. Uh the reason is guys because I was using T2 micro. T2
2:30:37
micro is very small node l RAM and CPU. Okay. And still some of the port was
2:30:45
running maybe internal port of C of Kubernetes the system ports they might need more resources till time we don't
2:30:53
have any autoscaling. So they might be in a hold or might be using limited resources. But as soon the autoc
2:30:59
screening come up all the port that is running behind the scene as system port also.
2:31:05
Okay. Now they got they they get the information uh and they ask them to launch
2:31:10
automatically. But it's working right. If it is working it means everything is is good. Is good.
2:31:19
Okay. And you can see the logs also again in the logs they might tell you some more information sometime if you
2:31:25
have okay so if any information to be seen
2:31:33
this live information I don't have but as you have something change either
2:31:38
scale in or scale down you will see in this log
2:31:44
okay they're checking the CPU memory everything they check
2:31:49
Okay. And according to the increase up and down. Okay. Now um uh this one thing
2:32:00
this is the current. So let me again brief for you right.
2:32:06
This is a program who capturing the entire cluster information
2:32:12
how much RAM CPU entire entire cluster overall using.
2:32:18
Okay. And this information they keep on sending to ASG.
2:32:26
Okay. Uh service a service duty is
2:32:32
based upon the information whatever they are sending.
2:32:37
Okay. They're sending they helping us to launch
2:32:42
by this particular facility.
2:32:48
Okay. to launch or terminate something like this. This is automating created by us by ektl
2:32:55
command. Okay. And they keep on launching. If you see in the instance management four
2:33:01
instance was launched. It was not there but they launch automatically.
2:33:07
Okay. This is one thing they do. Okay. Everything is done automatically by EKCL EK services.
2:33:15
But I'm trying to tell you this is what they done the setup. It means if you increase more load
2:33:23
okay one more node has been sorry if you increase more load in the
2:33:31
cluster one more node has been increased automatically but it will go till
2:33:37
maximum otherwise the maximum now I can use the ekle command also
2:33:44
and in the node sorry if we talk about your cluster
2:33:50
Okay. So in the a region called uh e US
2:33:56
east is to1. So in this region we have one cluster
2:34:05
one cluster and if you see the
2:34:10
data of node group if you guys remember I had created node group with have only one
2:34:17
one single so three uh three nodes right but if you notice automatically it will
2:34:24
increase to four increase to four desire change to four
2:34:32
okay of T2 micro same type they use
2:34:38
okay uh why they uh belongs to four because load increase but who is
2:34:44
increasing this four desired we don't we didn't done manually right we can do the scale command also but this is manual
2:34:50
right it done automatically who help us to automatically scale There's two way available guys in in EKS
2:34:58
to do the autoscaling uh AWS EKS
2:35:03
autoscaling uh services. One is Carpenter and one is
2:35:12
ASG. Right now we use ASG. How I know we use ASG? This is by
2:35:17
default even though we enable also in EKCL command or you can see from this page also they're using autoscaling
2:35:25
group to increase it the multiple way to check okay that's what they doing here
2:35:33
it's working now if you increase more load I'll show you how to increase more load then we'll increase it
2:35:39
okay and if you don't find any load further then they start uh shrink means
2:35:45
they start scaling they remove it automatically and the D capacity keep on changing on the fly. A point is now this
2:35:52
part is created. Now we don't have to worry about as load increase your
2:35:57
machines increase load decrease machine decrease more load increase you have to pay more
2:36:03
obviously and there's no load come up yeah automatic decrease you have to give
2:36:08
less amount but the point is not about amount here it is point is everything is happening automatically okay at the node
2:36:16
group level or at the node level. Okay. So outside the Kubernetes EKS is
2:36:25
managing with the help of other services like ASG managing your entire cluster scalability
2:36:33
or autoscaling but inside the A so inside the Kubernetes
2:36:40
okay increasing and decreasing the port is managed by HPA
2:36:49
managed by HPA. Okay. So here we have only
2:36:57
uh two replica running. I don't know it is not yet come. It should come. So
2:37:02
normally it takes some minute. Okay. There's a possibility the driver that I installed might not be compatible here
2:37:09
but it will work normally if your driver is compatible. Just try guys to find different software the way I download
2:37:16
from GitHub of metric server I'm talking about. might not be compatible
2:37:22
but try to do different version of myometric server. You can see some live status uh keep on coming here.
2:37:31
Okay. But if you increase the load on this port
2:37:36
okay automatically they increase the port. So let me show this demonstration to you. So for this what can you do
2:37:43
guys? This is in the cubernetes topic. Okay, there's a one
2:37:49
tool that will increase the load. This tool.
2:37:56
Okay, it will increase the um load.
2:38:02
Okay. So, what I'm going to do here, I'm taking this particular example here.
2:38:08
Same example what they have. So, they have some special application just for
2:38:13
testing the load. So what I'm doing
2:38:19
now let me delete my
2:38:25
all ports and deployment here in my current project my ID all the things
2:38:34
okay and I don't have an HPA
2:38:40
neither any ports and this is this is dimension of of
2:38:46
scaling the port automatically. So I'm using this example here.
2:38:52
Okay. So let me copy here does not
2:38:57
matter. Uh let's say
2:39:02
I'm just giving a name php yml.
2:39:08
Okay. So this is going to deploy one uh port
2:39:17
name PHP Apache from this image.
2:39:23
Okay. And uh this uh board is
2:39:30
requesting 200 MB CPU.
2:39:35
Okay. Now guys, this demonstration will show you both the demonstration together. If you don't have a node that has a 200
2:39:43
CPU mill means around around 02 CPU
2:39:51
available okay then one more
2:39:57
one more new node will be launched who will manage autoscaling group
2:40:03
okay so this example will help you to to show both the demonstration right so let
2:40:09
me save this and let me this is what we launched. Okay. And let me run this
2:40:15
right. So cube ctl apply this PHP file
2:40:25
and new deployment will run. It is a support they launch.
2:40:33
Okay. And now what we are going to do, I'm going to set the horizontal uh HPA here.
2:40:40
The way I already show to you with 50% average CPU we are setting here
2:40:48
with minimum minimum one. So when will you already have to go to 10 when when average CPU of
2:40:55
this port go above 50%.
2:41:00
Okay. So, HPA is created.
2:41:12
SP now guys coming up right. I don't know what last time my metric servers was working right. So maybe it's
2:41:19
possibility if you launch a uh newer then they come up maybe. Okay. I think
2:41:28
but is coming miser right so right now what do you see here this is now interesting demonstration now what you
2:41:34
see here at current point in time my port is running around 0.0 means around
2:41:40
very 0.0 something% of CPU they're using okay 0.0% 01% around they are using but
2:41:48
as this percentage go above 50%. My replica will change to two
2:41:54
automatically. Second port launch.
2:42:00
Okay. Second point launch. This is one thing. The same time guys I'm talking
2:42:05
about the nodes also the two things right. node kind see the auto scale down
2:42:14
because my my worker node might see or my entire cluster might see or my autoscaling group might see that we
2:42:21
don't need three worker nodes or four worker nodes where there not much there was not much load available
2:42:28
one of the reason maybe I launch I delete my some deployment what I was using till till now so that might be one
2:42:36
of the reason but my current status is pre-work on node everything come from
2:42:42
EC2 one port running inside the cubernetes
2:42:47
and current status from a port is this is the current CPU
2:42:54
percentage right now what we are doing here I'm going to increase the CPU load in
2:43:01
this pool for testing purpose and what you will see as the CPU load
2:43:09
increase go over 50% more port going to launch maximum 10
2:43:16
okay but let's say okay one port need one CPU completely
2:43:25
with three port need three CPUs but I have only three nodes running right and
2:43:31
one node by size here giving one CPU so if you launch four five nodes or sorry ports they will also increase your nodes
2:43:38
also that's also we can see. So two things together by this demo right if the load
2:43:46
increase in the port because client is kind of poor right then auto port increase the score
2:43:53
but replicable port increase autoscaling of the port by the help of HPA
2:43:59
port need more resources RAM and CPU so my EC2 instance will also increase my
2:44:05
cluster size increase with the help of autoscaling group two things will
2:44:10
scaling together, right? It's a two independent topic but is related to each other, right?
2:44:16
So, this is a guys load testing tool just to increase the load of the CPU
2:44:24
and by the meantime we also keep on uh running this command also.
2:44:30
Watch the command guys will show you live status continuously. Keep on reloading this page. The live status
2:44:37
going on. Okay. And this is a command we are using
2:44:42
for load generator just for testing purpose. Right?
2:44:49
So we are running this command and this command is increasing the load.
2:44:54
You can see a load will change that they the hitting a lot of time
2:45:01
application running inside it and you will see a load will increase in
2:45:06
some time maybe some minute maybe and as soon as the load increase your applic
2:45:12
[Music]
2:45:20
22%. And let me open one more thing. CubeCTL
2:45:26
get ports. This is the current status of my port. See here they increased right
2:45:33
because you see 200% right. This much load we have accordingly
2:45:39
they increase my four replicas here. Fantastic. Right. So as the load increase four replica now
2:45:46
they have one more. Maybe they have more loads coming. Right.
2:45:51
So more load going to come up here. This much load five increase or maybe this
2:45:57
five port the launching they need lot of RAM and CPU my current node does not
2:46:02
give you I don't know. So the mild possibility nodes can also increase
2:46:10
still they are not increased but but if they need a CPU that is not handled by these three uh things they will
2:46:17
increase. So this much load we have. So this much load can be handled by five
2:46:22
ports. So five port has been
2:46:28
created right. They are launching here the pending step by launching in some minute.
2:46:35
Okay. So still my applications keep on hitting.
2:46:40
So guys my point is this is what guys you can see as a demonstration. Let me launch stop this and relaunch.
2:46:48
This is the current status. So almost what I as 50% but it's go live almost five times. So accordingly
2:46:55
five uh port has been increased. That's what you can uh see here.
2:47:03
Now they start creating the container and let me check the status of nodes also.
2:47:10
Okay you guys see fantastic right? This is what I was thinking about right? Five nodes also launched right? That was the
2:47:16
reason there was so much time in the pending state because that much CPU won't get from the
2:47:23
older three nodes. Okay. So automatically your your cluster
2:47:29
autoscaling service wanted to as service and they ask to launch and because now AC take some takes second a minute to
2:47:36
launch and register to the cluster. So that much time they have pending state that time I show you the log you can see
2:47:42
they are pending state and as soon as the node launch and restore to the
2:47:47
cluster if nodes we have now total means two more they launch automatically now
2:47:53
port is is scheduled here somewhere in the new one
2:47:59
here they're scheduling now because load is keep on increasing
2:48:04
see here it's keep on increasing changing Okay.
2:48:10
So they might need more port. So they are launching one more port
2:48:15
here, right? Okay. And very quickly if I show the description
2:48:22
very quickly before they launch. So let me try describe
2:48:30
this board if they're still in the pending state. So what do you see guys? This is what
2:48:35
you can see right this is what the fantastic demonstration I want to show you by chance we can see it. So they say
2:48:42
we have five node available but all the five node is is completely
2:48:50
full is failed to schedule by my scheduleuler. Okay. One of the reason because of the
2:48:56
CPU is completely utilized. Okay. So we ask my scaleup service.
2:49:04
Who is the scaleup service? The cluster autoscaler.
2:49:10
Okay. And what they have done they change my
2:49:15
node group from five to six node. Maximum is 10 but they change to five to
2:49:20
six. Okay. Let me again describe this command.
2:49:26
And now what do you see guys? As soon as it change my scheduleuler will launch
2:49:31
here and this is the newer node they launch right so the schedule there
2:49:37
and how I know we can see this port has been scheduled and running
2:49:43
again more they're launching because my they keep on hitting my services
2:49:48
okay and you can see more node going to come up
2:49:55
come up right and if you go and See your all thing is everything is automatic
2:50:01
right the there's nothing your cluster also autoscale
2:50:07
okay autoscale sometimes they might also
2:50:12
because sometime they might say there's no load they minute also is scaled out
2:50:18
again this is the load they again initialize automatically and who is initializing
2:50:25
this initial this has been initialized by your auto uh autoscaling group.
2:50:31
Autoscaling group is the one who initialize uh this particular node. So
2:50:36
this service they use automatically they create by whatever the key they have.
2:50:42
Okay. Or you can just go and check in the auto scaling group. Now current distance is three says they
2:50:49
keep on increasing decreasing right. It's now it's become six right?
2:50:55
Six instance they connected and uh if you see here
2:51:02
three and three six instance is there who is managing these ports right now this node
2:51:10
is very small in size 32 micro but if node is big in size like some
2:51:17
some CB GB RAM so we can see sometime after some time we have more ports come up then then they keep on increasing it
2:51:23
right but it's a good thing I use we use T2 micro. So both the things we see
2:51:29
together. Okay. So this load generator tool keep on gen. But what you will see
2:51:36
after certain time after certain time
2:51:43
okay certain time uh when it goes above 8 so
2:51:50
10 then they stop. Okay. And then the 10 port will launch
2:51:55
and maybe in 10 port they launch in three node of 10 nodes does not matter. Okay. After they won't increase because
2:52:02
the maximum port is 10. Okay. So only thing after this if any load come up either the load will
2:52:09
fail or load will be slow down whatever case would be that's what we can say but this very very fantastic demonstration
2:52:15
we have created here. Uh two independent topic. Let me stop my load testing tool.
2:52:22
Okay. Now I stopped my load testing tool with Ctrl C. So this tool has been
2:52:29
stopped and now there's no load. So you see after some time you automatically
2:52:34
your port will decrease. Okay. Because according to my HPA I
2:52:43
I'll say that if there's no load just keep on decreasing till the one.
2:52:50
Okay. Now launching will do very quickly but by when I do scale in it takes some minute depend upon the bio settings we
2:52:57
can change okay and as the less port we have then we don't need these many nodes
2:53:03
so EC2 instance automate deleted by autoscaling group and who is connecting
2:53:10
to autoscaling group continuously by this port cube system if you go to name
2:53:16
space this one autoscaler
2:53:22
Okay. So, autoscale keep on checking all the node resources and keep on giving to
2:53:28
the ASG services and as service is launching and removing the node based upon what information they giving. Right
2:53:35
now we have six node and
2:53:40
these many boot but sometime because I now because I stop uh so stop the
2:53:46
service you can see everything is shrink down scaling automatically but point here is okay in this today's world of
2:53:54
devops tools or devops in in world okay we want as much as automation we
2:54:01
can achieve everywhere because we don't know when application goes is viral sudden sale coming up in the company in
2:54:06
a product. Okay, I want thing to be automatically scaled and all thing happen automatically and
2:54:13
scaled down also to save the cost also. Right? So there was a demonstration but my point is HP is a part of Kubernetes
2:54:21
that we can run directly in the Kubernetes right but HP is mostly highly not connected
2:54:30
but highly depend upon the nodes right worker nodes and who give the worker node right if
2:54:35
you manage your cubernetes then you have to manage a worker node manually right but if you're using a EK services
2:54:43
then the management of worker node done automatically by as a service.
2:54:49
Okay, a service here is called by service by EC2, right? So that is one of the one more good
2:54:55
reason why we need to run Kubernetes cluster or maybe open cluster on the top
2:55:01
of on the top of uh AWS cloud otherwise who will give the machines to you who
2:55:07
running this board who give this worker nodes to you that is the main thing uh
2:55:14
to know right okay so that's guys this very very fantastic demonstration right if you
2:55:20
those know h cubernet they might know HPA but I think that makes now makes more sense to have the Kubernetes run on
2:55:28
the top of AWS then your worker node machines can also be increase and decreased automatically
2:55:35
okay so that's all otherwise you can practice this demonstration I'll share this links uh my team will provide a
2:55:42
stepbystep PDF to you or we can use some of the link here I share this links to you uh so you can also uh set up this uh
2:55:50
requirement Okay. So today guys I would like to conclude till here.
2:55:57
This is a powerful demonstration. Practice this. Uh we need some more hours to complete this EKS because I
2:56:04
have some more things in my mind uh planned for you uh in this EKS. So we
2:56:09
will continue. Okay.
2:56:15
Uh uh we'll continue about u this topic.
2:56:20
So those who are part of EKS advanced training and those who are part of uh AWS advanced architecting this topic is
2:56:27
is there uh for you and plus for the earth student also. Uh so just this is
2:56:34
very very important topic very useful EKS is huge in demand in the market try to practice this try to add create more
2:56:42
example by yourself and u that's all
2:56:48
okay that's all uh from my side any u
2:56:55
any guys query if you have you guys can ask me
2:57:00
Okay, thank you ma. Great demonstration. Thank you vin.
2:57:06
Yes, uh you guys can use fluent DB also. Okay, but we have to find out
2:57:13
u does they have a connector available for this right for
2:57:19
the cubernetes. Okay, fluently also you can use uh instead of what the metric server by
2:57:26
default they use right uh him so I'll definitely try carpenter
2:57:31
itself a one big course al together to be discussed on but I'll try to talk
2:57:37
something about carpenter in my upcoming class of ease okay I will
2:57:44
talk you get the idea of carpenter something about it right but instead of ag service same thing we can do with
2:57:50
carpenter Carpenter give many more facilities what a zero would give you. I'll top it try to do it discussion very
2:57:57
quickly in my next class and try to show you demonstr also if possible okay
2:58:06
I plan to demo it using mayor we won't show you here in this training but we are having a specialized training I
2:58:13
think starting from this weekend I highly recommend especi
2:58:21
uh learning and service mass is very very important for have the entire uh setup implemented
2:58:29
right so we are studying I think this weekend coming weekend so anything and everything you know about want to know
2:58:35
about EKS you can join okay
2:58:45
bank is right
2:58:51
is possible deploy autoscaler in vanilla on top of AWS. I guess uh uh we can do a
2:58:57
uh by your own there's no challenge but uh point is you have to install the
2:59:03
drivers for a okay
2:59:08
so you install the driver for as autoscaling group right then it will work or you can use the carpet roles
2:59:16
okay there's a different way to do it so if you use vanilla kubernetes on the as
2:59:21
EC2 good thing is because EC2 so we have the roles attached that's No challenge of the security IM role we can test in
2:59:28
worker node but uh your worker node have to fetch the
2:59:34
information and connect to the uh a with a program called autoscaler. Autoscaler
2:59:40
program is available. You can install with cubectl uh command but how to
2:59:46
connect to as we have a driver to be installed. Okay that we have to do it by yourself.
2:59:54
uh what will be timing? So may your uh two timing will update you. So next weekend we have two classes. One EKS
3:00:00
class that your class will go on. Okay. And second the stew classes also start
3:00:06
next week uh weekend I can say we'll update you the time right about the ST
3:00:12
train. Okay. Um
3:00:19
I highly recommend Hmansu uh if you planning to go for STO just try to offer
3:00:26
Red Hat certification because they have a complete STO training with labs with
3:00:31
the exam boot uh boot camp with exam of red hat's one way you can go for second
3:00:39
thing is we are trying guys because not everyone want to go for redhead certific
3:00:44
cificacy because of the cost. So we're trying it's not yet finalized. Okay. We
3:00:50
are trying to come with only training not uh not not the red certification included. So that will we decrease down
3:00:56
the fees and anyone who wanted to go for training then that can be uh possible.
3:01:02
So we will update you if it's possible in this week.
3:01:10
uh Mayur will definitely try uh about the timing uh on Saturday and Sunday uh blessed
3:01:19
we'll try to start uh almost afternoon hours but let me see what can be done
3:01:25
right we'll definitely try okay uh I'll I'll uh I'll do uh bachu
3:01:33
bachu pow uh this demonstration of oidc and IM okay in the next week. Okay. So
3:01:42
we'll do this kind of demo also. Yes. Vene in STO service mesh training
3:01:50
almost all the application what we going to deploy it would be microservices application right obviously there won't
3:01:56
be micros service training this in individual training we have in uh that we do but a lot of thing of microser
3:02:03
we'll discuss there okay and uh and most of the demonstration in the service me
3:02:09
training will be a deployment of the microservices microservices app
3:02:20
can we launch the spark job of EMR using AKS cluster yes uh Manu you can do do so
3:02:28
okay even though we are coming up with the EMR training also soon that's called big data training and there also we will
3:02:34
talk about how to uh launch the jobs of Spark or maybe Hadoop MR jobs
3:02:43
uh on EKS services or EKS clusters right
3:02:48
on some particular resources. Uh so answer is yes what you're asking yes we
3:02:54
can do it but if you want to learn then if you belongs to big data world or want to learn about big data spark Hadoop and
3:03:02
EMR and these services so one training we're coming soon you guys can join the big data
3:03:10
yes uh Paris why not right that's the role of the node group right that's what I started the training with right node
3:03:17
group is a way we have a mix of instance type one node group has D2 micro other node group have other types of instances
3:03:24
and resources and facilities. So one cluster have multiple node group every node group have a different different uh
3:03:32
mix combination of the worker nodes. That's one of the demonstration one of the concept that I I show the demo also
3:03:38
in the in classes right
3:03:44
okay um u manu we might don't do any demo for
3:03:50
EMR here emvricus in this training okay
3:03:55
or let me think if I can do very quick demonstration if possible but I can give the commitment here right now we'll
3:04:01
we'll see let me check if I can do some A quick demonstration. Okay.
3:04:09
Uh any quick session? How about AP gateway? Are you aware of of any open source gator we can replace IBM data
3:04:16
powerhouse? Okay. Yes. So we can do mayor if you can if you remember uh
3:04:24
in my I didn't discuss in very detail but
3:04:31
there's a provisioners availables okay by default they connect a different
3:04:37
load balancer okay similarly we can connect a F5 load
3:04:43
balancers same concept is there in API gateway also Okay, by default in EKS
3:04:52
they have a integration with AP gateway service of AWS. But if you want to use a different API
3:04:58
gateway, so we have the provision available. For example, you want to connect with a F5 load balancer. So AP
3:05:03
F5 uh uh big IP API gateway or you want to connect to uh uh three scale
3:05:11
opensource API gateway from Red Hat and opensource community. So we have a provisioner availables connectors
3:05:18
availables okay that we can install in the EKS clusters or kubernetes cluster
3:05:25
okay provide some tokens something like this or keys that will be connected.
3:05:31
So answer is yes. We can uh we can connect whichever
3:05:37
uh load balancer or the API gateway you want to connect. Mostly they have the load official load
3:05:44
balancers sorry officer provisioners or the drivers available or you can find the third party mostly you can find in
3:05:50
GitHub to integrate with them. Okay. So process is something like this
3:05:57
not exactly the process something like this the way I show in the EBS classes right something like this the process we
3:06:03
have to do okay so
3:06:12
is this a may challenge because of the T2 micro
3:06:18
okay so T2 micro does not have so much RAM and CPU so I show you some eco command today you can flush your cache
3:06:25
from the T2 micro it give you extra memory or change your instance type then
3:06:30
your database will launch microscope will launch there okay so either we increase the memory of
3:06:38
the D2 micro by flushing the caches or uh change that instance type in your
3:06:44
node group
3:06:53
uh what So we need you need a YL file as YL concept I explained to you in my NES
3:06:59
classes but if you talk about YL with respect to EKCL this that is important
3:07:04
we will see in the upcoming classes I thought to discuss today but I think this topic little longer but this YL
3:07:11
topic we'll talk in very detail deep in upcoming classes there lot of advanced
3:07:16
WL file we can create for EKSL uh we will discuss about in next classes
3:07:25
Yes, maybe definitely you can check uh ingress controller demo we'll see in the next class.
3:07:32
Okay, when we go for more deep into the uh ALB topics then I will discuss this
3:07:38
inress controller demos. Okay, so that's all guys. I believe you like this topic.
3:07:44
Again we'll try every day to solve some kind of use cases multiple different flavor also we're trying to touch in EKS
3:07:51
but you now understand there's a lot of things to be discussed in EKS right and I'm trying to my best to cover almost
3:07:56
all the aspects of it okay that's all guys see you in the next class with some
3:08:02
more demonstration bye good and take

