ript


15:34
Good afternoon everyone. I hope I am audible. Today we sir will not join the
15:40
session. So today we will only have the revision session. Okay. So we will
15:45
revise all the concept whatever we learned from last two session. Okay. So today we will only revise last two
15:53
sessions. Okay. So let's start with revision. Let
15:58
me share screen.
16:08
I hope screen is visible. So let's start with revision from last
16:15
to last class. Okay, there you already learn about autoscaling. So let's revise
16:22
one by one. EKS helps to integrate with other service like EC2, EBS or EFS. So E
16:30
uh EKS integrate with ECU uh EC2 there EKS uh launch uh node and using EBS or
16:39
EFS they launch uh storage. Okay. So to implement EKS we use either graphical
16:46
method or EKS ctl tool. Autoscaling. For autoscaling there are multiple method
16:53
cluster node mode quad HPA or EK EKSCL YML. Master is like a serverless in EKS.
17:02
So what is mean by serverless? So serverless is a entire management of a master will run by uh AWS internally. We
17:12
don't need to think about that. Okay. So all the management of master will done by AWS and that is known as a
17:19
serverless. Okay. So adding computers is known as a horizontal scaling. So there
17:26
are basically two types of scaling. Uh one is a horizontal scaling and another
17:32
one is a vertical scaling. So what is mean by vertical scaling? So if we add
17:38
uh uh if we add more uh RAM CPU like this resource then in one particular OS
17:46
then that scaling is known as a vertical scaling. But if you add more computers
17:51
like replica that concept is known as a horizontal scaling. In Kubernetes we use
17:57
a horizontal scaling. If you increase the computer then then that is known as
18:02
a scale out. If you increase the pod or container that is known as a scale out
18:08
or if you decrease the uh resources that is known as a scaling. Okay. If load
18:15
increase or decrease AWS will automatically scale that is known as
18:20
autoscaling. It depends if load of the pod will increase or decrease that
18:26
concept is known as autoscaling. We have a scheduleuler in a master that is known
18:31
as a control plane. and they will schedule automatically. We have a scheduleuler program. Scheduleer
18:38
is a is one of the program in a master mode. Okay, that program will schedule
18:44
whatever we done as a user that uh that thing will schedule from scheduleuler
18:49
program whichever which run in master node. Okay. If you keep uh on checking
18:55
how much RAM, memory, how much um input output operation then that is known as a
19:01
matrix. Okay. So metric is nothing but uh we can say numerical information.
19:07
Okay. Means in in part in in particular OS how much RAM there um how much how
19:14
much RAM there how much CPU there uh how much is left uh okay the this kind of uh
19:22
numerical information uh information in is known as a metric. Admin keeps on
19:28
monitoring the node. they launch when the requirement come comes up. This concept is known as a scaling. When we
19:35
have to monitor u manual scale, we have to check manual information about no.
19:41
Okay. So, we have a total two-way manual or automation. If we uh scaling u if we scale the
19:50
manual then we have to check manually. Okay. uh manual manual checking means
19:55
there we have to go there we have to go then there we have to check how much RAM
20:01
how much uh RAM is left how much CPU is left then according to that we have to scale it we can have also some uh tools
20:09
to monitor like Prometheus and graphana graphana is uh providing information in
20:15
the form of graph okay Prometheus is we can say metric monitoring tool which
20:20
provide metrics we can capture the matrix from system any system any
20:26
operating system any database system for capturing the matrix we can use Prometheus tool but if you want that
20:33
information we want to see in a graph then graphana comes into the picture and
20:39
graphana is different tool Prometheus capture the metrics and graphana using
20:44
graphana we can see the graphs of that information so in AWS cloudatch
20:50
cloudatch checks the metrics on real time like Prometheus. Okay. In AWS we have a cloudatch.
20:57
Then if we click any OS then in below we can see the graphs. So this is a graph
21:03
of metrics. Okay. So you can see CPU utilization status check uh of fail
21:09
status check. Okay. So this graph uh comes from the cloudatch service. Then
21:15
if you search for cloudatch and if you click on that cloud watch and if you go
21:21
to metrics then you can see the different different matrix uh metrics of
21:26
EBS metrics of EC2 metrics of EFS whichever we want we have to just click
21:33
it then we can see the matrix of that particular resource or service. Okay.
21:39
Then how to do autoscaling of uh worker node. So autoscaling uh we already know
21:47
uh as soon as uh load increase then increase the um pod or as soon as load
21:53
decrease then decrease the pod. So this is known as autoscaling. So how to do autoscaling of worker node. This concept
22:00
is not managed by kubernetes. Okay autoscaling I'm talking about autoscaling. Autoscaling is not managed
22:07
by kubernetes. It is not the role of Kubernetes because Kubernetes is running
22:12
inside it inside in AWS. Okay. So autoscaling is managed by AWS.
22:18
Autoscaling is a feature of of AWS. Okay. So autoscaling is not managed by
22:24
Kubernetes. To autocale we have a service in AWS called ASG. Okay.
22:31
Autoscaling group. ASG means autoscaling group. If we if you need autoscale uh
22:37
then um if you need autoscale then AWS provide service that service name is
22:42
known as ASG which helps us to autoscale the instance. If you want to launch EC2
22:49
instance then we have a two way. One way is a manual launch means uh search for
22:54
EC2 service then go there then click on launch instance then click next next
22:59
click next fill that information according to our need that is known as a manual launch. Okay. And second one is
23:06
ASG. ASG is automatically uh automatically launch depends on load.
23:12
Okay. In ASG it launch instance keep keeps on
23:18
monitoring the instance and whenever load incre increases it autoscale. Okay.
23:25
It if any of the instance goes down ASG will launch new instance. Okay. So this
23:30
is the feature of ESG. If one of the instance goes downs uh due to any reason
23:37
then
23:44
matrix behind the scene it uses matrix.
24:11
is apply autoscaling group to the cluster. Okay. So using hyphen - name we
24:18
can provide the name of the cluster - ag accessy nodes - max. Here we pass
24:27
the how much node uh we want as a maximum. So here we pass 10. Then how
24:34
much nodes we want as a minimum. Then here we pass two using hyphen nodes
24:39
hyphen minimum uh keyword. Then hyphen nodes three. Here we pass three node.
24:46
Okay. So using h - node - type using this keyword we can pass type of node.
24:52
Here we pass T2.m micro using a hyphen node group name. Using this keyword we
24:59
can pass the node group name. So using - SSH - access.
25:06
Here we pass the uh SSS access and here we enable the SS uh enable the SSM using
25:13
iPhone enable SSM. Here we pass the version and here we pass the region
25:18
region name using iPhone region keyword. Okay. So using this image we deploy our
25:27
application using deployment. For this we use a cubectl create deployment.
25:33
So my ID here my ID is a name of the deployment and using - image name image
25:40
this keyword using this keyword we pass our image name
25:46
okay cubectl get ports this is this command is used for uh getting the ports
25:51
list of ports using cubectl scale deployments if we
25:57
see here cubectl get ports here we can see we have only one part Okay. But as
26:04
soon as we scale the ports cubectl scale deployments name of the
26:10
deployment and using hyphen replicas equal to five we increase our ports as
26:17
five. So as soon as we increase and if we display using cubectl get ports then
26:23
we can see it increase till five. Okay.
26:29
So next using cubectl describe command
26:35
we can uh get in extra information of the pod.
26:41
Then here we increase uh again increase our pods with six using replicas uh
26:49
keyword. Then here again we decrease our parts and if we again see the parts then it uh
26:58
there then we can see the parts. Okay.
27:03
So uh next one is a Kubernetes matrix.
27:09
Kubernetes metric server. Okay. So what is a metric server? Let me go below.
27:19
So autoscaling of a pod is not managed by EKS. Okay. So we already learned uh
27:25
in below uh in about autoscaling is is autoscaling of a pod is not managed by
27:31
EKS and AWS internally managed Kubernetes. We as a human being have to
27:37
monitor manually again and again. So we don't want manually. So for that we have to use a autoscale of the pod.
27:45
Kubernetes have one of resource that is name that is known as a HPA. HPA is a
27:50
horizontal pod autoscaling. Okay. Kubernetes provide one of the u
27:56
cubernetes provide HPA. HPA is one of the service like for uh replica set.
28:02
Okay. So like this we have one of the uh resource called HPA. HPA is for
28:08
autoscaling of the pod. HPA is a horizontal scaling of pod. HPA keeps on
28:14
monitoring CPU. As soon as load increase, it will scale out and as soon as load decrease, it will scale in. For
28:21
this minimum requirement is to keep checking the matrix. Okay. So, we need
28:26
matrix information because we are scaling. So we need certain tool that
28:32
keeps on retrieving the information of matrix and provide to HPA and for that
28:38
we have to write the rule for the HPA and for that we have to install matrix
28:44
server. So this is the use of metric server and that program will run on the
28:49
ports. Metric server will run on the ports. Okay. So metric server will start
28:56
monitoring the matrix of the port. So use of matrix server is is monitoring
29:02
the pod. HPA don't belong to EKS. It belongs to Kubernetes. HPA is a uh what
29:09
we say resource. HPA is a resource of Kubernetes not EKS. EK is EKS is just a
29:16
tool we can say for building the cluster of Kubernetes. We can also do HPA with YML.
29:22
So again if you go about
29:28
we can simply search Kubernetes metric server install then you can find the GitHub link. Okay
29:35
just paste uh copy it and paste it. Okay. Initially if you
29:41
[Music] initially if you see cubectl get ports
29:46
hyphen and cube system there you will not find any uh you will uh you will not
29:53
find any parts regarding um metric server but as soon as we apply
29:59
the code whichever we copy here as soon as we apply here cubectl apply - f okay
30:06
and if we again see the parts of cubectl get pods n cube system. So hyphen n cube
30:14
system is a name space. Here they they launch metric server pod. Okay.
30:21
And this matrix server pod will manage by or manage by deployment. Okay.
30:30
So here you can see
30:36
then if you uh for getting the HPA cubectl get HPA command is used
30:43
and for apply the autoscale to particular deployment we have a command
30:50
and that command is cubectl autoscale deployment name of the deployment and
30:56
how much pod we want as as a maximum. Then hyphen - max is equal to 10 10 part
31:03
we want and as a minimum using -imum keyword we can pass and here we pass as
31:09
a two and how much CPU percent we want then using - CPU percentage keyword we
31:17
can pass percentage of CPU here we pass a 60%. Okay. And after
31:25
uh after run this command and if you check cubectl get hpa then you can see
31:31
in my d deployment we apply autoscale. So as soon as it uh increase till 60%
31:39
then it will increase replicas. Okay. So
31:46
then next using eksl get cluster command we can
31:52
see the cluster and if you want some filter uh uh upon
32:01
uh some filter then ekpl get node group and if you want the node group which
32:08
belongs to my cluster ag cluster Then we can pass the name of the cluster using
32:15
hyphen - cluster. And if you want that cluster belongs to region equal to US -
32:23
east - 1 then we can pass uh the name of the region using hyphen - region.
32:30
Okay. And if we go to uh AWS console then EC2
32:35
instance console we can see all the uh node. Okay. Whatever we launch using the
32:43
EKSctl command that node we can see from AWS console that uh node launch in EC2
32:52
instance service and in that uh section if you go below
32:58
then you can see autoscaling main section in below you can see autoscaling
33:03
group then you can see EKS NGA this kind of name this uh auto Autoscaling group
33:11
you can see.
33:17
Okay. 1 minute. If you go below.
33:32
One minute guys. Cluster uh autoscaler keeps on monitoring and send information
33:38
to SS SSG. Cluster autoscaler is uh is one of the program. It keeps on
33:43
monitoring and send information to ASG autoscaling group. It capture the entire
33:48
information and that information sent to ASG and based upon the information it
33:54
keeps on launching and ter terminating the instance. So cluster autoscaler is
34:00
one of the program which monitor and send information to ASG. Okay. And again
34:07
if you go about
34:14
here uh we can find the code of autoscaler group and using curl command
34:20
we have to download that code. Okay, using curl - o command we have to download that code and if you open that
34:27
code using notepad then this is the code of autoscaler group autoscaler cluster autoscaler okay here we have to just the
34:36
edit the name of our cluster okay so this is the name of our cluster my
34:42
cluster - ag this is the name of our cluster we have to just pass our cluster
34:47
name which cluster we want to monitor okay so So here we have to just pass our
34:53
cluster and after edit we have to just apply that code again. So cubectl apply
34:59
- m and then name of the file. Then again if you see cubectl get pods - n
35:08
cube system in that name space we can see cluster autoscaler. So this is the
35:16
pod. In that pod um there is a pro one program running and that program is
35:21
known as cluster autoscaler. Okay.
35:28
If if you go inside if you go below here
35:34
we set uh for container uh here we set limits and request for the container. So
35:41
we set limit as 10 m uh 100 m for CPU
35:48
and 1,600 mi for memory and we set request 100 m for CPU and 200 mi for
35:55
memory. Okay, we set the uh request and uh
36:01
limits. Here we uh here we patch the deployment
36:08
using cubectl patch deployment.
36:15
Here we uh change the image of the deployment using cubectl set image name
36:22
of the uh uh deployment name of the deployment and uh this deployment in
36:29
cube-en system uh uh name space for this we have to pass the uh name space using
36:34
hyphen n name space okay then here we pass new um image name Okay,
36:46
for cubectl - n cube - system locks - m
36:51
- f name of the file. So this command loop is used for watch the locks
37:00
cubectl get nodes.
37:12
So uh here we we can see the HPA
37:17
increase and decrease of the port in is managed by HPA inside Kubernetes. Okay.
37:24
So increasing and decreasing of the pod is managed by HPA resource inside the
37:29
Kubernetes. HPA depends on worker nodes in EKS management of worker nodes. It's
37:35
done by EKS internally using ASG. So this is one of the example PHP.iml.
37:43
Okay. So this is the code of we can say deployment. Okay. So using cubectl apply
37:49
- f using this command we deploy one application. If you see the cubectl get
37:56
ports then this application is now in a container creating status.
38:04
Now here we apply autoscale using cubectl autoscale deployment command. So
38:09
name of the deployment and here we pass CPU percentage as a 50, minimum as one
38:15
and maximum as a 10. After that if we uh run the cubectl get
38:21
HPA command then you can see name of the deployment and this is a target we uh Q
38:29
as a 50% but uh now it has a uh 3% we
38:35
give 10 as a maximum parts parts minimum uh we give as a one and now we have one
38:44
replica. Okay. So we have uh one tool that that increase the load of the pod.
38:51
So if you run that tool using this command. If you run that tool and again
38:57
if we check the HPA
39:02
and if you check the HPA then you can see here our load will increase. Okay.
39:09
Here we can here you can see it will become 22. Okay. Again if you check then
39:16
it will become 249%. Okay again in next
39:21
image if we check okay it will become 249 and our replicas will increase by
39:28
five okay as soon as load increase then our replicas will increase. Okay.
39:38
So this is a practical of increasing the load increasing the load of the pod. As soon
39:45
as load increase then replica of that uh replica of that pod will increase.
39:52
So this is a practical this is a so by this scaling of the pod will be done
39:58
automatically. If load increase replica of pod also increase with the help of
40:04
HPA and if more resource are required it will autoscale the pod. If you want to
40:10
set the autoscaling of a pod then HPA. HPA is a horizontal pod autoscaling and
40:19
it is one of the resource which provided by Kubernetes not EKS. Okay. So HPA is
40:27
one of the resource which provide the autoscaling. If you want to scale automatically
40:34
then HP is one of the service and for autoscaling worker node or node in a EKS
40:40
then ASG comes into the picture autoscaling group. So this is a revision
40:46
of last to last class. Then now we will move to last class that is estrad class.
40:53
Okay. So this is a revision of last class.
41:02
When you use EKSCTL to create an Amazon EKS cluster in it uses AWS cloud
41:09
formation in the background to provision necessary resource for the cluster. EKSCL abstracts away from away many of
41:17
the underlying cloud formation details making it easier and faster to create an EKS cluster. Okay. So when we when we
41:26
use AKS CDTL command then it use behind the scene cloud formation uh service. So
41:33
cloud formation service is one of the service in AWS. We can say cloud
41:38
formation is uh infrastructure as a service. It decide the infrastructure.
41:44
It set the infrastructure in the cub uh in the AWS. If you want to uh launch the
41:50
EC2 instance, if you want to create uh VPC like this, if you want to do anything AWS, if you want to set
41:57
infrastructure in AWS by using code, then cloud formation comes into the
42:03
picture. Okay. So, uh EKSCL use behind the scene cloud
42:09
formation. Okay. EKSCTL uses cloud formation stacks to create and manage
42:15
underlying resources needed for the EKS cluster such as VPC subnet security
42:21
group IM role Amazon EKS service role. 1 minute guys.
42:34
It also create a necessary Kubernetes resource such as control plane worker
42:39
node work worker node groups as soon as we create cluster using eksl. So EKCTL use
42:48
the cloud formation service automatically and whatever we provide
42:55
uh using EKCTL command that command will uh use the cloud formation um service uh
43:03
in the uh AWS and create the VPC subnet security group control plane worker node
43:11
whatever okay the use of cloud formation behind the scene in EKS CTL simplifies
43:17
the process of creating and managing the EK EKS cluster. Okay. So it simplifies
43:24
the process of creating and managing the cluster. So there are multiple ways to
43:30
launch Kubernetes cluster in uh AWS such as YML code command line tool then cloud
43:36
formation. We already know about command line tool that is the EKSCL. So using
43:42
this uh using that command line tool we can launch uh AKS cluster but there are
43:49
or two method that is a YML uh code or cloud formation code. Okay. So command
43:55
line tool like EKSL provide an easy to use interface for launching Kubernetes
44:01
cluster on AWS. So we already use that tool ekl2. So we
44:07
already know about that. So using YML code to launch Kubernetes cluster is a more flexible option as it allows you to
44:15
define your Kubernetes resource in a granular way. You you have a full
44:20
control over the configuration of your cluster and can define your resource exactly as you need them. Okay. So if
44:28
you use a YML code then you have a full control of your configuration. You can
44:33
do uh you can whatever you can do. Okay. So you can directly use a cloud
44:38
formation code launch kubernetes cluster and manage the cluster but there are
44:44
some uh differences. So the command line may be less flexible than YML because
44:51
YML provide more options okay rather than command line as they
44:57
may not provide access to all the configuration options okay command line
45:02
may not provide all the options okay but YML is a more flexible option for
45:08
launching the EKS cluster YML code using cloud formation may
45:15
acquire if you use a cloud formation then you have a deep understanding of AWS and its services. If you use a cloud
45:23
formation then you have to know about all the service of AWS. Okay.
45:31
By default EKS CDL create a cluster will create a dedicated dedicated VPC for the
45:36
cluster. Okay. The VPC is a logical isolated network in AWS Amazon web
45:42
service. Each subnet within the VPC has a C block associated with it which
45:48
defines the range of IP address and that can be used within the subnet.
45:54
Worker node and ports in EKS cluster are assigned IP addresses from the subnet
46:00
associated with the EKS services. To launch an Amazon elastic Kubernetes
46:07
services EKS cluster using YML core, you can run the following command. you have
46:13
to just download the yml code. Okay, so this is a yml code. Using curl command,
46:18
you have to download the code and using cube uh etctl create a cluster command
46:25
you have to create a cluster for here you have to pass - f option f keyword
46:32
and in this keyword you have to pass name of the file. Okay, on the EKS CDL
46:39
website, you can find a section of the examples. Okay, go to EKS CDL website.
46:46
Then there you can find the examples um section. If you click on that example
46:52
section, you will land on GitHub page. Okay, Sum
46:57
already discussed about that. Okay, so you will find the GitHub page. There you can find repository containing a variety
47:04
of example yml configuration for different scenario. This example include yml code for creating a simple cluster.
47:12
So this website contain the yml code. So this website include yml code for
47:18
creating simple cluster. Cluster with spot instance cluster with multiple node
47:23
groups. Creating EKS cluster with Fargate. A cluster with custom VPC. a
47:28
cluster with multiple node groups and manage node groups and much more. Okay,
47:34
so using the yml file you can customize many things. If you use a yml then you
47:40
have a more choice. Okay. So each specific types of cluster uh
47:47
so using YML file you can customize many things such as enable specific types of
47:53
cluster control plane locks. You can custom custom customize VPC. Then you
47:58
can autoallocated IPv6 C for all subnet.
48:04
Then you can disable the public access to the end point and only allow private
48:09
access. You can create multiple nodes, node groups. You can launch Kubernetes
48:14
cluster on existing VPC. If you see here, okay,
48:20
if you see here, one minute, by default, EKS CL by default EKSTL
48:29
create a cluster will create dedicated VPC for the cluster. As soon as we
48:34
launch the cluster using eksl command then it will create dedicated new VPC in
48:41
the AWS for the cluster. But if you have already VPC and if you want if you want
48:50
in that VPC we want our cluster will launch then we can do and if you want to
48:56
do this we have to use a ML code. Okay. So if you want to do this we have to
49:02
just uh go there in GitHub then you will find the code. Okay.
49:10
Then more you can more customize node groups. For example if you want to set the label to the node then you can do
49:17
this. If you want to set the minimum size or maximum size for the node for uh
49:23
node affinity concept then you can do this for using um yml code. If you want
49:29
to access the CSI drivers then you can do this using YML code to set the label
49:35
after if if you have already a cluster and if you want to set the label to particular
49:41
node then this is a command cubectl label node and name of the node then
49:48
here you can set the label using key value pair. So env is a key and equal to
49:55
t is a value. So this uh you have to set the uh environment variable
50:02
uh as a key value pair. So after apply applying this uh command if you see the
50:10
label using cubectl get nodes - show labels then you can find the env equal
50:18
to de okay so what is the use of this label? So nur label enable kubernetes to
50:25
schedule pod on specific nodes. If you have a multiple nodes and if you want to
50:30
launch uh if you want to launch one uh if you
50:36
want to launch pod on particular nodes then you can launch using uh using the
50:42
label. We have to just pass the label at the time of launching the deployment or at the time of launching the pod. Okay.
50:50
So this is the simple use case of the node labels to enable all types of log.
50:56
Okay. So this is the command for enabling all types of logs. So EKSTL UT
51:02
utils update cluster - logging - cluster. This is the keyword for the
51:10
providing the name of the cluster and hyphen enable types equal to all and -.
51:18
So this is the command for enable all types of log. If you run this command
51:23
means in cloud watch all logs will come
51:29
whatever you uh run in in this cluster. Okay. So it allow all the logs. So using
51:36
this command you can enable all types of log. So that's it. If you have any query
51:42
then ask me.
54:47
Okay. So, uh good afternoon everyone. So, if you have any doubts in this session or in the previous session, uh
54:54
you can ask us here. So,
55:14
Yes, someone was asking about the uh that s will be joining. So only we have
55:19
revision session today. So I hope uh you all attended this session.
55:28
So you can ask me your doubts or technical or uh either non-technical if you
56:01
Uh, it depends. Romesh if you want to install it in your server so you can
56:07
also install the metric server that will get you the matrix of your cluster. Okay. Okay. So that depends on your use
56:14
case. Okay. So you can try uh having this
56:39
uh Raj Kumar can you please uh brief me more about your question I'm not getting
56:44
like you are saying what type of problem in production environment. I'm not getting this.
58:26
Okay. So, someone was asking here uh about the node group. So, we already
58:32
covered this. But in short, if I want to tell you what is node group. So, basically uh in your node group,
58:38
whenever you create a cluster, uh we have a node group which is already there. So, it is a default node group.
58:46
But uh whenever you want to create a cluster, we usually give a node group name. Okay. So basically everything
58:52
would be there in that particular node group and that node group should be have the same uh you know if you are doing it
58:59
in the instance and having a T2 micro instance type. So that should also have the same instance type and uh same AMI.
59:08
So that means uh that node group in that node group we have that you know cluster. So uh whenever you create a
59:14
cluster we give the node group. So if you remember we used the command uh while creating uh the you know cluster
59:21
that OC uh this uh get uh node group and then then the followed by the name of
59:27
that node group say nga. So if you remember that command so basically it depends on uh which if either you can
59:33
create your own node group or you can use the default which is already been there in that while you create. Okay. So
59:41
I hope you got it. Uh someone was asking so
59:46
um
59:53
Paris which demo are you asking for? Can you please tell me so that we can see
1:00:56
uh Rama Krishna it would be uh the shared in your respective group so right
1:01:01
now I can't uh tell you the exact date for that but yes uh I'll convey this to
1:01:07
team so that we convey you that when this uh program will start okay so
1:01:16
okay so Paris you're asking for autoscaling HPA So it would take uh okay
1:01:22
I'll try if we could get uh next time the revision uh so we will plan to have
1:01:28
this okay.
1:01:45
If a subnet has only 10 IPs and I need to create 10 nodes and two pods, this
1:01:52
will not be possible. Correct? Uh
1:02:07
yes, I think it is not possible. um because you are saying that uh it has 10
1:02:12
IPs and uh you need to create 10 nodes and two pods. So you have to scale
1:02:19
accordingly. I think you have to check okay scale uh the the number of nodes
1:02:24
and the pods and check if this works. Okay,
1:02:30
Dianu I'm talking about this question. Okay.
1:02:42
U Mayur if you could repeat your query. There are so many queries uh like um
1:03:33
So Rajkumar uh are you talking about uh some time client asking in an interview
1:03:39
give me use case what type of issue you are facing and resolve. So uh is the is
1:03:44
it the client asking you about uh this EKS? I am not clear with your question.
1:03:50
Please, if you could
1:04:26
So basically are you asking about the type of problem uh in the production
1:04:31
environment? Uh is this your question?
1:04:55
Okay. So talking about this uh like in your production environment if your client is asking that uh what are the
1:05:02
challenges you might face. So I'm taking in context of EKS and you know AWS
1:05:08
services. So uh that might uh maybe your uh sometimes it happens that uh your
1:05:16
cluster goes down. So that would be an uh challenge uh to handle and if your um
1:05:23
service is sometimes you know not enabled. So then also it might affect
1:05:29
your servers. Okay. At the same this these are the challenges you can face.
1:05:34
Okay. And um also uh if your uh you you know it is connected with everything
1:05:41
like if you you have a web server and it is connected with your uh you know backend services at the same time uh the
1:05:49
testing team is also working. So if uh something is missed by the testing team
1:05:54
and it is unable to test something so it might affect your uh working of your
1:06:00
application. Okay. So these are some challenges otherwise uh we have lot more
1:06:06
challenges of working in production. Okay.
1:06:11
I hope uh you got it. Yes. Welcome.
1:06:56
Okay. So, someone was asking me does HPA uh and contact to ASG. So, basically
1:07:04
I'll tell you what is the difference between HPA and ASG. Okay. So your HPA
1:07:10
is horizontal pod autoscaler that basically adjusts the number of you know
1:07:16
replicas of an application. So for example you have uh you want to scale it
1:07:22
for three replicas. So it will scale it to three. Okay. And what is ASG?
1:07:27
Basically ASG is when your pod fails. Okay. Uh so it schedules it with the new
1:07:34
nodes. Okay. So for that if you want if some time happens that uh we want to
1:07:39
have uh like our pod fail so it will autoscale okay our pod and make it up.
1:07:46
So in short I can say this. Okay. So I hope it is
1:08:40
uh Raj Kumar you are asking for how to host web application. So uh to this uh I
1:08:47
think you can use a load balancer you can integrate uh this load balancer if
1:08:54
I'm not wrong network load balancer you can work with and you can use engineext
1:08:59
okay uh what you're asking for otherwise I need to look into this and tell you
1:09:05
the exact steps what you need to do okay so I think you need to use your network
1:09:12
load balancer service and you have to host one web application. So you have to install this PHPDB okay in your uh
1:09:20
respective OS uh on which you are hosting your application and you need to
1:09:26
install EngineX in that and you have to connect that instance with your you know
1:09:31
uh load balancer service of that and
1:09:37
yes uh you can use this traffic use HTTPS port. Okay. So
1:09:47
first try to host it in your you know uh uh in your instance and uh install your
1:09:54
EKS ETL tool and connect everything in your EKS install this engineext PHP.
1:10:00
Okay. And then uh you can connect it with your subnets and the load balancer
1:10:05
service.
1:11:11
Yes, Paris. Uh if you give the minimum and the maximum value then it takes as a minimum value. Suppose I give at two uh
1:11:19
it as two so it will scale it to two as a minimum and if there is a requirement to scale it uh to scale your pods more
1:11:26
so it will scale at the maximum at uh say for example six okay so also at the
1:11:32
same time if you want to give the resource CPU percent so you can adjust accordingly okay so you have to give the
1:11:38
percentage of your CPU and it will uh take that into you know consideration
1:11:44
and um do add.
1:11:58
Okay. So, fine. I think I addressed all the queries. So, let's uh end the
1:12:04
session and u have a good day. Thanks. Bye-bye. Take care.

