ript


15:33
Okay. So, hello everyone. Good afternoon. So, uh let's start the
15:38
revision and uh so I won't be joining the session. So, we will be having only revision session today of EKS previous
15:46
session. So yes, I'll share my screen.
15:52
Give me a second.
15:59
Yes, I hope my screen is visible to you guys. So yes, in the previous session uh
16:05
you know we s discussed more about EKS autoscaling groups and uh we did a very
16:14
good practical you know. So let's see what uh what was there in the practical
16:19
and let's revise quickly. So okay so basically uh you know that EKS is
16:26
internally helping us to integrate with the other AWS services like EC2, EBS,
16:32
EFS. Okay. So to implement EKS we know that we have two methods either we can
16:38
go with the graphical method or we can use the EKSCTL tool. Okay. So you know
16:44
how to set up EKSCL tool uh that was uh discussed in the you know uh starting
16:50
sessions of this EKS training. So you can you know that how to download and uh
16:56
you know how to set up your environment for EKCTL tool. Okay. So we have autoscaling. So for that uh sir
17:03
discussed that we have uh three ways like either we can use cluster node or you can use pod uh that is HPA. We will
17:11
see what is HPA. uh in the upcoming part of this PDF. So
17:17
one is the pod hpa and other one is the ektl that is through yml file. Okay. So
17:23
master is uh more like serverless and entire management of master will be run
17:29
by AWS internally. So sir told you that uh we have a user and we have a image
17:36
for that board. So suppose we have a HTTPD image and internally like we have
17:41
a control plane here. So everything will be managed by you know AWS internally.
17:48
So we have to use cub cubectl commands for you know scaling and for
17:53
autoscaling. So we will see what are the commands for that. So master is more
17:58
like serverless in EKS and you know inter entire management will be uh done
18:04
by AWS internally for the master. So adding computers uh you know is known as
18:10
horizontal scaling. So if we want to increase the computer size you know so we can uh call it as a scale out and if
18:17
you want to decrease the size so we can use it as a scale in okay so if load
18:23
increases or decreases AWS will internally scale that is known as
18:28
autoscaling. So scaling automatically uh okay so that is known as autoscaling the
18:34
name itself give you the meaning okay so that is autoscaling so we have a
18:40
scheduleuler in master and that scheduleuler will schedule automatically
18:45
everything for that so if we keep on checking how much RAM memory and how
18:51
much input output we have then we can use uh then that for that we can have a
18:56
matrix okay so how to monitor matrix. So for that I think you might know we use
19:03
cloud watch for that. Okay. So admin keeps on monitoring the node. They launch when requirement comes up. Okay.
19:10
So this concept is known as scaling. So whenever there is a kind of requirement uh admin uh will keep on monitoring and
19:18
if uh the requirement uh there is no requirement then it will scale it. Uh
19:23
okay it will decrease the scaling part. Okay. So it will autoscale automatically. uh for that. So when we
19:30
have to manual scale, we have to check manually information about the nodes. So
19:36
we can also use some tools to monitor like Promethus Graphana for monitoring.
19:42
So basically Promethus Graphfana will give you a graphical presentation of how
19:47
you you know your CPU utilization is there. You can directly monitor your nodes with the help of these tools.
19:54
Okay. So yes, so but uh to monitor the matrix on real time we have a one
20:01
service in AWS that is called cloudatch. So you know we have this service cloudatch. So we have these clusters
20:09
running. So we can see the CPU utilization status and everything here.
20:15
And uh to go uh to use cloudatch we have to go to cloudatch service and then we
20:21
can uh you know see the we have lot of things like logs metrics uh okay so you
20:26
can check your matrix and logs of any of the service if you want. So we click to
20:32
matrix uh here and uh if you want to check matrix for EBS then you can go for
20:38
EBS. Uh otherwise if you want for EC2 you can choose the EC2 service for that.
20:45
So uh we used uh EC2 service to check and you can monitor directly from there.
20:51
Okay. So that uh that is all about this cloudatch. So now we uh s jump to the
20:58
practical that is uh we go to the EC2 service of AWS and we have few instance
21:05
running there. So how to do autoscale of worker node. So this concept is not
21:11
managed by KS it is not the role of Kubernetes. Okay. Because Kubernetes is
21:16
internally running inside it. So that is not a main part of Kubernetes. So to autoscale we have a service in AWS
21:24
called autoscaling group. Okay which helps us to autoscale the instance.
21:29
Okay. And if you want to launch EC2 basically we have two ways either manually we can launch it or we can use
21:35
ASD for that as well. Okay. So in ASD it launch instance and it keeps on
21:41
monitoring the instance. So whenever there is a load uh there is an increase in the load then it will autoscale.
21:49
Okay. So if any of the instance goes down, ASG will launch a new instance.
21:54
Okay. So it will make a new instance for you and it won't affect the you know
22:00
resources and everything there. So ASG keeps on monitoring the nodes metrics
22:05
behind the scene and it uses uh matrix behind the scene for everything. Okay.
22:10
So uh while launching the cluster we can launch worker node with the help of
22:16
autoscaling groups. Okay. So you know we uh sir discussed uh that we have a desired capacity as two. So uh and we
22:24
can use ASD uh internally we can uh connect with cloudatch to check and
22:29
monitoring the matrix and logs. So this was the thing discussed. Uh after that
22:36
uh sir uh the I think this diagram is also clear. s discussed that we have EKS
22:41
and it is internally connected with ASG service of AWS and we have ma one master
22:47
and we have worker nodes. So ASG will monitor all these you know it will scale
22:52
automatically if one of the worker node goes down so auto it will autoscale and
22:58
it will make the other worker node up. Okay. So I hope you got this and after
23:04
that you know the command how to create a cluster. So uh sir use this command
23:10
eksctl create cluster but we need to give ag access. So this command didn't
23:16
worked. So the correct command was this one. So we need to use eksl create
23:22
cluster and then we have to give your ag access. Then the name of the cluster uh
23:28
followed by you know number of nodes maximum nodes which you want to give. So we gave 10 as maximum and minimum as
23:35
two. Okay. And the we uh then we gave the node type as T2 micro and uh just a
23:43
second. Yeah. So yes uh then we gave a node group name that is nga and then we
23:50
gave the SSH access. And similarly we did enable SSM which we did in the
23:55
previous sessions and if you want to give the region you have to give the region say in our case we gave US east
24:02
one. So this will help you to launch the uh you know cluster uh in this
24:08
particular region US East one. Okay. So yes after that uh to
24:17
uh you know launch the uh you know deployment we need one image. So for
24:23
that we also use the image name this Apache web server PHP image. So,
24:30
uh, guys, can you hold for a second?
24:56
Okay. Sorry for that. Uh yes, let's continue. So uh we have a image named
25:02
Apache web server php. So whenever we want to create a deployment, we should always give you know the image name. So
25:09
you can use httpd according to your requirement. So here we use this Apache
25:14
web server PHP image. So how to create a deployment is very easy. You need to
25:20
give the cubectl create deployment and the name of the deployment followed by
25:25
the name of the image. Okay. So here we used Apache web server image. So we gave
25:31
that image name and your deployment my was created successfully. Okay. So till
25:38
now I think it is clear after that if you want to know how many pods running
25:43
in your you know environment so you can check get pods. So you can see that we
25:48
have this my uh which we created. So we have this my pod running. After that uh
25:57
if you want to scale any deployment so we have a command for cubectl scale and
26:02
then you have to give the deployment followed by the name of the deployment. Okay. And then if you want to scale to
26:09
it as five replicas, six replicas according to your requirement you can give the number of replicas if you want
26:16
to scale for that. Okay. So here we gave five. So you can see after using this
26:21
command group ctl get pods we were having five uh you know pods running
26:27
here. Okay. So you can see that there are five pods running for this since we
26:32
scale it to five. Okay. So you can also describe the pods. So you know the
26:38
command how to describe. So to describe you can use cubectl describe followed by
26:43
the name of your pod. Okay. So here we use this pod to describe that pod. Uh so
26:50
we gave the pod name here and then it described you the everything like config
26:55
map name and you can see the rest part here after description. Okay.
27:01
So yes so similarly we did for few more you know pods like same command we used
27:06
to describe the other pods as well. Check everything after that. Uh yes. So
27:14
after that uh if you want to scale it you know if you want to increase so we will uh this process is basically we are
27:21
manually scaling it like if you want to reduce you can reduce it to four here we
27:26
are increasing so right now we had five so now we increased it to six so it
27:32
scaled to six parts okay but you uh this was uh you know contain it was creating
27:37
the container so that's why uh the status was container creating so after
27:43
some time it will create. Similarly, if you want to scale uh decrease the replicas, so you can decrease it to
27:50
four. But you know, we are doing it manually, we are typing it again and again after like if you want six, we are
27:56
typing it again for six. And if you want to reduce it, we are doing four for that. So that is a manual thing you have
28:03
to do it again and again. So but right now if we are uh you know scaling this
28:09
uh deployment so here in this case we won't be getting HPA. So you can see we
28:14
use the command cube ctl get hpa uh so we were unable to found any of the
28:21
resource in that name space. Okay so for that uh we will see what uh we did. So
28:28
yes uh same command we used to get how many pods we have uh currently running. So we use this cube cl get pods command.
28:35
So to check uh in a particular name space or you can say in a particular project if you want to check that how
28:42
many pods are running. So you can give this hyphen n option and followed by the
28:47
name space. So since we want to check this uh you know number of pods running in cube system. So we gave it as cubectl
28:56
get pods hyphen n uh then cube system. So it shows you that how many pods
29:02
running in the cube system name space. Okay. So I hope till now it is clear. So now
29:09
we have uh sir discussed about kubernetes metric server. So we
29:15
installed uh you know we downloaded this cubernetus metric server. So since uh
29:21
sir wanted to show you the practical of uh how to see and how to monitor
29:26
everything. So we required this. So we use this cubectl apply this entire
29:31
command in our you know uh environment. So you can use the same command and we
29:36
will get our matrix server ready. Okay. So this was the command and uh you know
29:42
we got this metric server created. So to check uh what uh if we got that server
29:48
or not. So we used this cubectl get ports command and uh we want it we
29:55
wanted to check in the cube system. So we did this cubectl get ports hyphen and
30:00
followed by your name space that is cube system. Okay. So you can see that this
30:06
metric server 6 something was there uh in the status as running. Okay. So yes
30:12
we are good to go after that and uh uh then sir check how many deployment
30:19
we have in cube system. So it shows you that we have metric server deployment
30:25
there. after you installed that. So it is up to date and it is available to you
30:31
know it is ready in the state of ready. So yes so this was the thing discussed
30:37
and after that uh if you uh see that this uh you know metric server is having
30:44
the status running and is in a ready state. So yes it is one by one that
30:49
means it is ready to use. So after that uh if you want same things are scaled to
30:57
you know uh like replicas is equals to one if you want to reduce the number of
31:02
replicas and increase. So it depends on your on your requirement. Okay. So you
31:08
know the basic command how to scale your deployment. So to scale uh we use this
31:13
cubectl scale deployment followed by the deployment name and then the number of
31:18
replicas which you want to give. Okay. So yes, after that uh I gave this clear
31:26
image for this. Uh okay. And then uh if you want to uh you know uh we created
31:33
uh if you want to get HPA that is uh so you use this command cubectl get HPA.
31:40
Okay. But uh whenever uh how whenever you have to get HPA so HPA will be
31:47
created only when you autoscale any deployment. Okay, right now above we are
31:53
just scaling the deployment but now we will autoscale the deployment and you can see the number of uh you know HPA
31:59
ready after that. So how to autoscale your deployment? We have this command as
32:05
cubectl autoscale deployment. Uh then followed by your deployment name. Then
32:11
you have to give the maximum value. So we have 10 as maximum value. Minimum two
32:17
and we gave the CPU percent as 60. So you can see that it it is showing you
32:22
that horizontal pod autoscaler my autoscale. Okay. So that means this HPA
32:29
is horizontal pod autoscaler. So it is ready. So if you want to check how many
32:34
HPA are running there so you can use this cube cl get HP. So earlier we won't
32:40
uh we didn't get this you know anything there as a resource but after autoscaling you will get the HP ready.
32:47
Okay. So you know you can also see here that we gave minimum number of pods as
32:52
two. So it is showing you two and we have maximum as 10. So it shows you 10.
32:58
Uh similarly it also give you the CPU percentage as 60%. Okay. So if you
33:04
mentioned uh 70 then it will give you 70. So you know it is uh the same
33:10
command but just need to uh do scale instead of instead of scale you need to
33:16
do auto scale. Okay. So similarly if you want to get pods so
33:21
you use this cubectl get pods command here to check number of pods running.
33:26
And uh yes the same commands and uh it depends on your requirement. If you want the number of minimum pods to be reduced
33:34
or you can uh it depends on your requirement. If you want to increase the pods so you can give the maximum value
33:41
as you know 12 10 according to that. Okay. So same command just need to
33:46
change the minimum maximum and the CPU percent values. Okay. So yes so
33:51
autoscaling of a pod is not managed by EKS. AWS is internally managing KS.
33:57
Okay. So we as human being have to monitor manually again and again. So we
34:02
don't want manually. So for that we autoscale the parts. Okay. So since we
34:07
don't want to again and again type this uh you know scale it as replica is equals to four then six. So we want it
34:14
to do automatically and uh you know we want automation. So for that we use
34:20
autoscale of the board. So KS have one resource that is called HPA. HPA is
34:27
horizontal scaling of a pod. So HPA keeps on monitoring CPU and as soon as
34:33
the load increase it will scale out and as soon as the load decrease it will scale in. So for this minimum
34:40
requirement is to keep on checking the matrix. Okay. So we need matrix information but uh we are scaling. So we
34:47
need a certain tool that keeps on retrieving our matrix information and it provides it to HPA. So for that we have
34:55
to write a rule for HPA and for that we need to install this matrix server and
35:01
that program will run in the ports. So that is the reason we you know downloaded this H met matrix server so
35:09
that it it internally gives the matrix uh okay to the pod and it will run that
35:16
program with the help of that pod okay so it will autoscale so the matrix server will start
35:22
monitoring the matrix of the pod okay so HPA don't belong to EKS it belongs to
35:29
kubernetators so we can also do HPA with yl also But we did with a command. So you can
35:37
also do it with the help of yml. So you can see that we have two things like we
35:42
can either do it from pod that is by hpa or either we can do it with the cluster or third way you can use the yml file.
35:49
Okay. So after that if you want to check how many clusters are running so you use
35:56
this command ekl get cluster. Okay. So we will uh it will show you how many
36:01
clusters running. So you can see that we have a cluster ASG cluster one cluster two uh also it will show you the region
36:09
in which region you have that cluster. So yes you can check with the help of
36:14
this EKS uh ctl get cluster. Okay. After
36:20
that, uh yes, if you want to describe a particular node, so you have this
36:25
command as cubectl describe nodes and followed by your node name or whatever
36:30
IP they are written. So you can copy that. So here in our case when we do cubectl get nodes, we have these three
36:38
ready with us. So you can use any of that and you can check uh you know the
36:45
uh description of that. So you can uh describe these nodes. So since uh we
36:50
gave the name of our node group as nga. So you can see whenever you describe so
36:55
we have this node group name as nga. So yes you can describe that nodes as well
37:01
with the describe command. And u so to you know uh if you want to uh get the
37:09
node group in a particular cluster so you need to use this command eksctl get
37:15
node group followed by your cluster name. So since we wanted to check in cluster asg my cluster ag so we give
37:23
hyphen cluster and then we gave the name of our cluster with the region if you
37:29
want to check in the region say US east one. So you have to give it in this otherwise it will give you all the you
37:35
know everything which is there in that uh default region which is which that
37:40
command is taking. Okay. So yes just give me a second I'll
37:47
connect my charger.
37:53
Yes. So okay so after that we have uh cubectl get nodes uh to check the number
38:00
of nodes running at that time. So you can also check it with the GUI. So you
38:06
can see that we have three instance running. After that uh you can also check that
38:13
ASG is ready or not by the you know GUI. So you can go in the autoscaling groups.
38:19
uh we have this option here in this service EC2 uh in down part. So you can see autoscaling group and you can see
38:26
that we have this EKS NJ this ready. So you can check there also with this.
38:34
Apart from that you can also check. So we have the command. So discuss both the
38:39
ways how to check either you can use the command to check or we can also check with the help of GUI. So since we gave
38:46
the minimum value as two and maximum as 10. So it shows you that two minimum and maximum 10. Okay. So yes. So this was
38:55
the thing I think it is clear to you. So sir showed you the documentation of
39:02
Amazon EKS autoscaling groups. Okay. So you can go with this. And it says that
39:07
autoscaling is a function that automatically scales your resources up or down to meet changing demands. So I
39:15
also mentioned it. Uh okay. So same thing uh so you if you want to know more
39:21
about you know more description about autoscaling so you can use this fine uh
39:28
so I hope it is clear after that uh we uh you know used one um we did one
39:36
practical to monitor so let's see what was that so to get the number of nodes
39:42
you know the command cubectl get nodes okay so we use this uh you know GitHub
39:48
link and we curl this link. So it says that speed upload and everything. So uh
39:55
we created one cluster autoscaler autodiscocovery file. So we took it from you know GitHub
40:03
same thing but just we changed one thing in that uh in this file. So you need to
40:08
change your I think cluster name and uh here you change this you know cluster autoscaler. So we have this my cluster
40:15
ASD. So you need to just change this in this file and run that. So to get the
40:20
cluster we have this cluster name my cluster ASG. So we in this yml file we
40:26
will change the name of the cluster. Okay. So only thing you need to change is this file is name of the cluster.
40:33
Okay. So after that uh when you uh change the name so then after you have
40:39
to apply uh this yml file to run that. So we use this command as cubectl apply
40:45
- f and then the cluster autoscaler that yml file name. Okay. So that means your
40:52
deployment was created and after uh the deployment created you can use this
40:58
cubectl get ports command in this cube system to check uh you know so we have
41:04
this you know cluster autoscaler ready after you apply this file. Okay.
41:10
So uh you can also describe similarly we described the you know cluster
41:15
autoscaler. So you can see the description of that. So this was the
41:20
description after describing that and um yes uh we after that what we did we
41:27
connected one of the instance running. So we you have to click on connect and
41:33
be connected to the instance to check you know uh what what are the you know
41:39
free uh memory and everything uh about the resources. So we did few few of the
41:45
commands to check in that instance. So after that uh since uh sir checked that
41:52
so sir gave the memory sir changed the memory value to as 200 mi particularly
41:58
to perform that you know practical so we change it to 200 mi after that um same
42:06
uh thing we reapplied that and we use the same command as cubectl apply uh
42:12
hyphen f followed by your cluster autoscaler yiml file okay so it
42:18
reconfigured everything and then uh you can check the number of pods running in cube system. So we have the status
42:25
ready. Okay. And then uh what we did uh if you want
42:31
to use AM service so you can go and use IM. So sir discussed you know some point
42:37
about AM to manage the access for your AWS resources. We use the AM. Okay. So
42:44
yes uh we have uh this we use this command cubectl deployment cluster and
42:50
then we gave this you know sir took it from the you know uh your documentation
42:56
uh to perform this. So basically cluster autoscaler keeps on monitoring and sends
43:02
information to ASG that is autoscaling group. So it captures entire information
43:08
and that information is sent to the ASG and based upon that information it keeps
43:13
on launching and terminating your instance. Okay. So I hope this line is clear that it captures entire
43:21
information and that information is sent to ASG and based upon that information
43:26
it will keep on you know launching and terminating that means it is autoscaling for you depending upon your requirement.
43:33
Okay. So there are two ways to autoscale. One is we can use carp painter and other way was ASG which we
43:41
discussed during that session. So which you can say it as a default uh you know
43:47
so we can say default thing for ASG. Okay. So this was the file um in so
43:55
showed you. So yes we created this autoscaler image updated. We use this
44:01
command. So we took it from you know documentation and our deployment image
44:06
was updated. So after that to check number of pods in cubec uh cube system
44:12
we use this command cube system get pods. So if you want to check the logs
44:18
so you can also check logs uh so you use this command cubectl - n uh then the
44:25
name space then logs and since we wanted to check in the deployment cluster
44:31
autoscaler so we use this okay so we were able to figure uh the log part of
44:37
this okay so yes uh after that uh we you
44:43
know you can see this nga running in a North Virginia region. So number of you know my cluster ASD was having a running
44:51
instance state. So yes we were able to see this also at the same time uh we can
44:57
check the time that this cluster was whenever we launch this. So we were able to see the time of that cluster. Okay.
45:05
So uh yes same command repeated to check what we have. Okay. So to you know I
45:13
told you if you want to check the region uh in a particular region if you want to check number of clusters there so you
45:20
can use this ekstl get cluster hyphen region followed by your which region you
45:25
want to check. Okay. So we have this command for that. Okay. So if you want
45:31
to delete everything so we use this command cubectl delete all hyphen all.
45:37
So it will delete everything your deployment your HPA everything will be
45:43
deleted number of pods everything will be deleted. So if you want to delete everything related to pod deployment so
45:50
it will delete that. So increase and decrease of the pods is managed by HPA
45:56
in KS. So HPA depends on worker nodes in EKS. Management of worker nodes is done
46:02
by EKS internally using the ASG. Okay. So we have this php.imml file. So we did
46:11
uh cubectl apply uh and uh then our php apache service was created. To check the
46:18
number of pods we use this cubectl get pods. Similarly we autoscaled it to as
46:24
CPU percent and minimum and maximum value. So same thing uh sir did it again
46:29
uh you know to check uh okay. So what we did uh we did uh you know we increased
46:36
the load. So to increase the load we use this load generator. So we use this
46:42
command cubectl run. Uh we took it from the you know documentation to
46:47
particularly increase the load in that and you know it will internally autoscale everything for you. So we
46:55
purposely u generated load and you can see that we generated load into uh that.
47:02
Okay. And after some time you can see that uh use this command you know cube
47:08
cl get hpa uh and use this watch command to figure how many minimum pods are
47:16
there and replicas have been created. Okay. So you can use this command cubectl get hpa php apache and then use
47:24
this watch command. So to check okay so to watch uh that how many number of you
47:31
know if we have so see you you can see that 249% was increased at that time so
47:38
it will automatically scale up your number of parts. So if you parallelly check uh you know by checking cube cl
47:45
get pods so it will increase if the load decreases it it will decrease the number
47:50
of pods running. Okay. So you can see even the number of uh nodes you know so
47:56
the number of nodes got increased. So so we purposely you know uh generated the
48:02
load and after that we can figure that yes it is autoscaling uh the number of
48:07
nodes as well as the pods. So yes you can see the number of pods were increased. So it depends on the number
48:14
of load. As the load increase it will increase the number of pods. Uh and if it decreases the number of you know the
48:21
load then your pods will automatically be reduced. Okay. So this will save your
48:27
even cost and uh you know resources. So yes we use this cube get hpa. So see so
48:34
much load was increased and you can see this target having 47%.
48:40
So this was the thing. So also you can check cubectl get pods in this uh you
48:45
know uh to check the autoscaler is there or not in the running state. So you can use this cubectl get pods hyphen n
48:54
followed by the name of the project or your name space. Okay. So by this
48:59
scaling of pod will be done automatically and if the load increase replica of the pod will also increase
49:06
with the help of HPA and if more resources are required it will autoscale the pod. Okay. So this was the entire
49:15
practical which sir performed in the previous session. So
49:20
uh I hope uh you all if you have missed uh might be this is helpful for you.
49:26
Okay. So yes that's it. Uh so if you have any questions you can ask. Also I
49:33
would like to you know uh it is recommended in the market like uh in the industry that we should uh go forto
49:41
training whenever you do this EKS or you go with the service of AWS. Okay. So we
49:49
should always go with uh you know STO. So it is recommended in the market also
49:54
recommend. So if you want uh you know to join ISTTO training so we are having our
50:00
batch starting from today at 4:15. So this is the ISTO training. I hope my
50:06
screen is visible. So you might have gone through maybe some of you might have registered but just wanted to share
50:13
this uh so EKS is very helpful as well as Q is helpful of whenever you do you
50:20
know EKS and cubernet is something related to this. So you can see the you know content here uh how uh briefly will
50:29
uh you know discuss all these topics in the sessions of STO. So we are starting
50:34
it from today at 4:15. So yes that's it from my end. If you have any doubts uh
50:41
you can ask me here in the chat box. Okay. So I'm just um
50:49
um uh yes. So there are so many
51:00
uh
51:14
Anika uh you are from earth one okay I'll check with the team Anika um if it
51:21
is not been there then might they have shared in your hash3. So I'll confirm
51:27
with this and let you know about this with the team. So
51:32
how to download the summary? Please provide form to register. So Jendra I
51:38
think uh we have a discord channel for that and if you are a part of that discord channel we are providing the
51:44
summaries there and you have that option available there to download. uh this session uh PDF I didn't share but I
51:52
think uh apart from uh this session before sessions were already being shared in that okay in your discord
52:00
channel so I would be sharing this PDF um just after this session in your
52:05
discord channel and you can download it from there Okay.
52:22
Uh Moner uh sir is taking uh EKS service. So EKS internally uses
52:30
Kubernetes. So you need to uh you know uh follow few more like few of the
52:36
commands you need to take to you know learn EKS. So if you are you know
52:41
interested in that part so you can study. Okay. So yes
52:51
so many repeated things are there.
52:59
Yes Shiva we can we can autoscale. Okay. And uh we can scale
53:06
you can try scaling that node group you know. So yes
53:12
uh please guys uh if you are a part of earth one uh and earth two we uh we are
53:19
we I think our team might have shared you the links maybe uh if they might have missed or uh they thought of to be
53:26
shared that videos in your hash 13. So please check your hash3 portal you might have got that access of that videos.
53:33
Okay.
53:40
Uh yes Arvin we are providing you the you know PDF for all the sessions in
53:46
your hash3 also we are sharing it in the discord channel. You can download it
53:51
from your discord. Okay. So please I would recommend everyone of you to join the discord. It is very helpful for you
53:58
guys. You can ask your queries there and you can also download these documents.
54:03
Okay,
54:13
Shiva, can you please uh elaborate what are you asking? I mean uh how many sessions till now we did or uh I mean
54:20
what what is your question? I'm not getting this.
54:32
Uh Nishan this session was delivered last week uh last weekend by women sir.
54:38
So yes,
54:50
uh Mayor uh I think you might have uh been there in the WhatsApp group. Okay,
54:56
please share your number to me. Uh I'll look into this if you don't got the link
55:02
for STO training. Please share me your number. uh I I'll ask my team to you
55:08
know share your the link of session. So we are having at 4:15 so I'll help you
55:15
with that link. Okay.
55:22
Uh yes bindu we purposely didn't give this option to download from hash 13. So
55:29
that is why I'm asking all of you to you know go to discord channel and then download these PDFs okay you are only
55:36
able to
55:41
uh Rajes I think uh we have this option you know either if you are a part of
55:46
discord you just click on you know uh some arrow button has been there so you can click on that and your PDF will be
55:53
downloaded otherwise if you're not getting you know uh you can ping me I'll help you with that
56:04
Arian um okay for you guys I'll reshare
56:09
that PDFs again in the discord channel now please be aware and download it so
56:15
till now whatever we have completed I'll reshare each and every PDF in discord
56:22
also if I could help you uh by providing you know hash3 13 access uh and uh if
56:29
you are able to download so I'll check with the team if they allow so you can also download it from hash3 okay
56:38
uh form uh okay so what the ones who haven't joined the discord I'll ask my
56:43
team to reshare the form again and please don't miss now and uh fill it
56:48
okay so that you've been added to the discord
57:10
for the earth learners. We are looking you know we are looking for how we can provide you the uh you know PDF and uh
57:18
the recorded session. I think recorded sessions won't be provided to you guys but yes we can share the PDF to you. So
57:25
either we will bring you to the discord and we'll share there. Uh otherwise
57:30
please wait we'll share this. Okay.
57:36
Uh the ones who haven't uh you don't have the discord uh you know this um you
57:43
are not a part of discord then we will be sharing a form. you have to fill that form and after that uh uh you would be
57:51
able to access the discord. Okay.
57:56
So many repeated um
58:09
as I said uh for earth we are not pro earth one and earth two I think we are not providing you in the hash 13. So you
58:16
have to take these sessions live. Okay. So yes.
58:36
Yes. Uh for 2nd April someone is asking me again and again. I am seeing you again that please uh
58:44
it's been shared in your group. maybe you might have missed otherwise if not
58:49
uh I'll recheck with the team and help you with that session okay so please
58:55
don't retype it again and again it is very harder for me to you know check
59:06
yesun Kumar uh if you are not a part of discord I'm saying you that our team
59:12
will share the form and now After that you would be added to that discord channel. Okay. So we have shared it
59:19
previously. So some of people some of you might have been a part of that channel. So if you are not a part of
59:26
that we will reshare it again and you can fill that form and you will be there in the discord. Okay.
59:39
So,
59:47
so I think we don't have any Google Drive for this. Uh, okay. So,
59:54
uh, Na, whoever you are, we don't have this.
1:00:02
Um, Rajes, I am not taking care of this. I'll I'll check with the team uh why
1:00:08
this has not been there. So I'll get in place by today itself. Okay. So yes. So
1:00:15
any technical doubts if you have you can ask me that otherwise discord and you know the earth one and earth two
1:00:22
recording and all will be taken uh will be in placed by this weekend. Okay. So
1:00:29
don't worry you will be getting everything. So I think the recording won't be provided to earth learners earth one and
1:00:36
earth two but yes uh we can help you with the PDF. So either we will share it
1:00:42
in your respective groups or uh we will get you in the discord channel so you
1:00:47
can download it from there. Yes Parish if you are a part of EKS
1:00:55
advanced training we are sharing this in your hash3
1:01:00
portal.
1:01:25
uh Paris uh how long this would be available for you you know you can watch
1:01:31
it anytime so right now we don't have any restriction that we would be providing it for 24 hours or what but uh
1:01:39
you can watch it anytime it is available for you. Okay. So you can watch if you
1:01:45
are a part of EKS advanced training then we are sharing it in your hash3. Okay.
1:01:51
Otherwise for earth we are not sharing earth one and earth two.
1:02:04
Rahul, you might have missed that or if you haven't missed that, uh we will
1:02:09
check if we could provide you the recording of that session.
1:02:29
Uh yes, thanks BU. So I'll take five to 10 minutes more Q&A.
1:02:36
If you want you can ask me any of your technical doubts. If you are feeling you know any problem in downloading or you
1:02:44
know uh in setting up your EKS cluster EKS ctl tool you can share your screen
1:02:49
and I'll also help you with that. Soft.
1:03:34
uh yes mayor I have noted uh your number you know and I'll help you with Back.
1:04:15
Shoubam. Uh yes, I just shared you that if you have missed that session or maybe
1:04:23
our team might have uh not shared you the you know link of that session. So we
1:04:28
will help you with the recording for that. Okay. So please uh don't write it again and
1:04:34
again. It is very harder for me you know to check each and every command uh you know your comments here.
1:04:43
So someone was asking me one question you know related to EKS. I just missed
1:04:49
that. I'm just looking for that, you know, comment.
1:05:06
So yes, we can scale and you know auto scale according to that. Okay. So
1:05:12
someone was asking me a question. So I
1:05:25
U Paris uh can UI be used instead of CLI? So if you create a cluster you know
1:05:33
by UI then also you need to you know if you want to scale it you have to go with
1:05:39
this um CLI command. So it is better and it is recommended to use CLI. It is I
1:05:45
think pretty easy for you also. We can use it but u but at the same time if you
1:05:51
use CLI then it is more easier for you to you know check everything.
1:05:57
So we can create a cluster um you know manually by going to the web UI but to
1:06:03
scale and autoscale uh you can use the CLI. Okay.
1:06:19
Uh I think Shiva this demonstration took in the you know previous session
1:06:24
what I discussed in the you know this s uh this revision. So it is a little
1:06:30
harder like I will take if you say in uh the next revision session if it is
1:06:36
possible for me then I'll take this practical okay if it is possible. So
1:06:42
but the same thing sir did in the previous class. So you can you know um
1:06:48
practical you perform your practical and if you face any challenge in that you can ask us.
1:07:00
Yes. So you you do the practical Shiva and if you face any challenge there
1:07:06
we'll help you with that. Otherwise uh if you want uh we'll try if I could help
1:07:13
you you know with the practical demonstration for that again. So yes
1:07:32
no it is not equivalent. uh AWS have EKS
1:07:37
you know elastic Kubernetes service as a service you know but Kubernetes is
1:07:44
different I think swimmels told you that on the top of you know EKS we use this e
1:07:49
on the top of cube uh ctl you know uh we use this cubectl tool so on that we use
1:07:56
this cubernetus commands so for checking the number of pods number of you know uh
1:08:02
nodes we are having cluster we are having. So for that so basically they are not equivalent.
1:08:09
So um you can compare you know by checking
1:08:14
both the uh you know dashboards you can easily compare
1:08:20
what's the purpose of both the dashboards of EWKS and cubernetes
1:08:30
na whosoever you are I don't know but uh please uh hash 13 I see the Google drive
1:08:36
link
1:08:42
Can you please help me with that, you know, drive link? If you could help me with that link, I'll check. Okay.
1:08:58
Okay. I'll check this uh you know link and
1:09:57
Uh yes, we can create a PL uh you know private cluster Muhammad Van but uh you
1:10:04
must have to give you know the end point for that your you know private access
1:10:09
should be enabled. So that you need to check the settings for that and you need to give the access for this and uh you
1:10:16
need to give you know you need to enable your private access for that cluster and
1:10:22
at the same time you are asking um
1:10:27
yes you can pull image from your docker hub uh I think there won't be any
1:10:33
challenge for that so you can check uh okay you can easily use uh you know
1:10:40
docker hub to pull any of the image if you want.
1:10:51
Okay.
1:11:26
Spin up you mean to say you need to scale le uh I mean you want to spin up
1:11:32
that means you want to increase you want to check how many instance should be increased I'm not clear with your Sh.
1:12:09
So you wanted to check I think uh the you know uh it says that if my pods are
1:12:16
using by default requested resources within the worker nodes is there any formula to calculate how many pods will
1:12:23
I be able to spin up within a particular instance type. So you want to manually uh you know you want to manually scale
1:12:30
it up. So you can use it you know we have this command oc scale or you know
1:12:35
deployment. So you can scale according to that. Otherwise if you want to internally check then you can use help
1:12:42
command for that to check. Otherwise I also need to figure what command is this.
1:12:54
So you know it depends on how many uh val you know minimum maximum value whenever you autoscale we give this
1:13:00
value dish uh you know minimum value as you know suppose six and maximum value
1:13:06
as 10. So it depends on that and it will autoscale according to the you know load
1:13:12
suppose the practical which we did. So uh there was an increase in the load. So it will autoscale the pods and uh
1:13:20
suppose it requires more pods. So it will autoscale it to more pods and if
1:13:26
there is no requirement of pods so it will reduce the number of pods for that.
1:13:31
So that is the purpose of giving minimum and maximum value of pods. Okay.
1:13:40
No Jendra we are only having revision session. So yes.
1:13:57
Uh yes dish if you are going with the default values uh I think you are able to you know do that. So
1:14:06
if you are asking me for a particular command for that to check. So um
1:14:13
I have to check that but you can take help also uh to you know figure that but
1:14:19
that will autoscale uh you know internally. So why do that? Why you wanted to do that?
1:14:39
No NA we won't be having session today we are just having the revision right now so yes
1:14:55
yes parish what you are asking uh yes to that question okay
1:15:17
Okay. So you are saying that uh in T2 micro instance I was able to spin up to
1:15:22
maximum of 18 pods. Okay. Uh yes. So we can uh you know do that. So it have a
1:15:28
limitation that you can scale it to 18 pods. So that is AWS you know uh that
1:15:35
might be a thing for AWS. So it depends.
1:15:49
Okay. So fine that's it. Um let's uh end the session and I hope you
1:15:56
understand the sum. So, let's uh end the session and thank you guys. Bye-bye.
1:16:03
Have a good day.

