ript


13:41
Hey guys, uh welcome back and uh let's continue with new topic and very
13:47
interesting service also. Right. So today the plan is we are going to
13:54
uh discuss and do a multiple demo on uh
13:59
one more very powerful service in the world of AWS again more related to serverless that's
14:06
called AWS farget okay so this is the one service we are going to deal this
14:12
service is more related to container technology and in the container technology it is
14:18
related to serverless concept Okay. So what I mean by this? So in AWS
14:24
guys there's a mostly there's two different way to manage your container
14:33
technology. I'm saying docker container or maybe whatever container management engine you
14:39
are talking about right. But there's two main um way to uh manage container related
14:48
technology. Okay, there may be many more other things also but that they take two dedicated
14:53
services uh for the containers right and what is container I believe you guys know in the
15:00
simple term you can think like a docker container okay so one way is there's one service
15:06
called ECS elastic container service is purely dedicated for the container
15:12
management okay and one is EKS that you guys know
15:17
we have discussed in my last classes about the EKS. Okay, it is again purely
15:23
about container management but it is not directly managing the container. Okay, this service is actually behind the
15:29
scene managing uh Kubernetes or I guess the Kubernetes cluster and
15:37
Kubernetes is the one who actually managing the container finally in a way of ports
15:43
that you know. Okay. But finally, EKS is also one service that is handling the
15:49
container. Okay. So, in both the services either
15:55
you talk about ECS or you talk about EKS, both the service is given by AWS
16:01
but there are fully managed services. Okay. Some of the part you have to
16:07
manage. But if you want to go for fully managed services that is also sometime known as
16:13
serverless services then what we can do we can integrate
16:19
these services with a tool that help these tools or services go fully managed
16:25
or serverless. So what we can do there's one tool again in the container technology but that
16:31
makes the container management services in AWS make them serverless.
16:39
Okay. And name of the tool or name of that maybe kind of one subcomponent is
16:45
called farget is not an individual service. They work with them. So I can say ECS
16:54
with farget or farget on ECS whatever the way you say. So if you want to make ECS service
17:01
serverless pure serverless then you can integrate with farget.
17:07
Okay. Similarly, if you want your EKS service fully serverless, then you can integrate with [ __ ] or use with the
17:14
faret or fet on on ECS whatever the way you want to say here. So this guys this
17:21
class is about not about ECS at all. Okay, today we are going to discuss how
17:27
we can uh make on EKS service.
17:36
Okay. So this is a plan for uh today's class. So so we are going to learn not
17:42
ECS with farget. We're going to learn forget on EKS. How can we implement
17:48
this? So main topic for today is forget.
17:53
Okay. But not just the forget it's forget on or with EKS service. That's
18:02
the plan. Okay. Let me search what happened. Let me write in this way. E
18:07
KS. Okay. So the minimum requirement guys for today's class is is you should know
18:15
a little bit about container. I believe you know you should know a little about Kubernetes or the EKS. That's also I
18:20
believe you guys know that what we have discussed in my last classes in detail. But basic information of EKS or
18:26
Kubernetes more than enough to understand the today's topic called forget on EKS.
18:32
Okay. But what forget will do very quick information is a kind of service that will help for doing
18:39
something on the container world but that make the container related services serverless.
18:45
Okay. So what I mean by this so to understand this service even though you
18:51
can see here fet is compatible with both ECS and EKS we are planning to go for
18:57
EKS. Okay. So what IDM
19:02
so if you guys talk about EKS you guys know what is EKS. AK is just a service
19:08
is just a service. Okay. But this service help us to
19:15
manage complete Kubernetes cluster
19:23
and to manage almost all the component of Kubernetes cluster. Okay. So I can use EKS to launch the
19:31
cluster. How? Maybe I can use graphical way or maybe I can use command line way
19:37
to launch. So guys, let me log to for my account. Just give me one minute.
19:43
Okay. So there's a multiple way to launch the uh to launch the format EKS
19:53
to use the EK service and from the EKS service to launch the Kubernetes cluster
19:58
right there's multiple way one simple way is I can let's say I'm in Mumbai region I can go to e elastic cubernetes
20:05
services over here okay over here and from here I can just
20:11
go and launch the cluster for my B has two clusters running right now. Okay, I
20:17
don't need this. Let me delete. I don't need this guy. F1 cluster.
20:24
Let me delete. And same I also don't need this far get cluster.
20:30
Let me delete this. So from here you can just actually go you know uh you can go and delete this.
20:38
Right. So to delete you to first it is something about I'll delete it
20:45
sometime so it's not important right now. So from here you can launch it right or other way is from my last class
20:50
what we have discussed right there's a way to uh launch the cluster with eksl
20:57
command. So this command I already have set in my uh system by this command I
21:02
can see how many cluster is running. uh so there's two cluster that's what we
21:08
can see from here this command we can use to create the cluster also that you guys know so I can
21:13
say ekctl create command cluster then you can give the name of the cluster let's say this
21:19
is a Linux world cluster one okay hit enter it will create the entire
21:27
EKS cluster that you know okay my plan is not to
21:34
launch only EKS cluster I want to launch EKS with far
21:40
so why far this is our main topic for today guys today the plan is plan is
21:45
forget on EKS what forget that's what we going to see in some minute but but to
21:51
it will take some time guys to launch so what I'm going to do I'm running one command here this command I will explain you in some time but let me run one
21:58
command called forget so this command you know this command will create a cluster with this name but this first
22:03
term I'm I'm uh passing this option here hyphen for farget. So it will launch a
22:09
entire pk cluster with the faret. What a forget? That's what I'm going to discuss in a minute. So let me launch this
22:15
command at least because it takes some time to create. Okay.
22:22
Now what is forget guys? So what ease you know it is a service who going to
22:30
create the uh Q&A cluster for you. But what they do
22:37
okay this service guys will create one master computer
22:43
a controller node a master node is going to control entire cluster. So typically
22:50
it's called control plane they create. Okay. Same command they also create if
22:58
you ask them to create but is optional but they also create EKCL command or EKS
23:04
cluster will also create your worker node or a compute node or slave node
23:09
what do I say and the node where finally your
23:14
container is going to run. So these are node where we have container engine or this is a node where your final container that you guys know is known as
23:21
going to run and deploy. Okay. Now point here is okay so this is
23:29
the control plane and this is the uh worker node A worker node B and worker
23:36
node C. So this is kind of uh this is a system where uh my port is
23:43
like a container container like a operating system operation need uh resources like RAM and CPU compute
23:49
resources. So a compute resources if any port need come from the worker
23:57
node. Okay. And there's also one thing you guys might know. Whenever you launch a
24:04
port, you need a image, right? Container image docker ms. And from the image when
24:09
you launch a port, this called port launch or port deployed.
24:15
While deploying the port, okay, while deploying a port that is you
24:21
guys know again it is like a container. So the available in the container okay container say I need 1 GB RAM I need
24:31
two CPU core and all lot lot of many things right
24:36
but whatever the request they demanded let's say the compute resources this compute resources p will get from the
24:43
worker node worker node
24:48
it means right now in this worker node If you have a 4 GB RAM total
24:57
okay and a port is looking for 2 GB RAM because the worker node has so they can give to the port
25:03
or they allocate to the port okay but one thing guys
25:08
how much resources your port need has to ask
25:14
okay so if you know a little bit about the the yl file of the cubernetes there's a
25:20
one keyword called request keyword I show you today. Okay. So when you launch any port that
25:26
time you have to request. Okay. So if you don't request they give you some kind of by default but we can
25:33
request. So it is something like this. Uh it is something like this. Whoever create the
25:40
image or developer they might say this image is some kind of Java code that required less 2 GB RAM. So from from uh
25:47
if you launch a port or the container so you have to request
25:52
minimum 2 GB RAM okay a minimum 2 GB RAM or some CPU if
25:59
you don't request your port will fail container will fail they won't able to run your process inside this
26:05
so this is the duty of the of your IT team
26:11
okay IT team to to uh to write in the YML code or
26:19
manifest file or YML code while launching the port directly or while the deployment or some way when we launch
26:25
this code we're requesting 2 GB RAM because we're requesting 2 GB RAM means
26:31
I need 2 GB RAM it means if I need 2 GB RAM it means I'm going to launch on the
26:37
top of worker node so worker node have to provide me the point here is okay if
26:42
if a port say I'm looking for 8 GB RAM.
26:49
8 GB RAM. Obviously, this worker node does not have a 8 GB RAM. Obviously, the port
26:55
will fail because they're looking for 8 GB resources, but it's not there. And when you see cubectl describe port
27:02
command, you can see some event failure come up, right? As a message that we are looking for resources, but the resource
27:08
is not available. So you know some kind of memory or some kind of resources error going to come up because of this
27:15
if they fail okay but normally what master node do
27:21
right because we don't we don't send this request directly to worker node right to launching the port okay
27:28
typically this is how it happen we have IT team who use the cubectl command they
27:34
contact to the master node and they are the one who say this is the image I have and from this image I want to launch a
27:40
code and they are the one who say in the vim code I'm looking for 8 GB RAM this
27:45
is what I'm requesting for my container to launch the v code they write
27:52
okay and here the role of controller control plane or master node going to come up right in the master node guys
27:58
there's one program component that you guys might know is called scheduleuler program
28:05
this called cube scheduleuler there's one program running inside the master node
28:10
or this is a duty of the cellular program to check how much request they requesting or how much memory or
28:16
resources they're requesting. A scheduleuler
28:21
sees that IT team is looking for launching this port for 8 GB RAM minimum. That's what they're requesting.
28:27
So this is the duty of the scheduleuler to schedule this port in the right worker node. In this case know worker node have to
28:35
first register with the master node and they keep on sending the metrics to the master node that how much RAM and resources I have right now. So know
28:42
schedule know it that this system have 4 GB RAM. So schedule does not schedule this port over here. They might check
28:50
let's say B system they have 16 GB RAM C system might have 16 GB RAM but out of
28:56
16 maybe 12 is used. So only free is 4 GB. still know it 4G free they again
29:03
don't schedule here but in the C computer they have 16 GB RAM out of 62
29:10
used 14 GB free so they know 14 GB is free in the C so this is a duty of the
29:16
scheduleuler to schedule this port this look for 8 GB and the 14 free so
29:22
they can give this much resources to the port so they can properly deploy it
29:28
okay so this is the duty of the scheduleuler Right. Seduler will schedule
29:35
which worker node your port should go. Okay. But instruction is being given by
29:41
your uh cube cubernetes user or your IT team or the user of the cubernetes cluster
29:48
in maybe enviral code somewhere. Okay. Now in this this concept you guys might
29:53
know I want to quickly revise if you guys missed this concept but this is what the concept purely from the
29:59
cubernetes. This is not a EKS concept. It is a Kubernetes concept.
30:05
What EKS can do for you? EKS can help you to launch this master node. Ease
30:10
will help you to launch these worker node. Okay, that's what EKS will do.
30:18
It means while launching Kubernetes cluster from EKS
30:24
that point in time, you don't have to tell anything master node. master node they will done and do behind the scene
30:32
they won't show you. So in E case you guys know master node is pure fully managed serverless you can't see also
30:40
where the master node is even though you can't see the master node in the EC2 uh
30:46
dashboard okay but worker node in EKS they will
30:51
launch for you okay but we have to ask them how many
30:57
worker node you need so let's I say I need three worker node in the worker node how much memory your
31:03
resources you need you tell these much computing resource I need let's say 16 GB RAM something like this
31:10
okay so interesting thing is they will launch the worker node but you have to
31:16
tell your requirement according to your requirement they will launch the worker node for you
31:23
okay but challenge guys here is mostly we does not know the requirement
31:28
actually how much resources we need in the future
31:34
they were requirement come up maybe one of them p looking for 32 GB RAM I don't have the work or not to launch 32GB RAM
31:40
required port okay we don't know requirement or maybe requirement maybe right now my port is I
31:48
requested for 8 GB but uh is my app port uh a lot
31:55
of requests come up they have a lot of data they they keep on putting in the RAM and 8 GB they they go to 16 GB then
32:01
20 GB be that 100 GB maybe request means
32:07
means this is the minimum requirement they have initially okay there's one more concept is called
32:14
limit that also can set that this board can't use more than 16 GB RAM so some of
32:20
the cases we set the limit but most of cases we might not set the limit in this case they can go for 16 GB 100 GB 200 GB
32:26
RAM okay because a lot of client is coming a lot of data is there I wouldn't put all that in my RAM
32:32
So my resource is going to increase and increase. So initially you might launch a port on node worker node C because
32:41
they have 16 GB RAM or maybe 7 GB RAM free a port is happy initially they got
32:46
8 GB memory here but but this is a this is
32:51
this is a app right while a customer of these guys or client of these guys they connect a lot of data they start
32:57
populating over here and their size will increase to 20 2800 lot more size but
33:05
your work or not have don't have that much resources then finally your application goes down or the fail or
33:10
something will happen. The point here is okay
33:16
point here is worker node they are configuring for us
33:22
okay setting for us resting the worker node for the mark worker node to the master
33:28
node but they're not fully managed they just a managed worker node they're
33:35
managing some part of it means in the future if you want to enhance the version of the worker node a
33:41
container You want to do some patches, you want to upgrade, you want to add more RAM, more
33:46
state resources, you want more worker node, those thing you have to handle. You have to keep on monitoring over
33:52
here. EKS won't do for you. Okay? So in the simple term, what I'm
33:57
trying to tell you here is EKS is not at all the fully managed serverless
34:03
service. Only one part is fully managed. That's called serverless. So it's called manager node or master node or
34:09
controller plane. where they give the guarantee if the more load come up they can increase the
34:15
resource RAM network bandwidth many more things
34:20
that what they handle but in worker node you can tend the
34:27
requirement that's what we will launch for you but in the future if more resources come up EKS won't handle it
34:32
you have to keep on monitoring maybe you can use some kind of auto scaling concept that is a different point but
34:38
you have to handle either manually or with some autoscaling okay way but they won't handle neither
34:45
they will manage the patches that neither they will upgrade the software so the libraries or whatever you do in
34:51
in worker node uh none of things they will going to manage so what I'm trying to tell you here is in EKS worker node
34:58
is not fully managed is not serverless so all EKS does not serverless that's what I'm trying to tell
35:04
you here okay so what we want to do here is we want to make are EKS fully managed
35:13
fully serverless or I can say fully serverless fully we want to make EKS serverless
35:19
okay only this part is not serverless work or not that also we can do
35:24
how by integrating EKS with one concept
35:30
in the container world in AWS called farget
35:36
so if you integrate e EKs far then your worker node can also become fully
35:43
managed or I can say serverless. What benefit you get? You get all the
35:48
benefit of serverless. Now you don't have to manage your servers because if you if you run the EKS cluster you might
35:57
see worker node there in the in your EC2 pages right there server running you can see it you can just go and do something
36:02
inside this but if you run with you can't see any worker node it is
36:08
completely managed by AWS like serverless you don't have to do any update patches all the things is done by
36:14
AWS for you and beautiful thing is everything is you don't have to pre-plan
36:22
how much resources work or node you need three or 10 with this many RAM and CPUs
36:28
as per the demand come up they automatically scale the resources for you
36:34
okay and again this was reason why I created this example so if your IT team
36:40
if your IT team say I want a port with 8 GB RAM so on
36:47
the fly this one interesting thing on the fly the look for this request
36:53
okay so if I if I go here let me get this diagram so if I now let's say I
37:01
want to launch EKS with far and if you guys want to launch EKS with
37:07
far here you have to run this command that's what I run here
37:13
I want my EKS cluster with this name but this should be run with a forget. So
37:18
then this is the option we have to provide. Even though if you go to EKS cluster
37:26
command line and see the help. Okay. So in the help you can find this
37:31
option there that if you want to schedule a pool
37:38
okay in EKS onto far or with farget you have to pass this option
37:44
and as soon as you pass this option then your EKS become fully manage this score serverless. So the
37:50
reason guys why I use this keyword here and if you notice guys here is as as I
37:56
pass this option over here if you notice they are creating this cluster with this name in my case they're using Mumbai
38:03
region but what they're doing guys here is they are running
38:09
two main task one they are launching a control node as your master computer or your controller
38:16
master node that's also only EK case also do for you. But if you notice guys
38:22
here, it is there's no node group. In EKS, there's called node group, right? And inside the node group, you to keep
38:30
your node worker nodes that is not fully managed. You to manage by yourself after they launch.
38:36
Okay. But here there's no node group name. Okay. Here there profile. What is
38:42
project profile? We'll discuss. But at least what is forget? You know now they are managing your worker node
38:49
with the help of farget so how they do and how it work so it is something like this even though after I
38:55
launch I'll show you they still launching so what you will see now if you go to
39:03
ease with there will be nothing none of the server you will see okay because you remember
39:11
from my ease class worker node is visible in EC2 but here you you won't
39:16
see any worker node. So if you go to uh for example let me refresh this
39:21
cluster has been launch or launching it is not yet come launch in sometime visible in some time and then I'll show
39:27
you okay so what happened behind the scene they launch the
39:34
kind of master node this control plane
39:40
a beautiful thing is okay they does not launch any worker node for you
39:48
because it does not require it does not know your requirement initially they only launch a master node for you that's
39:55
called control plane so that your IT team or the user of the cubernetes with
40:00
cube shell command they contact now in the future when IT team come up and say I have a image and from the
40:07
image I want to deploy my port
40:12
and the port guys we mostly uh write a resource
40:19
request. This is a good practice normally we should do. If you don't do then they will go for by default size 1
40:26
GB RAM or 2GB RAM whatever would be this is default normal practice as a cubern
40:31
users we have to do or IDT we have to do so let's say
40:37
uh let's say somebody requested that we need a port uh to be deployed with and
40:42
I'm requesting because this port going to need 8 GB RAM
40:48
okay so as soon the requests come to master node and Again the master node guys this one program is there that
40:55
program you know that's called cube scheduleuler they keep on looking for that if any request come up they have to
41:01
schedule but there's no worker node so behind the scene what they do because
41:07
this cluster we launching with a extension or with a component called forget so far is also keep on listening
41:14
what how much request is coming okay and as soon as the forget see there
41:20
request for port of 8 GB RAM on the fly is everything is invisible.
41:27
Okay. Serverless on the fly they know launch one worker node
41:33
where they give this much sufficient resources 8 GB maybe 8 GB exactly maybe
41:38
more than this something like this let's say 8 GB they launch this worker on the fly by
41:44
looking at the request okay and then finally deploy the port over here
41:53
deploy port over here so means my point is as the request come up.
41:59
Okay, after request come up then only they're going to deploy the then only they launch the worker
42:06
node on the fly and on the top of they launch the port. This everything is same but it's been completely and governed by
42:14
far with EKS. Okay, I can say far is the one who going to launch the worker node and register
42:21
to the EKS master node and launch the port over there. It means if I launch
42:26
one more board then automatically one more worker node they launch for you and
42:31
next board they will deploy on the top of this something like this but there's a
42:38
possibility they also launch a worker node bigger in size so next port they might launch here they might also launch here that's also they can do
42:46
okay and this machine is again one kind of operating system behind the scene but where the operating system is we don't
42:52
know how they're launching we don't know which this Linux or Windows or Mac
42:57
obviously will be like Linux mostly what content engine they use as such we don't know
43:03
okay and even though this board is right now looking for 8 GB RAM in the future this port might looking for 88 GB RAM or
43:12
880 GB RAM automatically this RAM of the worker will autoscale
43:18
you know like a vertical scaling so we increase the resource on the current
43:24
Okay, that is called vertical scaling. So automatically they do the vertical scaling for you behind this. So my point is everything has been completely
43:32
fully managed when when you uh use farget along with
43:39
your container services. This container services can be ECS as not a plan for today's topic or container service can
43:45
be ek pure kubernetes service that's also this can be uh done for you. So
43:51
guys this is the main role of the far right. So my point is it's pure serverlessness and most of the cases not
43:58
always but most of the cases we need this but looking at this case you might see compared to a little bit slower
44:07
because what happen in this case as a p request come up by looking at the
44:14
request then they launch a worker node and then the register then they'll launch a port. So launching the worker
44:19
node on the fly then the register it takes some second even though they they are very fast the way whatever
44:25
technology they use behind the scene and launching the worker node so very quickly within some second they launch
44:30
it and on the top they will launch the port code port again is a container so they launch very quickly uh so it won't
44:39
take much time but but it takes some time okay
44:46
but if you go to pure EKS the work is always there. So you don't have to launch the work on node. You
44:52
don't have to register. So that much time to save you directly launch the port. So launching the port in this kind
44:57
of pre cubin cluster is competitive always faster.
45:05
Okay. So both the setup has their own drawback and pros and cons. Okay. And here we the uh the the challenges we
45:13
have is we have to manage the worker node and resource is completely to launch by ourself or by some autoscaling
45:19
way. Okay. And not serverless but here everything is automated. Okay. And AWS
45:26
is the one who taking the guarantee of upgrade patches security management all the things whatever the things possible
45:32
we need. Okay. So at some second delay you will see only when when you launch first
45:37
that's all not after this okay when you launch a new port first and then only you can see some
45:44
competitive some second delay okay that is only uh you know challenge you have
45:49
here otherwise everything uh goes very very fantastic okay so finally guys if we see your
45:58
ease cluster has launched with the far what is profile we'll see in a minute but this is
46:04
their launch. Okay. And they also give the Kubernetes keys in local file in the
46:11
cube config file. It means I can run my cube cl command. And if you want to see how many nodes we have.
46:20
So guys, they give you some nodes pre-created. Okay. So guys, they give you some node
46:27
also pre-created. But this is just a node, right? they by
46:33
default they give you two maybe this the need this is just a node so you know the
46:39
way I explain to you they has two but my intention is to tell you that let's say
46:44
maybe initial they have two one or two they give a restor to us so any new launching you want to do they can do
46:50
very quickly but the future if you need more nodes they will launch more for you on the fly that's alo also I want to say
46:57
here okay but yeah they give you this two node but it's fully managed look like a node But it is not there in EC2
47:04
services. That's also want to tell you here is two nodes are visible.
47:13
Two node is visible here but in your uh EC2 you won't see any node here. So
47:20
here there's no running node over here. Okay.
47:28
That's also one to see show you. But right now if you see how many ports are running
47:34
so there is no port. And if you try to examine about this node, if you try to
47:40
describe describe
47:45
about your node, let's say this is a node I have. Okay. So if you want to
47:52
describe the node, uh this node completely managed by Kubernetes. You
47:57
don't have to tell how much resources you have to give. So initially
48:03
they launched this worker node. It's a pure worker node actually. They launch this worker node with some
48:09
name and they're using some Linux operating system running in Mumbai somewhere but you don't able to see it
48:17
in your EC2 page this because company managed by see computeriz serverless.
48:24
Okay, that's what they're trying to tell you here is okay and uh plus uh some basic
48:31
information they talking about. They also talk about the IP address of this worker node but it's not visible so I
48:36
can't see the IP address but they give you because some server is running right behind the scen so obviously they should
48:42
have some IP addresses okay initially they give you two CPUs or or some memory
48:51
with this guy okay so that's what they give you initially and um that's all and you know
49:00
guys why they actually give Technically the way I explained to you it shouldn't give to you but why they
49:06
give you uh there's also one more reason right why you have we have seen these two worker nodes
49:13
okay because because in Kubernetes
49:18
we have to run some ports right so this one DNS port is running
49:26
so when it runs a Kubernetes there's some internal Kubernetes system internal ports you have you know DNS port or some
49:33
many ports you have okay internal ports so because the we run this port and
49:39
every port need one worker node or maybe one one one one worker node we have
49:45
multiple port maybe so that is one of the reason why they launch this particular
49:52
worker node just so that we can launch our services or the port okay so if you
49:58
if you see here that there's a node node over there. It not it does not means it
50:03
like a precreated two worker nodes they give you. They they launch this two worker node because on this worker node
50:10
they launch a port which port the port they're running in this name space this Kubernetes intend
50:17
name space. So try to see in this name space.
50:23
Okay we are running with some port because we launched two port. So one port is dedicated to this worker node.
50:29
one port is dedicated to this worker node that's what they launched two
50:34
worker node because we need it my point is forget say if you need a port to launch so if you don't have worker node
50:41
I will launch a worker node for you and you on the top of you can launch it so a code service is not important for us but
50:47
I don't want you confused with this why they launch because something is running behind the scene even the very quickly
50:53
want to describe your port also maybe you can use o wide option
50:59
You can see this port this port
51:05
running on this worker node 60 and this port running on this worker node that is
51:13
30. So technically if you launch EKS with far you have only the controller node.
51:22
Okay, worker will only come up when you need it. Either for the port that has been used internally by the cube or by
51:28
the port that you guys are going to launch.
51:33
Okay, this is what I would like to tell you. Okay, so what I'm going to do, I'm going
51:39
to launch a new port. Right now in my current name space in default name space, I have no port.
51:46
Okay. So, let me launch a code or launch a deployment. The same thing just to
51:52
quick test. Okay. Sorry. So, let me do it. So, what
51:59
I'm going to do uh I'm going to launch a port or the deployment
52:05
with the name let's say my from the image. You can use any of the image. Uh
52:12
let me do one thing. Let me use one of the image I mostly use. So I'll go to hub.docker.com.
52:20
Okay. And going to use image uh in my account.
52:25
This one. Okay. So this image I'm going to use right now.
52:34
So I'm going to launch this port from this image
52:39
and with this deployment and deployment will launch the port for you. So this deployment launch and if you see your
52:46
port your port in the pending state and if you just try to describe it
52:56
describe it uh nothing much you see right now
53:03
it describe much you see even though if you see a port is still pending state if
53:08
you see a node nothing is there nothing is happening But if you see guys, it is in pending
53:15
state. Right? Let me again check. What I'm trying to tell you, I'll tell you in a minute. Okay. So, but this is very
53:21
comic. Now, you will see something happened, right? Okay. Something happened. And now if you
53:28
see the port now, they start doing creating. And if you see the node,
53:36
they launch one more node. See on the fly. So you know what I'm trying to tell you
53:41
guys here is because we requested
53:46
we requested this port to be launched and because right now this is running
53:52
with a far so because you asking this thing to be launched but for the port I
53:58
need a node. I don't have a node. Maybe I have some node but they may be fully engaged by
54:04
some other ports. Okay but I need a node. So first let me launch a node for
54:09
you. So father is the one who actually launched the node for you. Which node I
54:14
think this 19 second ago this node has been launched.
54:20
So as the request come up on the bas of request they launch the node.
54:28
Okay they launch the node. Okay. And that is reason you can see my
54:33
port was waiting right was in the pending state. pending normally means O is looking for the worker node and
54:39
worker node is not there so they pending straight there was a small delay delay what I'm talking about right so in the
54:45
forget world this is a small second delay you can see even they launch with no worker node and quickly restored to
54:50
the cubernetes very quickly in some second that's what you see here but worker node is not launch and not been
54:56
registered they will be going to pending state yeah if you have pure fully running cubernetes there then you wants
55:04
as the pending state they very quickly schedule and they they they launch the container. There's only small drawback
55:11
here. Not a drawback but this is what they because obviously they launching so it takes some time that in seconds
55:18
otherwise thing is come now everything is fully managed. So in this worker node
55:24
in this worker node any come up upgrade up patches increase more memory if you
55:30
need more everything being completely serverless managed by AWS guys and
55:35
that's what you can see also in the port describe command. So in this kind of what you see uh initially they are
55:42
waiting for doing something then they launch the worker node and as the worker node run cube scheduleuler is the one
55:49
who assign this port to this worker node and the reming part will be done the way it should done always finally port has
55:56
been launched and if you see your board has been launched so for the kubernetes standpoint of view
56:04
there's no change in the command the internally underline in
56:10
infrastructure is been serverless. Okay, that's what I'm trying to tell
56:16
you. Okay, and one more quick thing here is if you
56:22
describe this port. Okay, this particular port. So this port
56:29
I use this image. Okay. And in this image while launching
56:37
while I launching I does not mention any request
56:44
okay that this image is going to launch a container. In this container we have some services running database running
56:50
web server running. How much resources they going to use? How much RAM and C CPU they're going to use.
56:57
So if you don't mention anything how to mention this is a pure Kubernetes way. I should tell you in a minute but if you
57:02
don't mention anything so that this particular port
57:09
okay will use a by default RAM and CPU
57:14
that is this one so 005 V means this is like a half CPU say 14 CPU and half GB
57:23
RAM will be attached to this guy in the annotation part of the deployment
57:28
command this from the far automat is attached. Okay,
57:34
it means that they're trying to tell you here is this is what the your current
57:40
image need to launch in the container and because you request actually you
57:46
don't request but this is by default because you're not requested this is go by default it means okay your fate is
57:53
going to launch a worker node okay where this uh port is running with this many
57:59
resources so this particular worker node or I can say this particular worker node is launching with this much
58:06
resources. So if you even though if you just try to describe
58:11
your node I think this is the one and this worker node is is launching around
58:17
these resources only. So this worker node details
58:23
in this worker node. Okay. So we are running this particular course here. So in this worker node
58:30
we only run one port this my port they're not using any resources right
58:35
now because it's like very small nothing we are doing here but in this worker node they are using uh very limited
58:46
memory one port is running in this worker node uh but I think they're using
58:52
some minimum right very much minimum so because we requested for this
58:58
So this worker node maybe worker node they might have some capacity they say I
59:03
always launch two CPU with this much memory but in you can think in this way
59:10
this worker node guys you have only two CPU and this is my memory maybe this is a by
59:17
default minimum worker node they launch why because maybe in the port if they need need more so this worker node has
59:23
more so they can give it but if you launch a code
59:29
you launch a port where you need more than two CPUs and more than this much memory
59:36
automated target will detect and they launch a new work or node with that much CPU and that much memory limit on the
59:41
fly so you don't have to do anything over there okay this one thing okay I'll
59:47
show you one more example of this if you want and uh one more small thing is right now this port is running
59:55
let me scale this code for cubectl scale. I'm going to scale my deployment
1:00:01
because I launched with my deployment and let me create a replicas. Let me get two replica. Now this port has two
1:00:08
replicas. Okay, two replica and if you see is still pending right?
1:00:16
Okay, because by default or I can say the main behavior not by default I can say the main behavior of it of the fire
1:00:22
is okay. They always launch every port.
1:00:31
They always launch every port in a different different nodes. So first they
1:00:36
launch the node then the rest of the node will work on node and they launch the port on the top of this. So it's
1:00:43
like a one to one mapping they're doing uh they're doing for you. So as soon the node going to come up the four node
1:00:50
right now they launch and now this particular port launch on the top of this guy
1:00:56
now they are creating and they launch for you okay this one thing so you can
1:01:02
think one to one kind of mapping one port equals to one node they are uh given to you okay but because we does
1:01:08
not ask how much resources they need so they will look at us with by default resources and minimum kind of worker
1:01:14
node they launch for Okay. So that's one thing but if you
1:01:19
send the request that you need more and CPU then they can launch more for you.
1:01:28
How? So again this is not of this is not u the things that you have to do with
1:01:34
farget or neither from the EKS. Okay. So we are the IT
1:01:40
users. Okay. And while launching the code in
1:01:45
our yl file, we have to write this.
1:01:51
So there's a part we to write in the yml file. Okay, just to quick uh test what
1:01:57
I'm going to do. So I might have some yl file of cubernetes.
1:02:02
So I'm taking some of the deployment file. So this is one deployment file I'm going
1:02:08
to use. So if you know a little bit about Kubernetes, this file is very very simple file.
1:02:15
Okay. So I'm going to launch a new uh port
1:02:21
with this name mys port is just a name. Under this port I'm going to launch a
1:02:27
container. It's called my web con is one container from the image httpd.
1:02:33
Okay. And this port I'm going to launch with the help of deployment. And here I'm going to launch let's say
1:02:38
only one particular replica. Okay. Again this file I am the user of
1:02:45
the Kubernetes or the IT team. So I'm say I just need one port only of this particular image of this port
1:02:54
because I'm not defining any request how many resources we need. So again they will go for go for by default and then
1:03:02
by default is like a default. It doesn't mean is a limit. This is not limit hard limit. If your if your image need more
1:03:10
they will scale. Okay. But by default they has the minimum 1/4 CPU and half GB
1:03:17
RAM. But good practice is that's what always we do normally in Kubernetes
1:03:23
world also even though it's not AWS EKS we always do what we do normally while
1:03:29
launching the code we have a container and in the container there's a keyword
1:03:34
guys this called resources keyword
1:03:40
and in the resources keyword we can say for this container I'm requesting
1:03:48
I'm requesting that I need a CPU.
1:03:55
Okay, how much CPU? Four CPU I need. That's what you can request. Is not
1:04:01
maximum. It means while launching this port, my cube scheduleuler program has
1:04:07
to schedule this port only in the worker node that has a four minimum four CPU
1:04:13
available over there. more than maybe okay but minimum 4 CP we need
1:04:19
and here here we give the unit in this way M this is how we give 4,000 m means
1:04:25
millore 4,000 m means 4,000 more that is equals to four CPUs
1:04:32
okay similarly you can also say I'm looking for a memory
1:04:39
RAM that is some memory let's say 4
1:04:44
96 m memory with 4 GB memory I need minimum.
1:04:51
So this is what guys normally cubernetes user will do always. So schedule will schedule automatic
1:04:57
schedule to wide work but here the work is not there right. So here in this case fet will read this instructions from
1:05:04
your kubernetes uh request in the API kuberneti and and [ __ ] will see okay they are
1:05:11
looking for this. So they will launch a worker node that has these many resources minimum and then this port will deploy there.
1:05:18
This how it work right. So let me show you. So let me save this file and if I run
1:05:24
cube apply deploy
1:05:30
what you will see it cube get
1:05:36
this board is in again see in the pending state and this has been launched
1:05:41
by this deployment deployment will ask this port that you need this much resources and against is pending so
1:05:49
internally what this these guys are doing they're launching the node that has this much resources
1:05:57
so you can't do this thing in EKS right here see no if I run this code in your EKS cluster
1:06:04
typically not fully managed EKS cluster they might fail you might not have a worker not available for this major resource request
1:06:12
or this is this is a minimum this is not minimum I can say this is the request means we are looking at least this
1:06:18
default it might go around in the future but it might increase also but if you it
1:06:23
might increase in the future the requirement of MCP of this particular app
1:06:30
then in pure case cluster they will fail
1:06:35
okay or slow down maybe but here they will autoscale for you so you don't have to worry it's purely serverless again so
1:06:41
you can see here they launch this worker node okay and the top of the worker node they
1:06:48
will launch this port right but if you again describe this port you'll see something
1:06:54
here okay so describe
1:07:00
port this is the board we launch and what you will see here
1:07:06
in this board they launch in this particular work pull the image and they launch it that's what they need and this
1:07:13
is what we requested right so it's coming from your manifest file or yl file
1:07:19
uh from a deployment they come to port. This is what we requested and because this is what we requested see here.
1:07:26
This is what the interesting thing is right. Okay. Automatically
1:07:32
forget is the one who read this instruction from your cubernetes file
1:07:38
because file we always give to API of cubernetes and controller on control plane the read from it. They understand
1:07:46
this is what the requirement is. So they say this is the capacity looking for and
1:07:52
because the capacity looking for I provision this capacity
1:07:57
okay so they in the provision it right again they don't provision in any random order you might give 3GB also so they
1:08:05
have some some fixed range you know like a T2 we have instance type so 1 GB RAM 2
1:08:12
GB RAM 4 GB RAM we don't have 3 GB so they launch 2 GB RAM or 4 GB RAM
1:08:18
again with the combination of CPUs. So 4 CPU with 4 GB RAM worker node is not
1:08:23
available actually. So they have some kind of internally serverless servers with some fixed instance types they
1:08:32
have. So because they you looking 4 GB and four memory RAM so 4G CPU with four
1:08:37
memory RAM is not available. So they launch 4 GB with whatever the next available memory instance let's say 8 GB
1:08:43
that's what they launch for you. So here you don't have a control. So
1:08:48
my main point is they will make sure what is the minimum requirement they will launch but but what in the category
1:08:54
what type of instance type category they have. Okay that's also one cool thing over
1:09:00
here. Okay. And if you um if you see
1:09:05
your nodes, if you see your node
1:09:10
and describe uh your node, I think this is the node we have created and you will see guys
1:09:16
this time a difference, right? This node has a bigger capacity.
1:09:21
Okay. So this node has a 4 GB RAM and uh this much memory. This is actually what
1:09:27
you requested. Okay. allocated requested to you. Okay.
1:09:34
But you talk about this memory, this is a 4 CPU. That's what we are looking for. And this is I think maybe
1:09:41
of 8 GB RAM something like this. Okay. So, but you see this this number
1:09:49
increase. Last time it was two, but we have increased this. Okay. So u this so one of the great use
1:09:59
case guys for farget is this one.
1:10:04
Okay because normally what happen guys kubernetes
1:10:10
cluster is being used by the users and most of the user are developers.
1:10:18
Okay, they might does not know anything about IT infrastructure management.
1:10:26
Okay, they they know how to write a Kubernetes file. They have a port. They
1:10:31
write this mostly they also have some kind of very beautiful web UIs. They don't have to type this. They will tell
1:10:38
you image name replica and say I know need this one. Click enter. That's all.
1:10:45
Okay. So for them we does not know how much they need right. So guys understand this this use case very interesting. So
1:10:52
I want to create one kind of portal mobile app web app then you can create very easily okay where I want to give
1:10:59
somebody a service this service called self-service self-service very very important concept. So if you want to
1:11:06
give somebody a self service in the world of Kubernetes when anyone
1:11:11
come to your portal your your apps they type their number of replicas and image
1:11:18
name and resource record and hit enter that's it now if you have a managed kind of
1:11:25
kubernetes then it will very change they always fail
1:11:30
but in the serverless world in this Now thing is very much uh you know
1:11:38
purely serverless and purely automated and scalable. Okay. So you can think in this way if
1:11:44
you want to give seral selfservice concept to your users especially the
1:11:51
developers okay of your team and your projects then this can be a great service for them to
1:11:58
use. Okay guys can hold for a second please.
1:12:27
Okay. So guys, this I think this this particular practical makes sense, right?
1:12:32
Because normally when we learn the [ __ ] we say serverless and we launch a port automatically, they launch it.
1:12:38
But I'm trying to make you aware the perfect and the right use case of the
1:12:44
farget kind of service. I mean so not saying is a only thing you have to use. Sometime we to use without forget
1:12:49
sometime you use with farget depend upon your requirement right what kind of requirements you have.
1:12:56
Okay. Now next thing guys here is there's one concept guys called
1:13:02
let me do one thing. Okay. So in the cubernetes guys we have multiple name
1:13:08
spaces. Okay right now my port is running the
1:13:14
default name space but let me create more nom space. This is what normally we do always we rarely
1:13:20
launch in the default name space in the real world. Okay. So we have multiple name spaces to go for managing different
1:13:27
teams and securities and permissions. So let's say we have a different name space for our project. Let's say we have
1:13:33
project we have is let's say for uh team A
1:13:38
team A name space. Okay let me give the name T A.
1:13:44
So new name space we created this should be small I believe.
1:13:49
Okay so one more new new name space we created now in this name space or
1:13:55
something also known as project how many port is running. So in this name space team A
1:14:02
there's no port is running. Okay. So let me quickly launch one code
1:14:08
cubectl create deployment let's say my d1 with a image let's say
1:14:15
httpd in a namespace of team a. So you can pass
1:14:21
your name space name and uh the launch there's no challenge here.
1:14:28
And now if you see in the team A this board has been launched with the help of
1:14:33
this deployment
1:14:39
from this deployment this what I've given this port has been launched it's a
1:14:44
pending state you know why it's pending okay because this port is looking for the nodes even if you try to describe
1:14:52
this port also okay this port if you see in the And
1:14:58
there's not a schedule right don't schedule
1:15:03
but here you see something happen right finally you will see this message failed
1:15:10
mean this port is trying to launch pending but for launching what they need
1:15:18
they need a worker node okay so sh looking for worker node and
1:15:24
they say we're looking for a worker node code. Okay. Five worker node is there a
1:15:31
available. I can see five is available.
1:15:37
So five is available. Okay. But I can't use anyone.
1:15:45
I can't use anyone. Why? This is some concept called unlerated unlerated tent
1:15:51
challenges we have. Okay, in the simple term, so those who
1:15:58
know about Kubernetes as one concept, you might know what is tolerance and tend concept, we don't need to learn
1:16:04
that much. But in the simple term, what I'm talking about would need a node
1:16:12
worker node to launch. We have a lot of worker node. Maybe
1:16:17
memory is free. Okay. But but all the workers say no we
1:16:25
are not allow you to launch this concept called tent concept nobody able to
1:16:31
tolerate you so you're not allowed this is in the field scheduling
1:16:39
okay plus right now [ __ ] is there right fet is
1:16:45
not able to help this guy is not helping this guy can say because right now this cluster is running with [ __ ] right but
1:16:51
fget say I can't help you. My responsibility is if you anybody launch
1:16:57
a port I will launch a worker node and and attach this guy to there
1:17:02
but I can't help you. Why? Because by default guys Farget will
1:17:08
work only for the post that will launch in a default name
1:17:16
space. Anything run apart from different or maybe for this name space also this is
1:17:21
cub internal anything that you run in default on cube system will help you
1:17:27
to launch a worker node for you serverless but apart from this and any other name
1:17:34
space if you try to launch a port does not work here
1:17:41
does not work here so none of the worker node launch and this port does not able
1:17:46
to launch there And because there's no worker node launched so cubernetes actually try to launch this port on the
1:17:52
on the running worker nodes but all the worker nodes say no I'm dedicated to other port I'm a port I won't allow this
1:17:58
guy I won't tolerate this guy it's called tolerant tolerance console
1:18:04
so in the main that the single line story what I'm trying to tell you here is forget by default only work
1:18:12
with uh name space called default and cube term. Why? Because this concept is
1:18:19
called target profile. Correct. Okay, that's what I was trying to tell
1:18:25
you. So when you launch a uh key e case that time that message was coming, they
1:18:30
are launching a [ __ ] profile. If I can able to show you here it is. So they are launching a ferret profile
1:18:37
and in this profile profile they say I only support
1:18:43
these two uh name spaces even though you can come to a portal
1:18:50
here also you can see it
1:18:55
okay so in the far
1:19:01
this cluster I not showing you here the cluster name.
1:19:06
Oh sorry this one in this cluster if you see in the
1:19:11
compute there's no worker node guys because everything is serverless
1:19:18
okay and who is managing compute your farget there's no node group also farget
1:19:24
is going to manage this and the forget what I have I have one profile
1:19:31
and this profile is only manage these two name space so it means any port launch in this name space then only
1:19:36
target profile will help you and what this profile will do they will launch right the the worker node for you and
1:19:45
that is and launch the port over there this what your fire profile will do and this is more about your your AWS thing
1:19:54
right so they launch in this VPC subnets there's a different point so they have a
1:20:01
uh they they have the you know any port running in one worker node other port on
1:20:07
other worker node. So VPC give a CGI plugins to to do a networking part and
1:20:12
any power they need they have attached with some AM rule. So that's the common
1:20:18
thing that you always see over here. Okay. And this is nothing nothing much
1:20:24
here. Okay. But this is a guys reason. So it means
1:20:30
okay what if you want to launch a port in different name space that's what we
1:20:36
always want. So here the role of farget profile come in play.
1:20:42
So I can create a profile for per per name spaces. So if you go to compute and
1:20:48
here you can go and create a profile and for creating the profile you can
1:20:55
give a profile name whatever name you can give that is a proper IM role whatever the power your port need in the
1:21:02
future you might need to with EBS service or EFS service that much role you can give here which VPC you want to
1:21:10
use you can give here and just click next let me just do some let's say this is target
1:21:18
okay for team a some name I'm giving
1:21:25
ex role I'm just taking some randomly here nothing just as is and uh name
1:21:31
space you can type here okay so this is for the t name space so
1:21:36
this name space should be same as kubernetes so this is kubernetes internal this is
1:21:42
AWS right but they should match same okay click next and finally you can
1:21:49
create so either you can create from graphical
1:21:54
let me cancel I don't want to create from graphical I want to show you this same thing from the command line so you
1:22:01
can also use the command line to do uh the same thing
1:22:07
okay how so in the command line again we have ekctl command
1:22:14
Okay, in the create if you see the help you will see a target profile you can
1:22:21
create. Okay, profile
1:22:26
again the help uh they're very clear you create profile on which cluster
1:22:32
tell the cluster name okay and uh tell the f profile name and
1:22:38
main thing is the name space okay so I would have to create my f
1:22:45
profile name let's say my target profile for team A I'm going to launch inside
1:22:53
this cluster. Cluster name is LW cluster one. Yeah. And main thing is your your
1:23:02
name space name. My nameace name is team A. This should same as what you have in
1:23:08
the cubern. Or maybe you can can create first profile and maybe you can add the name space in the future.
1:23:15
That's all. Now you have one more f profile.
1:23:21
Okay. It means any code launch in
1:23:27
U team a name space of Kubernetes.
1:23:32
Okay, because we have target profile. So my port will launch there. So target
1:23:39
profile target profile guys is a way to manage different team traffic also because every target fet profile has
1:23:45
their own IM role different power own subnet VPCs.
1:23:51
Okay. So fer profile is just to organize a separate permissions.
1:23:58
Okay. So normally what happen if you know a little bit about kubernetes in the cubernet we get a name spaces.
1:24:05
Okay. So every name space we give to different team different projects and cubern we attach this name space limited
1:24:11
power this cubern stand point of view but a
1:24:16
stand point point of view what I can do now in this name space that's what I
1:24:22
written here if anything launch port so what port power I have what IM power
1:24:28
they have which VPC subnet they going to launch which kind of network traffic they want to manage
1:24:34
that will decide by your fate profile file. This one this is how the things map
1:24:40
and one more thing is now in this team a if you launch any port that port
1:24:46
automatically launch with a work by fiber profile and it allocate to you in that particular VPC and subnet. So this
1:24:53
is how guys your fer profile linked each other. And now if you see here okay last
1:24:59
time guys my port was failing in team A. Okay is in pending.
1:25:06
Maybe I think this might also launch in a minute. Okay in a minute or maybe if
1:25:12
you launch again as a new then they will launch for you.
1:25:18
Okay. So it was actually older uh thing but if we launch one more you will see
1:25:25
the let me show you very quickly. Uh now I'm going to launch one more port.
1:25:31
Okay deployment let me give a name my ID1 under the same team a name space.
1:25:38
Now because we have this time um target profile 14 A. So this time
1:25:45
you'll see this will launch. Okay. How? by launching the
1:25:50
worker node. So we have five you can see one more worker node they launch because we have this fant profile. So if you
1:25:57
also go to this portal okay in the compute
1:26:03
you can see one more name going to come up. It should come up. Let me refresh my page.
1:26:11
They can see this will come up for the team A with with some IM ro and
1:26:16
something right here. Fantastic, right? They launch now.
1:26:22
Last time they fail, but they launch and the port will launch here.
1:26:27
Fantastic, right? So this guy how so it's like a because there's two this is
1:26:33
a hybrid setup guys. Cuber is different AWS different. Okay. So profile is
1:26:39
managing this kind of management between Kubernetes and the AWS world in terms of security resources, networking and many
1:26:47
more things. Okay. So that's it guys almost all the main concepts of the farget I explain u to uh to you okay
1:26:58
main thing profile and those are the concepts uh especially the resources concept okay based on resource they
1:27:04
launch the work or not that I giving you as a demo so but it's very interesting serverless service useful most of the
1:27:09
cases and that's it from my uh side so that's it guys any query if you have uh
1:27:16
otherwise you see you in the next class with some more discussion about this one. Yeah, those who guys are the part
1:27:23
of serless topic class or my EKS class
1:27:29
okay or mastering class or developer class almost everything is we have done
1:27:35
I think some more small point to be discussed yet I think we might take one more class tomorrow and what are the
1:27:41
remaining portion or you is not yet done is being may be completed tomorrow and
1:27:48
uh most of the guys some of the topic we have already given you mostly I think almost every topic I take on on this
1:27:56
live classes some of the topic guys uh we have given in your uh portal has started a recorded
1:28:03
class of one of some from some of my live classes so in this that way almost
1:28:08
every topic whatever plan and given agenda is covered okay some topic is not
1:28:14
yet covered so we might cover maybe very quickly tomorrow
1:28:20
okay And um uh that's all guys. So we'll we'll talk about what the remaining portion would be in the next class,
1:28:26
right? So any query if you have you guys can ask
1:28:31
okay so uh Manu that's what I I explained to
1:28:38
you. Okay. I explained to you uh that where
1:28:44
to use forget right. So if you does not know your
1:28:51
does not know about your what I say how much resource you going to need in the future.
1:28:58
Okay you're going to need in the future. So uh
1:29:07
here the role of here the role of [ __ ] going to come up. [ __ ] will as the need come up they
1:29:13
automate launch for you. Okay, this is a main use or you want you don't want does not want to manage any worker node in
1:29:20
the terms of upgrade or scaling here the role of target come in play this again
1:29:25
the some concept of serverless right okay so that's what uh serverless is
1:29:33
Okay.
1:29:44
The na tented. Can't we add tolerance to launch deployment on the vagger node?
1:29:49
Uh yes archment we can do it but my point is because this is a pure
1:29:56
u pure um serless thing mess around with the white in settings of them maybe
1:30:03
maybe create a lot of challenges. So those who know tent and tolerance you
1:30:09
can also might go there okay and remove the tent then it will obviously it will
1:30:15
work as a cubern point of view. uh but technically you're you know your mess
1:30:21
around with the management of the AWS forget right if you do so again you
1:30:27
won't see the the serverless facility then you won't see the uh that scaling
1:30:33
facility if you you need more resources uh so it will it will might not work so
1:30:40
my point is argument you might have removed the tent that's okay but when if if somebody providing some kind of
1:30:47
serverless test and automate services. Normally we don't want to go and do some
1:30:52
intense changes in the settings. Okay? Because they test everything properly working. So uh we we shouldn't uh do
1:30:59
this square
1:31:05
himu this will do every year. This year mostly mostly we do in September October
1:31:12
month but that is only for our technical volunteer and the ambassadors. So anyone who is the complete year somehow part of
1:31:19
some of my in some of the area in the programs they work for us as ambassadors
1:31:24
or they work with us as a technical volunteers. So squared of p by platform
1:31:29
event is for these guys only
1:31:45
I also would would like to meet everyone guys
1:31:50
okay uh jet yes don't worry jin the other topic has been written okay till
1:31:56
the time whichever program you are in till the time topic is not yet completed
1:32:02
okay we will don't close the uh class either we have conducted this uh those
1:32:10
topic in the live classes or we might have provided for my classroom live recorded in the training portal in any
1:32:17
other way okay all the content whatever given the agenda will always cover
1:32:24
okay or mostly we we will do more than is so if you if you see your EKS
1:32:31
agenda you might not see any topic there called ingress controller right it was not there part of your agenda but you
1:32:37
guys I think some of you guys were asking so I cover inress control in very detail right so a lot of topic like this
1:32:43
we keep on covering okay that might not part of agenda but yeah whatever part of the agenda it will always cover so don't
1:32:48
worry whatever topic you name you've written it won't take much time very basic topics but we'll cover it whatever
1:32:54
is not covered Okay. So
1:33:05
is in the forget is a serverless service. So they will only cost you that much you use.
1:33:13
Okay. So all the resources whatever you use there will only cost that much.
1:33:19
Okay. Can we use EK case with lambda for
1:33:25
launching the node? Uh can we use EK case with lambda? I can't understand this use case mo.
1:33:33
Okay. So what kind of use case you can do anything with lambda. Okay. Maybe you
1:33:39
can do something happen in EKS then event trigger and the event will contact to lambda with event brace and they will
1:33:46
launch go to EC2 and launch it and do something. So anything we can do with lambda and events right? So that's
1:33:52
that's what we can do. But I can't see any use case for this moit. If you have something just let me know. I will lot
1:33:59
talk a little bit more about it.
1:34:16
So from starting June 1st who will create as free account they won't launch
1:34:23
uh Wun actually I don't understand what exactly talking about what you mean by say June 1st starting if you can a
1:34:29
little bit elaborate when you get say sample uh is AWS
1:34:36
dynamics we just started Okay, we just started with kinesis right now. So at least little bit some more topic been
1:34:44
completed. Okay, then we start sharing the sample question for uh uh AWS data
1:34:50
analytics exam for for your practice.
1:35:00
So
1:35:05
high port will be allow available till what time for us? Uh I'm not sure manu is I think depend program to program but
1:35:12
mostly is one year I think I'm not pretty sure if you and contact to pri or
1:35:17
some of my men team they will help you but almost for one year uh or whichever
1:35:23
program you have registered we give the access for one year. But better I can suggest you can check
1:35:28
with the team.
1:35:38
Yes, Moind right in some way resources consumption is getting increased. That's what I was talking about the use cases.
1:35:45
Okay. So um if you launch a port may need more resources in the future if
1:35:51
worker node does not have so what you can do that you might have to handle some way manually.
1:35:58
Okay. So here again get fire will help us autoscale it the underlying resources
1:36:15
I want I have to check this I have not u known about this fact maybe possibility
1:36:21
I have to check right I have to check in the as document or maybe somewhere in uh in some link from AWS So if this is the
1:36:29
case from 1 June then then we see we find some other way to practice right
1:36:35
but let me check first can you take one session on how to take
1:36:40
backup and upgrade EKS I'll try definitely Ashish to talk about backup and um and upgrade part of EKS cluster
1:36:48
definitely I will try to take this part also so that's all but I think you enjoy
1:36:54
this topic of I tries to touch more almost all the main component concepts of far over here. That's it guys from my
1:37:01
side. See you in the next class with some more topic to be discussed. Bye.
1:37:08
Good day guys. Take care.

