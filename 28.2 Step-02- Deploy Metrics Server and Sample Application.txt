# 28.2 Step-02- Deploy Metrics Server and Sample Application

-: Welcome back.We are in the section 15,EKS HPA Horizontal Pod Autoscaling.So just now, we have looked intothe introductory part,what is Horizontal Pod Autoscaling,how it works,and then how it is configured.So, the first thingwe need to install in our clusteris install the Metrics Server.Okay?So the current version,which is live is v0.3.6.But if you want to goto the latest versionand use the latest version,so here is the releases in the referencesyou have the metric server releases section,so you can go there.And so, if you see the latest releases 0.3.6, okay?And, at 17th April they have 3.7,but they still didn't make it aslive release for us, okay?So, now we are using that one,but whenever you are doing this courseand then if you want to crosscheck this commandto be a different value here,so go to this releases sectionand then crosscheck what is the latest release.So, verify whatever this means,like verify if metric server is installedon our respective Kube System, okay?So let's go here,and verify it first, okay?So there should not be any metric serverin our existing cluster.So we can go aheadand then install the Metrics Server.So let me paste it,and it should install the 0.3.6 Metrics Server, okay?So it'll create equaland cluster roll bindingsand then all the related stuff.And then,so now it is installed.So if I say 'kubectl', right?'get pods-nKube-system' right?So we can verify the same thing.See, we can see that Metrics Serveris up and runningin our Kube System name space, right?So if you want,you can even verify that deployment is availablefor us, not using deployment command, okay?So Metrics Server deployment is successful.So the next thing is,deploy our application, okay?So let me refresh here once, okay,so yeah.So deploy our application.So before deploying itwe are going to review our template, right?So let me go here,and if you see hereso this is my EKS HPA Horizontal Pod Autoscaler,so this is the 'HPAdemo.yml'.So if you see it here,this is a standard deploymentwhatever we have createdand it's SQL NodePort servicewith 31231, also,we have created.So the image here is simple Kube NGINX image,so which we have used in our initial sectionsof the course, the same thing.Only thing here,we have also provided the resources sectionwith the, how much memory it need to use,and then how much it is limited for it, okay?So it can go max till 200M,and then after that it cannot move on.So like this, we have also defined the resourcesfor this respective deployment,nothing but for this respective HPA NGINX application.So this means,like metrics should have something right here,so in consideration with metricswe are going to enable.So as we already deployed the metricsso in equivalent with thatthe resources also we have defined our deployment.So let's move on,and then deploy the manifest, okay?So we are here,and then we'll go to 15which is EKA HPA Horizontal Pod Autoscalerand I'm going to say 'kubectlapply-F Kube manifest', right?So 'kubectl get pods',should give the podand HPA demo deployment is up and running for us.So, if you say 'kubectl get SVC'should give the serviceswhere it will say that it is runningon 31231, this HPA demo service.And 'kubectl get nodes-no wide', right?So our deployment is presentin our private subnet,so we are not able to accessthis application externally,but you can generate the load internally.But even though we deployed our application,so we know that if it is onlyour cluster is in public subnetswe will be able to access via NodePorts, right?So that's fine, okay,we can even deploy a load balancertype of service, and then even access it.But our core focus hereis on Horizontal Pod Autoscalerand not anything related to other stuff.So what we are going to dois like, in our next lecture,we'll focus on enabling the Pod Autoscalerand then generating the load and then test it.So, I'll see you in the next lecture.Until then, bye bye.Thank you.