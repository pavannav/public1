# 30.3 Step-03- Load Test, Verify Cluster Worker Nodes Scale-Up and Scale-In

-: Welcome back.In this lecture, we are going to deploy a simple applicationand then verify scaling upand then scaling down of our clusterand then clean up the stuff, okay?So, let's go ahead and then verify our stuff here, okay.So, this is a simple nginx application kube nginx, okay.With one replica we are deployingand we are ensuring that it, it requests further CPU 200mand then memory 200Mi in the initial stage.So whenever we increase this replicas to 30 or 40,so then we will run out of system resourcesin our respective cluster node group,and then it'll try to add two more nodes,whatever it is required, okay.So let me go hereand then paste it and copy and then paste it, okay.so clear, and I'll say kubectl apply -f,I am in 17 EKS auto scaling cluster autoscaler.And then I am deploying my sample application, okay.And now my application is deployed.My next step is to cluster scale upscale our application to 30 pods.So in two to three minutes, one after the other,new nodes will be addedand pods will be scheduled on them.Our max number of nodes will be fourwhich we provided during node group creation.So if you see here during the note group creation,so we said that minimum number of nodes is two,and then maximum number of nodes is four,that's what we have given.So if I go here and for our private thing, right?So which will be in classic and then network load balances.Okay, so this is the value I am talking about.So minimum nodes are two,and then maximum nodes it can scale up to four we said.Okay, so now whenever we increase the loadand then in the two resourcesin the two servers, if the resources are notsufficient then it is going to launch two more nodesand then schedule those stuff.Okay, So let me close this,and then close this and come back and come down.And now first thing is,in one terminal we are going to watch the logswhatever is happening, okay?So in this terminal, okay, andin another terminal we are going to verify howmany pods we have first thing, right?So we have one pod for CA demo,CA means cluster autoscaler.And we are also going to see,get nodes to see how many nodes we have currently.So we have two nodes running which arein private subnet, no external IP.And then now we are going to scalewith the replica's 30 further deployment CA demo deployment.Okay, so for this we are goingto scale up to 30 replicas, okay.So now let's see what happens, okay.So if you see the pods,so few pods will be in the pending state,because it really don't havethe resources to schedule, okay?So now the containers whatever it is getting created,it'll create but rest ofthe things will be in the pending mode.So because it doesn't have the resources to schedule.So now if you see here,here the logs will be running quickly because,so it doesn't have the resources and then it need to scale,scale up to provide more nodes, okay?So let's wait for it to happen, Okay.So now let's come back hereand we can even watch our nodes here, okay.So nodes and then -w right?So and then we can come backand then keep watching these logs also.Okay, so let it schedule the new nodes and then we will see.So out of 30 pods it is saying that six other pods markedas unschedulable can be scheduled.So it has issues with scheduling fewof the pods and let's see what happens, okay.So kubectl get nodes if how many notes we have got, okay.See two more nodes already createdand then they're getting ready and as soon as they're ready.So now our pods also will be in the running stage.So now all the nodes are availablethe two more nodes are in actionand then you can see all the pods are in running state.So that's the good thing, right?So if you see here kubectl get pods.So all 30 are running and if you see kubectl get nodeswe have four maximum, it can scale up to only four nodesand on four nodes all these 30 pods are sufficient, okay?But if you increase it to 50 podsthen remaining will be in pending state,and it already reaches the max of four nodes.So it is going to put all those podsin the pending state only.So the Kubernetes scheduler will put itin pending state only.So that's about it.So now if you want to scale down, right,so what you can do is like you can again bring backto one node and one podand then verify the nodes, okay?So scale replicas to one and then waitfor the pods to watch, okay?And all the pods will go over terminating and thenwe'll have only one pod at the end, okay.So only one pod we have now and slowlythis will also come down.So unneeded nodes will be removed immediately,within 5 to 10 minutes.It might take close to 5 to 10 minutes to cool down,and then bring back to from four nodes to two nodes, okay.So let's wait for that to happen.So we can see it here.Few of the nodes are unneeded and then it is sayingthat we can remove them, okay?So it will, it will slowly remove it and thenfinally we'll end up with only two nodes.So let's wait for it to happen.So we can see it nowafter 10 minutes for the two nodes, scheduling is disabled.So once the scheduling is disabled,the next step is to go,It will go aheadand then automatically terminate those nodes, okay.So let's wait for it to also get it, terminate it.We can see now our number of nodes became two from four.So that's about it.So let's go back and then clean up our applicationwhatever we have created and we will clean that up.We'll go to 17 EKS clearand the clean up the application, whatever we have created.And that's all, okay.So we have completed the end to end section relatedto cluster autoscaler.In our next section we are goingto discuss about another topic.So I'll see you in the next section.Until then, bye-bye.Thank you.