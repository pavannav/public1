# 24.8 Step-08- Build Stage- AWS CodePipeline Introduction

-: Welcome back.In this lecture we are going to understandabout Code Pipeline on a very high level, right?So Code Pipeline is a continuous delivery service to model,visualize and automate rhe steps requiredto release our software.So its benefits include we can automateour release processes,we can establish a consistent release processfor our applications so we can even speed up deliverywhile improving the quality.So it also supports external tools integrationfor source build and then deploy phases.So, and it also provides a view progress databaseand glances.In addition to that we can even havethe pipeline history details for our reference.So code pipeline architecture.So we already know that source build and deployare the major stages inside the code pipeline.So from source perspective, it primarily dependsor it primarily integrated with code commiteasy to contain a registry, and then simple storage serviceand also GitHub in addition to that.So from build perspective,it uses code build and then Jenkins.And from Deploy perspective,it uses Cloud formation, CodeDeploy,Elastic Beanstalk, Service Catalog,Simple Storage Service, Elastic Container Serviceand also Elastic Container Service Bluegreen deployments.And to monitor the source it is going to useeither the CloudWatch when we are using the code commitor other services.And when we are using GitHub,it is going to use the GitHub web hooks.And so what is this continuous delivery, right?So as developers, I check in the code to the sourceand what happens at that time?So whenever the change happened on our sourcea CloudWatch event is triggered and then entire pipeline,this entire pipeline gets triggeredand the build process starts and then changes are builtand then push you to the S3 bucket for our artifacts.And then the same is pulledand then deployed to our respective staging environmentor whatever the environment configure.And then an email approval flow is sentand then respect to approval,if he approves, then only it'll be deployedto our production.And then once again,customers start giving the requirements,it comes back and then developers makes the changes.And then again, checks in the codeand entire pipeline story follows.This is on a high level, the continuous delivery flow.So in our case, how we are going to implement it, right?So there are different ways to implement these stuffbut we are going to do in a minimal waywhatever is possible for us, right?So as a developer, I will do my docker related docker filesand also cube manifest.I'll check into the local Git reportand I'll push the same to the code commit repository.What happens immediately,a CloudWatch event will be triggeredif I configure the pipelineand then pipeline is available for this respective source,automatically a cloud watch event will be triggeredand then pipeline will be triggered.So immediately a code build process will startand then whatever we defined in our code build processwhich is nothing but these things, right?So the entire things are going to get executed,the pre-build steps and then build steps will executeand then it'll create a new DACA image and thenit will push the docker image to the docker repositorywhich is nothing but our elastic container repository.And it also pushes the artifacts to the S3 bucket.So whatever the artifacts generated,these will be available in the S3 bucket.And then it also applies, means like the next thingafter post-build we also saidthat cube CTL apply hyphen F cube manifests, right?So it will take into considerationwhenever this command is executed automaticallythat changes the change, the docker imagewill be applied to our respective deploymentand then old parts will get terminatedand then new ports will get create.I'll see you in the next lecture, until then bye bye.Thank you.